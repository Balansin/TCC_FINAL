{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Global Optimization employing Deep Gaussian Process Regression\n",
    "\n",
    "See DGPR implementations in:\n",
    "\n",
    "https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/KISSGP_Deep_Kernel_Regression_CUDA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m qmc\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m erf\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultivariateNormal\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExactGP\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import erf\n",
    "\n",
    "import torch\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel\n",
    "from gpytorch.utils.grid import ScaleToBounds\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.settings import use_toeplitz, fast_pred_var\n",
    "from torch.optim import Adam\n",
    "\n",
    "from BSA import bsa\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *GPR Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=1.5))\n",
    "        self.scale_to_bounds = ScaleToBounds(-1., 1.)\n",
    "        \n",
    "        # Store train_y for later use\n",
    "        self.train_outputs = train_y\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.scale_to_bounds(x)\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, g, training_iterations):\n",
    "    # Initialize the models and likelihood\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x=x, train_y=g, likelihood=likelihood)\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.cuda()\n",
    "    #     likelihood = likelihood.cuda()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = Adam([\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.01)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Example training loop\n",
    "    def train():\n",
    "        loss_value = 1e8\n",
    "        patience = round(training_iterations/50)\n",
    "        wait = 0\n",
    "\n",
    "        for _ in range(training_iterations):\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get output from model\n",
    "            output = model(x)\n",
    "\n",
    "            # Calc loss and backprop derivatives\n",
    "            loss = -mll(output, g)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Callback\n",
    "            if loss_value <= loss.item():\n",
    "                wait += 1\n",
    "            else:\n",
    "                wait = 0\n",
    "\n",
    "            if wait > patience:\n",
    "                break\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # print(f\"Iter {_+1} - Loss: {loss.item()}\")\n",
    "        print(f'Loss: {loss.item()}')\n",
    "    train()\n",
    "\n",
    "    # print('Test MAE: \\\n",
    "    # {}'.format(torch.mean(torch.abs(preds.mean - test_y))))\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial sampling plan (Latin Hypercube sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LHS_sample(num_points, DIM):\n",
    "    # Number of variables and sampling points\n",
    "    size = int(num_points)\n",
    "\n",
    "    # Generate LHS samples\n",
    "    sampler = qmc.LatinHypercube(d=DIM,\n",
    "                                 optimization=\"random-cd\")\n",
    "    lhs_sample = sampler.random(n=size)\n",
    "\n",
    "    return lhs_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, model):\n",
    "    \"\"\"\n",
    "    Function to calculate the Expected Improvement using Monte Carlo Integration.\n",
    "    \n",
    "    Parameters:\n",
    "        x (array): Individual under evaluation.\n",
    "        y (array): Valor mínimo obtido até o momento.\n",
    "    \n",
    "    Returns:\n",
    "        array: The value of the Expected Improvement.\n",
    "    \"\"\"\n",
    "    # Get the minimum value obtained so far\n",
    "    ymin = torch.min(model.train_outputs)\n",
    "    # ymin = torch.Tensor(ymin)\n",
    "\n",
    "    # Calculate the prediction value and the variance (Ssqr)\n",
    "    with torch.no_grad(), use_toeplitz(False), fast_pred_var():\n",
    "        preds = model(x)\n",
    "    f = preds.mean\n",
    "    s = preds.variance\n",
    "\n",
    "    # Check for any errors that are less than zero (due to numerical error)\n",
    "    s[s < 0] = 0  # Set negative variances to zero\n",
    "\n",
    "    # Calculate the RMSE (Root Mean Square Error)\n",
    "    s = torch.sqrt(s)\n",
    "\n",
    "    # Calculation of Expected Improvement\n",
    "    term1 = (ymin - f) * (0.5 + 0.5 * erf((ymin - f) / (s * torch.sqrt( torch.from_numpy(np.array([2])) ))))\n",
    "    term2 = (1 / torch.sqrt(2 * torch.from_numpy(np.array([np.pi])))) * s * torch.exp(-((ymin - f) ** 2) / (2 * s ** 2))\n",
    "    \n",
    "    return -(term1 + term2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun(x):\n",
    "    # Entra com x e constrói o modelo\n",
    "    coordenadas(x)\n",
    "    barras\n",
    "    secoes\n",
    "    materiais\n",
    "    massa(x)\n",
    "    \n",
    "    # Análisa cada caso de carregamento no ANSYS\n",
    "    P, h = 0, 1e5\n",
    "    for carregamento in range(n_carregamentos):\n",
    "        M, N, d = analisa_no_ANSYS(modelo)\n",
    "        for elemento in range(n_elementos):\n",
    "            if abs(d[elemento]) >= d_adm:\n",
    "                P += h*(d[elemento] - d_adm)/d_adm\n",
    "            \n",
    "            if abs(N[elemento]) >= N_adm:\n",
    "                P += h*(N[elemento] - N_adm)/N_adm\n",
    "        \n",
    "        P /= n_elementos\n",
    "    P /= n_carregamentos\n",
    "    \n",
    "    # Penaliza (se precisar) o Momento, força axial e deslocamento de cada estaca,\n",
    "    # para cada caso de carregamento e soma na penalização\n",
    "    y = massa + P\n",
    "\n",
    "    if isinstance(y, float):\n",
    "        return torch.Tensor([y])\n",
    "    return torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 2000  # epochs for training the GPR\n",
    "N_INITIAL = 100  # initial number of points\n",
    "N_INFILL = 400  # number of infill points\n",
    "BSA_POPSIZE = 25\n",
    "BSA_EPOCH = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 18 # dimension of the problem\n",
    "LOWER_BOUNDS = torch.zeros((DIM))\n",
    "UPPER_BOUNDS = torch.ones((DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate initial sampling plan using LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LHS_sample(N_INITIAL, DIM)\n",
    "x = torch.from_numpy(x)\n",
    "x = x.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = obj_fun(x)\n",
    "f = f.view(N_INITIAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Global Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.009805323014733e+25\n",
      "\n",
      "Iteration 0. Best: 4480.72314453125 kg\n"
     ]
    }
   ],
   "source": [
    "model, likelihood = train_model(x, f, TRAINING_ITERATIONS)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "it = 0\n",
    "print(f'\\nIteration {it}. Best: {torch.min(f)} kg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 200\n",
      "Loss: 1.0076856768285132e+25\n",
      "\n",
      "Iteration 1. Best: 4480.72314453125 kg\n",
      "Iteration 2 of 200\n",
      "Loss: 1.0040422142896547e+25\n",
      "\n",
      "Iteration 2. Best: 4480.72314453125 kg\n",
      "Iteration 3 of 200\n",
      "Loss: 1.0017775305781555e+25\n",
      "\n",
      "Iteration 3. Best: 4480.72314453125 kg\n",
      "Iteration 4 of 200\n",
      "Loss: 9.988835823094418e+24\n",
      "\n",
      "Iteration 4. Best: 4480.72314453125 kg\n",
      "Iteration 5 of 200\n",
      "Loss: 1.0117107563853967e+25\n",
      "\n",
      "Iteration 5. Best: 4480.72314453125 kg\n",
      "Iteration 6 of 200\n",
      "Loss: 1.0248187821398738e+25\n",
      "\n",
      "Iteration 6. Best: 4480.72314453125 kg\n",
      "Iteration 7 of 200\n",
      "Loss: 1.0224989887804543e+25\n",
      "\n",
      "Iteration 7. Best: 4480.72314453125 kg\n",
      "Iteration 8 of 200\n",
      "Loss: 1.0181254963448787e+25\n",
      "\n",
      "Iteration 8. Best: 4480.72314453125 kg\n",
      "Iteration 9 of 200\n",
      "Loss: 1.0169700384129617e+25\n",
      "\n",
      "Iteration 9. Best: 4480.72314453125 kg\n",
      "Iteration 10 of 200\n",
      "Loss: 1.0146694988426692e+25\n",
      "\n",
      "Iteration 10. Best: 4480.72314453125 kg\n",
      "Iteration 11 of 200\n",
      "Loss: 1.011815672242316e+25\n",
      "\n",
      "Iteration 11. Best: 4480.72314453125 kg\n",
      "Iteration 12 of 200\n",
      "Loss: 1.0088581979986984e+25\n",
      "\n",
      "Iteration 12. Best: 4480.72314453125 kg\n",
      "Iteration 13 of 200\n",
      "Loss: 1.0048502969722336e+25\n",
      "\n",
      "Iteration 13. Best: 4480.72314453125 kg\n",
      "Iteration 14 of 200\n",
      "Loss: 1.0038322672836658e+25\n",
      "\n",
      "Iteration 14. Best: 4480.72314453125 kg\n",
      "Iteration 15 of 200\n",
      "Loss: 1.0010759778426022e+25\n",
      "\n",
      "Iteration 15. Best: 4480.72314453125 kg\n",
      "Iteration 16 of 200\n",
      "Loss: 9.978616326877583e+24\n",
      "\n",
      "Iteration 16. Best: 4480.72314453125 kg\n",
      "Iteration 17 of 200\n",
      "Loss: 1.0108922974092763e+25\n",
      "\n",
      "Iteration 17. Best: 4480.72314453125 kg\n",
      "Iteration 18 of 200\n",
      "Loss: 1.0077585414676044e+25\n",
      "\n",
      "Iteration 18. Best: 4480.72314453125 kg\n",
      "Iteration 19 of 200\n",
      "Loss: 1.0055897808252885e+25\n",
      "\n",
      "Iteration 19. Best: 4480.72314453125 kg\n",
      "Iteration 20 of 200\n",
      "Loss: 1.018272493836716e+25\n",
      "\n",
      "Iteration 20. Best: 4480.72314453125 kg\n",
      "Iteration 21 of 200\n",
      "Loss: 1.0155196631601663e+25\n",
      "\n",
      "Iteration 21. Best: 4480.72314453125 kg\n",
      "Iteration 22 of 200\n",
      "Loss: 1.013247946627489e+25\n",
      "\n",
      "Iteration 22. Best: 4480.72314453125 kg\n",
      "Iteration 23 of 200\n",
      "Loss: 1.0253735679678906e+25\n",
      "\n",
      "Iteration 23. Best: 4480.72314453125 kg\n",
      "Iteration 24 of 200\n",
      "Loss: 1.0222823548297387e+25\n",
      "\n",
      "Iteration 24. Best: 4480.72314453125 kg\n",
      "Iteration 25 of 200\n",
      "Loss: 1.020277309041077e+25\n",
      "\n",
      "Iteration 25. Best: 4480.72314453125 kg\n",
      "Iteration 26 of 200\n",
      "Loss: 1.0181067037243536e+25\n",
      "\n",
      "Iteration 26. Best: 4480.72314453125 kg\n",
      "Iteration 27 of 200\n",
      "Loss: 1.0146480545026835e+25\n",
      "\n",
      "Iteration 27. Best: 4480.72314453125 kg\n",
      "Iteration 28 of 200\n",
      "Loss: 1.010904748961526e+25\n",
      "\n",
      "Iteration 28. Best: 4480.72314453125 kg\n",
      "Iteration 29 of 200\n",
      "Loss: 1.0252921717096653e+25\n",
      "\n",
      "Iteration 29. Best: 4480.72314453125 kg\n",
      "Iteration 30 of 200\n",
      "Loss: 1.0221737496240047e+25\n",
      "\n",
      "Iteration 30. Best: 4480.72314453125 kg\n",
      "Iteration 31 of 200\n",
      "Loss: 1.0337555378906833e+25\n",
      "\n",
      "Iteration 31. Best: 4480.72314453125 kg\n",
      "Iteration 32 of 200\n",
      "Loss: 1.0304742079964217e+25\n",
      "\n",
      "Iteration 32. Best: 4480.72314453125 kg\n",
      "Iteration 33 of 200\n",
      "Loss: 1.0285974823712227e+25\n",
      "\n",
      "Iteration 33. Best: 4480.72314453125 kg\n",
      "Iteration 34 of 200\n",
      "Loss: 1.0266350946782314e+25\n",
      "\n",
      "Iteration 34. Best: 4480.72314453125 kg\n",
      "Iteration 35 of 200\n",
      "Loss: 1.0241069684029295e+25\n",
      "\n",
      "Iteration 35. Best: 4480.72314453125 kg\n",
      "Iteration 36 of 200\n",
      "Loss: 1.0214622817635118e+25\n",
      "\n",
      "Iteration 36. Best: 4480.72314453125 kg\n",
      "Iteration 37 of 200\n",
      "Loss: 1.0331796535991322e+25\n",
      "\n",
      "Iteration 37. Best: 4480.72314453125 kg\n",
      "Iteration 38 of 200\n",
      "Loss: 1.0309269602712808e+25\n",
      "\n",
      "Iteration 38. Best: 4480.72314453125 kg\n",
      "Iteration 39 of 200\n",
      "Loss: 1.0284989075825788e+25\n",
      "\n",
      "Iteration 39. Best: 4480.72314453125 kg\n",
      "Iteration 40 of 200\n",
      "Loss: 1.0256446198136236e+25\n",
      "\n",
      "Iteration 40. Best: 4480.72314453125 kg\n",
      "Iteration 41 of 200\n",
      "Loss: 1.0239415241670184e+25\n",
      "\n",
      "Iteration 41. Best: 4480.72314453125 kg\n",
      "Iteration 42 of 200\n",
      "Loss: 1.0218272967118704e+25\n",
      "\n",
      "Iteration 42. Best: 4480.72314453125 kg\n",
      "Iteration 43 of 200\n",
      "Loss: 1.0323349080127067e+25\n",
      "\n",
      "Iteration 43. Best: 4480.72314453125 kg\n",
      "Iteration 44 of 200\n",
      "Loss: 1.02992069038206e+25\n",
      "\n",
      "Iteration 44. Best: 4480.72314453125 kg\n",
      "Iteration 45 of 200\n",
      "Loss: 1.0278455469659181e+25\n",
      "\n",
      "Iteration 45. Best: 4480.72314453125 kg\n",
      "Iteration 46 of 200\n",
      "Loss: 1.0387820450664682e+25\n",
      "\n",
      "Iteration 46. Best: 4480.72314453125 kg\n",
      "Iteration 47 of 200\n",
      "Loss: 1.0369287237478127e+25\n",
      "\n",
      "Iteration 47. Best: 4480.72314453125 kg\n",
      "Iteration 48 of 200\n",
      "Loss: 1.0475105831935457e+25\n",
      "\n",
      "Iteration 48. Best: 4480.72314453125 kg\n",
      "Iteration 49 of 200\n",
      "Loss: 1.0455887783375166e+25\n",
      "\n",
      "Iteration 49. Best: 4480.72314453125 kg\n",
      "Iteration 50 of 200\n",
      "Loss: 1.0428188844226986e+25\n",
      "\n",
      "Iteration 50. Best: 4480.72314453125 kg\n",
      "Iteration 51 of 200\n",
      "Loss: 1.040201983191542e+25\n",
      "\n",
      "Iteration 51. Best: 4480.72314453125 kg\n",
      "Iteration 52 of 200\n",
      "Loss: 1.0382469741961802e+25\n",
      "\n",
      "Iteration 52. Best: 4480.72314453125 kg\n",
      "Iteration 53 of 200\n",
      "Loss: 1.0360131887810044e+25\n",
      "\n",
      "Iteration 53. Best: 4480.72314453125 kg\n",
      "Iteration 54 of 200\n",
      "Loss: 1.0471897251388136e+25\n",
      "\n",
      "Iteration 54. Best: 4480.72314453125 kg\n",
      "Iteration 55 of 200\n",
      "Loss: 1.044802831747826e+25\n",
      "\n",
      "Iteration 55. Best: 4480.72314453125 kg\n",
      "Iteration 56 of 200\n",
      "Loss: 1.0419523486198361e+25\n",
      "\n",
      "Iteration 56. Best: 4480.72314453125 kg\n",
      "Iteration 57 of 200\n",
      "Loss: 1.039099329064536e+25\n",
      "\n",
      "Iteration 57. Best: 4480.72314453125 kg\n",
      "Iteration 58 of 200\n",
      "Loss: 1.038102282547352e+25\n",
      "\n",
      "Iteration 58. Best: 4480.72314453125 kg\n",
      "Iteration 59 of 200\n",
      "Loss: 1.0483148612351595e+25\n",
      "\n",
      "Iteration 59. Best: 4480.72314453125 kg\n",
      "Iteration 60 of 200\n",
      "Loss: 1.0464904782462696e+25\n",
      "\n",
      "Iteration 60. Best: 4480.72314453125 kg\n",
      "Iteration 61 of 200\n",
      "Loss: 1.043341042572135e+25\n",
      "\n",
      "Iteration 61. Best: 4480.72314453125 kg\n",
      "Iteration 62 of 200\n",
      "Loss: 1.0416298764749976e+25\n",
      "\n",
      "Iteration 62. Best: 4480.72314453125 kg\n",
      "Iteration 63 of 200\n",
      "Loss: 1.0392684626492618e+25\n",
      "\n",
      "Iteration 63. Best: 4480.72314453125 kg\n",
      "Iteration 64 of 200\n",
      "Loss: 1.037228598631161e+25\n",
      "\n",
      "Iteration 64. Best: 4480.72314453125 kg\n",
      "Iteration 65 of 200\n",
      "Loss: 1.0350896986558143e+25\n",
      "\n",
      "Iteration 65. Best: 4480.72314453125 kg\n",
      "Iteration 66 of 200\n",
      "Loss: 1.0322058960963412e+25\n",
      "\n",
      "Iteration 66. Best: 4480.72314453125 kg\n",
      "Iteration 67 of 200\n",
      "Loss: 1.0429844439507602e+25\n",
      "\n",
      "Iteration 67. Best: 4480.72314453125 kg\n",
      "Iteration 68 of 200\n",
      "Loss: 1.040523071830575e+25\n",
      "\n",
      "Iteration 68. Best: 4480.72314453125 kg\n",
      "Iteration 69 of 200\n",
      "Loss: 1.0394348292223766e+25\n",
      "\n",
      "Iteration 69. Best: 4480.72314453125 kg\n",
      "Iteration 70 of 200\n",
      "Loss: 1.0366393404501564e+25\n",
      "\n",
      "Iteration 70. Best: 4480.72314453125 kg\n",
      "Iteration 71 of 200\n",
      "Loss: 1.0339027659668216e+25\n",
      "\n",
      "Iteration 71. Best: 4480.72314453125 kg\n",
      "Iteration 72 of 200\n",
      "Loss: 1.032205319635589e+25\n",
      "\n",
      "Iteration 72. Best: 4480.72314453125 kg\n",
      "Iteration 73 of 200\n",
      "Loss: 1.0301812506421011e+25\n",
      "\n",
      "Iteration 73. Best: 4480.72314453125 kg\n",
      "Iteration 74 of 200\n",
      "Loss: 1.0269747453534886e+25\n",
      "\n",
      "Iteration 74. Best: 4480.72314453125 kg\n",
      "Iteration 75 of 200\n",
      "Loss: 1.02579634428363e+25\n",
      "\n",
      "Iteration 75. Best: 4480.72314453125 kg\n",
      "Iteration 76 of 200\n",
      "Loss: 1.02390186366726e+25\n",
      "\n",
      "Iteration 76. Best: 4480.72314453125 kg\n",
      "Iteration 77 of 200\n",
      "Loss: 1.0220745983746085e+25\n",
      "\n",
      "Iteration 77. Best: 4480.72314453125 kg\n",
      "Iteration 78 of 200\n",
      "Loss: 1.019580829160144e+25\n",
      "\n",
      "Iteration 78. Best: 4480.72314453125 kg\n",
      "Iteration 79 of 200\n",
      "Loss: 1.029495723515462e+25\n",
      "\n",
      "Iteration 79. Best: 4480.72314453125 kg\n",
      "Iteration 80 of 200\n",
      "Loss: 1.026834434806378e+25\n",
      "\n",
      "Iteration 80. Best: 4480.72314453125 kg\n",
      "Iteration 81 of 200\n",
      "Loss: 1.0258551432803649e+25\n",
      "\n",
      "Iteration 81. Best: 4480.72314453125 kg\n",
      "Iteration 82 of 200\n",
      "Loss: 1.0356691570040297e+25\n",
      "\n",
      "Iteration 82. Best: 4480.72314453125 kg\n",
      "Iteration 83 of 200\n",
      "Loss: 1.0329544880292824e+25\n",
      "\n",
      "Iteration 83. Best: 4480.72314453125 kg\n",
      "Iteration 84 of 200\n",
      "Loss: 1.0307534455848375e+25\n",
      "\n",
      "Iteration 84. Best: 4480.72314453125 kg\n",
      "Iteration 85 of 200\n",
      "Loss: 1.0288450146182618e+25\n",
      "\n",
      "Iteration 85. Best: 4480.72314453125 kg\n",
      "Iteration 86 of 200\n",
      "Loss: 1.0270185563706636e+25\n",
      "\n",
      "Iteration 86. Best: 4480.72314453125 kg\n",
      "Iteration 87 of 200\n",
      "Loss: 1.0253758738108998e+25\n",
      "\n",
      "Iteration 87. Best: 4480.72314453125 kg\n",
      "Iteration 88 of 200\n",
      "Loss: 1.0221366255515564e+25\n",
      "\n",
      "Iteration 88. Best: 4480.72314453125 kg\n",
      "Iteration 89 of 200\n",
      "Loss: 1.0214289623320287e+25\n",
      "\n",
      "Iteration 89. Best: 4480.72314453125 kg\n",
      "Iteration 90 of 200\n",
      "Loss: 1.0307397258189327e+25\n",
      "\n",
      "Iteration 90. Best: 4480.72314453125 kg\n",
      "Iteration 91 of 200\n",
      "Loss: 1.0287289154227479e+25\n",
      "\n",
      "Iteration 91. Best: 4480.72314453125 kg\n",
      "Iteration 92 of 200\n",
      "Loss: 1.0249157428384112e+25\n",
      "\n",
      "Iteration 92. Best: 4480.72314453125 kg\n",
      "Iteration 93 of 200\n",
      "Loss: 1.023580775028227e+25\n",
      "\n",
      "Iteration 93. Best: 4480.72314453125 kg\n",
      "Iteration 94 of 200\n",
      "Loss: 1.034184655274698e+25\n",
      "\n",
      "Iteration 94. Best: 4480.72314453125 kg\n",
      "Iteration 95 of 200\n",
      "Loss: 1.0320650090884783e+25\n",
      "\n",
      "Iteration 95. Best: 4480.72314453125 kg\n",
      "Iteration 96 of 200\n",
      "Loss: 1.0306928019136952e+25\n",
      "\n",
      "Iteration 96. Best: 4480.72314453125 kg\n",
      "Iteration 97 of 200\n",
      "Loss: 1.0279444676310134e+25\n",
      "\n",
      "Iteration 97. Best: 4480.72314453125 kg\n",
      "Iteration 98 of 200\n",
      "Loss: 1.0246835444473834e+25\n",
      "\n",
      "Iteration 98. Best: 4480.72314453125 kg\n",
      "Iteration 99 of 200\n",
      "Loss: 1.023367830426326e+25\n",
      "\n",
      "Iteration 99. Best: 4480.72314453125 kg\n",
      "Iteration 100 of 200\n",
      "Loss: 1.0226994818301055e+25\n",
      "\n",
      "Iteration 100. Best: 4480.72314453125 kg\n",
      "Iteration 101 of 200\n",
      "Loss: 1.0320561315928928e+25\n",
      "\n",
      "Iteration 101. Best: 4480.72314453125 kg\n",
      "Iteration 102 of 200\n",
      "Loss: 1.0298124310527774e+25\n",
      "\n",
      "Iteration 102. Best: 4480.72314453125 kg\n",
      "Iteration 103 of 200\n",
      "Loss: 1.028338766785589e+25\n",
      "\n",
      "Iteration 103. Best: 4480.72314453125 kg\n",
      "Iteration 104 of 200\n",
      "Loss: 1.0255730233881876e+25\n",
      "\n",
      "Iteration 104. Best: 4480.72314453125 kg\n",
      "Iteration 105 of 200\n",
      "Loss: 1.023105771368329e+25\n",
      "\n",
      "Iteration 105. Best: 4480.72314453125 kg\n",
      "Iteration 106 of 200\n",
      "Loss: 1.0329209380134984e+25\n",
      "\n",
      "Iteration 106. Best: 4480.72314453125 kg\n",
      "Iteration 107 of 200\n",
      "Loss: 1.0314432385210438e+25\n",
      "\n",
      "Iteration 107. Best: 4480.72314453125 kg\n",
      "Iteration 108 of 200\n",
      "Loss: 1.0285093991682707e+25\n",
      "\n",
      "Iteration 108. Best: 4480.72314453125 kg\n",
      "Iteration 109 of 200\n",
      "Loss: 1.0269243626837372e+25\n",
      "\n",
      "Iteration 109. Best: 4480.72314453125 kg\n",
      "Iteration 110 of 200\n",
      "Loss: 1.0250874128504472e+25\n",
      "\n",
      "Iteration 110. Best: 4480.72314453125 kg\n",
      "Iteration 111 of 200\n",
      "Loss: 1.0230906680966186e+25\n",
      "\n",
      "Iteration 111. Best: 4480.72314453125 kg\n",
      "Iteration 112 of 200\n",
      "Loss: 1.02137673498787e+25\n",
      "\n",
      "Iteration 112. Best: 4480.72314453125 kg\n",
      "Iteration 113 of 200\n",
      "Loss: 1.0194894024848286e+25\n",
      "\n",
      "Iteration 113. Best: 4480.72314453125 kg\n",
      "Iteration 114 of 200\n",
      "Loss: 1.0290136870343858e+25\n",
      "\n",
      "Iteration 114. Best: 4480.72314453125 kg\n",
      "Iteration 115 of 200\n",
      "Loss: 1.0270602921291304e+25\n",
      "\n",
      "Iteration 115. Best: 4480.72314453125 kg\n",
      "Iteration 116 of 200\n",
      "Loss: 1.0247761240442033e+25\n",
      "\n",
      "Iteration 116. Best: 4480.72314453125 kg\n",
      "Iteration 117 of 200\n",
      "Loss: 1.0222290898562259e+25\n",
      "\n",
      "Iteration 117. Best: 4480.72314453125 kg\n",
      "Iteration 118 of 200\n",
      "Loss: 1.0197542285544368e+25\n",
      "\n",
      "Iteration 118. Best: 4480.72314453125 kg\n",
      "Iteration 119 of 200\n",
      "Loss: 1.0175581436724617e+25\n",
      "\n",
      "Iteration 119. Best: 4480.72314453125 kg\n",
      "Iteration 120 of 200\n",
      "Loss: 1.0174023839771893e+25\n",
      "\n",
      "Iteration 120. Best: 4480.72314453125 kg\n",
      "Iteration 121 of 200\n",
      "Loss: 1.0264392133145987e+25\n",
      "\n",
      "Iteration 121. Best: 4480.72314453125 kg\n",
      "Iteration 122 of 200\n",
      "Loss: 1.0245221354367384e+25\n",
      "\n",
      "Iteration 122. Best: 4480.72314453125 kg\n",
      "Iteration 123 of 200\n",
      "Loss: 1.0216386787537167e+25\n",
      "\n",
      "Iteration 123. Best: 4480.72314453125 kg\n",
      "Iteration 124 of 200\n",
      "Loss: 1.0205464009202522e+25\n",
      "\n",
      "Iteration 124. Best: 4480.72314453125 kg\n",
      "Iteration 125 of 200\n",
      "Loss: 1.019013245903426e+25\n",
      "\n",
      "Iteration 125. Best: 4480.72314453125 kg\n",
      "Iteration 126 of 200\n",
      "Loss: 1.0170823329675104e+25\n",
      "\n",
      "Iteration 126. Best: 4480.72314453125 kg\n",
      "Iteration 127 of 200\n",
      "Loss: 1.0142044103077108e+25\n",
      "\n",
      "Iteration 127. Best: 4480.72314453125 kg\n",
      "Iteration 128 of 200\n",
      "Loss: 1.0129262815277037e+25\n",
      "\n",
      "Iteration 128. Best: 4480.72314453125 kg\n",
      "Iteration 129 of 200\n",
      "Loss: 1.0114923930524241e+25\n",
      "\n",
      "Iteration 129. Best: 4480.72314453125 kg\n",
      "Iteration 130 of 200\n",
      "Loss: 1.0091705244342964e+25\n",
      "\n",
      "Iteration 130. Best: 4480.72314453125 kg\n",
      "Iteration 131 of 200\n",
      "Loss: 1.0073026763046829e+25\n",
      "\n",
      "Iteration 131. Best: 4480.72314453125 kg\n",
      "Iteration 132 of 200\n",
      "Loss: 1.014929482641958e+25\n",
      "\n",
      "Iteration 132. Best: 4480.72314453125 kg\n",
      "Iteration 133 of 200\n",
      "Loss: 1.015171480865775e+25\n",
      "\n",
      "Iteration 133. Best: 4480.72314453125 kg\n",
      "Iteration 134 of 200\n",
      "Loss: 1.0124747974664996e+25\n",
      "\n",
      "Iteration 134. Best: 4480.72314453125 kg\n",
      "Iteration 135 of 200\n",
      "Loss: 1.0101940881460864e+25\n",
      "\n",
      "Iteration 135. Best: 4480.72314453125 kg\n",
      "Iteration 136 of 200\n",
      "Loss: 1.0096886473584667e+25\n",
      "\n",
      "Iteration 136. Best: 4480.72314453125 kg\n",
      "Iteration 137 of 200\n",
      "Loss: 1.0077441299487968e+25\n",
      "\n",
      "Iteration 137. Best: 4480.72314453125 kg\n",
      "Iteration 138 of 200\n",
      "Loss: 1.016362448780034e+25\n",
      "\n",
      "Iteration 138. Best: 4480.72314453125 kg\n",
      "Iteration 139 of 200\n",
      "Loss: 1.0149053865825118e+25\n",
      "\n",
      "Iteration 139. Best: 4480.72314453125 kg\n",
      "Iteration 140 of 200\n",
      "Loss: 1.0129327378881295e+25\n",
      "\n",
      "Iteration 140. Best: 4480.72314453125 kg\n",
      "Iteration 141 of 200\n",
      "Loss: 1.0101746037726585e+25\n",
      "\n",
      "Iteration 141. Best: 4480.72314453125 kg\n",
      "Iteration 142 of 200\n",
      "Loss: 1.0096983318991054e+25\n",
      "\n",
      "Iteration 142. Best: 4480.72314453125 kg\n",
      "Iteration 143 of 200\n",
      "Loss: 1.0060554458209992e+25\n",
      "\n",
      "Iteration 143. Best: 4480.72314453125 kg\n",
      "Iteration 144 of 200\n",
      "Loss: 1.015505828102111e+25\n",
      "\n",
      "Iteration 144. Best: 4480.72314453125 kg\n",
      "Iteration 145 of 200\n",
      "Loss: 1.0142266616927497e+25\n",
      "\n",
      "Iteration 145. Best: 4480.72314453125 kg\n",
      "Iteration 146 of 200\n",
      "Loss: 1.0119228939422443e+25\n",
      "\n",
      "Iteration 146. Best: 4480.72314453125 kg\n",
      "Iteration 147 of 200\n",
      "Loss: 1.01132118420899e+25\n",
      "\n",
      "Iteration 147. Best: 4480.72314453125 kg\n",
      "Iteration 148 of 200\n",
      "Loss: 1.0088176151617363e+25\n",
      "\n",
      "Iteration 148. Best: 4480.72314453125 kg\n",
      "Iteration 149 of 200\n",
      "Loss: 1.0070078742759549e+25\n",
      "\n",
      "Iteration 149. Best: 4480.72314453125 kg\n",
      "Iteration 150 of 200\n",
      "Loss: 1.0153632117119912e+25\n",
      "\n",
      "Iteration 150. Best: 4480.72314453125 kg\n",
      "Iteration 151 of 200\n",
      "Loss: 1.0243917400145674e+25\n",
      "\n",
      "Iteration 151. Best: 4480.72314453125 kg\n",
      "Iteration 152 of 200\n",
      "Loss: 1.0226999429987073e+25\n",
      "\n",
      "Iteration 152. Best: 4480.72314453125 kg\n",
      "Iteration 153 of 200\n",
      "Loss: 1.0205843320377537e+25\n",
      "\n",
      "Iteration 153. Best: 4480.72314453125 kg\n",
      "Iteration 154 of 200\n",
      "Loss: 1.0190201634324536e+25\n",
      "\n",
      "Iteration 154. Best: 4480.72314453125 kg\n",
      "Iteration 155 of 200\n",
      "Loss: 1.017656833753256e+25\n",
      "\n",
      "Iteration 155. Best: 4480.72314453125 kg\n",
      "Iteration 156 of 200\n",
      "Loss: 1.0258368118284416e+25\n",
      "\n",
      "Iteration 156. Best: 4480.72314453125 kg\n",
      "Iteration 157 of 200\n",
      "Loss: 1.024033527303086e+25\n",
      "\n",
      "Iteration 157. Best: 4480.72314453125 kg\n",
      "Iteration 158 of 200\n",
      "Loss: 1.022561246541703e+25\n",
      "\n",
      "Iteration 158. Best: 4480.72314453125 kg\n",
      "Iteration 159 of 200\n",
      "Loss: 1.0200469553244565e+25\n",
      "\n",
      "Iteration 159. Best: 4480.72314453125 kg\n",
      "Iteration 160 of 200\n",
      "Loss: 1.0191969062991099e+25\n",
      "\n",
      "Iteration 160. Best: 4480.72314453125 kg\n",
      "Iteration 161 of 200\n",
      "Loss: 1.017080718877404e+25\n",
      "\n",
      "Iteration 161. Best: 4480.72314453125 kg\n",
      "Iteration 162 of 200\n",
      "Loss: 1.0153922653339073e+25\n",
      "\n",
      "Iteration 162. Best: 4480.72314453125 kg\n",
      "Iteration 163 of 200\n",
      "Loss: 1.0236149015047633e+25\n",
      "\n",
      "Iteration 163. Best: 4480.72314453125 kg\n",
      "Iteration 164 of 200\n",
      "Loss: 1.0225838438031934e+25\n",
      "\n",
      "Iteration 164. Best: 4480.72314453125 kg\n",
      "Iteration 165 of 200\n",
      "Loss: 1.0308032517938365e+25\n",
      "\n",
      "Iteration 165. Best: 4480.72314453125 kg\n",
      "Iteration 166 of 200\n",
      "Loss: 1.0287300683442525e+25\n",
      "\n",
      "Iteration 166. Best: 4480.72314453125 kg\n",
      "Iteration 167 of 200\n",
      "Loss: 1.0273758467449413e+25\n",
      "\n",
      "Iteration 167. Best: 4480.72314453125 kg\n",
      "Iteration 168 of 200\n",
      "Loss: 1.0257977277894354e+25\n",
      "\n",
      "Iteration 168. Best: 4480.72314453125 kg\n",
      "Iteration 169 of 200\n",
      "Loss: 1.0234643299562616e+25\n",
      "\n",
      "Iteration 169. Best: 4480.72314453125 kg\n",
      "Iteration 170 of 200\n",
      "Loss: 1.02194985226781e+25\n",
      "\n",
      "Iteration 170. Best: 4480.72314453125 kg\n",
      "Iteration 171 of 200\n",
      "Loss: 1.0201855364893102e+25\n",
      "\n",
      "Iteration 171. Best: 4480.72314453125 kg\n",
      "Iteration 172 of 200\n",
      "Loss: 1.0189215886438097e+25\n",
      "\n",
      "Iteration 172. Best: 4480.72314453125 kg\n",
      "Iteration 173 of 200\n",
      "Loss: 1.0164595247707218e+25\n",
      "\n",
      "Iteration 173. Best: 4480.72314453125 kg\n",
      "Iteration 174 of 200\n",
      "Loss: 1.0139790141535602e+25\n",
      "\n",
      "Iteration 174. Best: 4480.72314453125 kg\n",
      "Iteration 175 of 200\n",
      "Loss: 1.0128603344176402e+25\n",
      "\n",
      "Iteration 175. Best: 4480.72314453125 kg\n",
      "Iteration 176 of 200\n",
      "Loss: 1.0106267795867653e+25\n",
      "\n",
      "Iteration 176. Best: 4480.72314453125 kg\n",
      "Iteration 177 of 200\n",
      "Loss: 1.0191240416600187e+25\n",
      "\n",
      "Iteration 177. Best: 4480.72314453125 kg\n",
      "Iteration 178 of 200\n",
      "Loss: 1.0265622300391402e+25\n",
      "\n",
      "Iteration 178. Best: 4480.72314453125 kg\n",
      "Iteration 179 of 200\n",
      "Loss: 1.0258762417438992e+25\n",
      "\n",
      "Iteration 179. Best: 4480.72314453125 kg\n",
      "Iteration 180 of 200\n",
      "Loss: 1.0245043804455675e+25\n",
      "\n",
      "Iteration 180. Best: 4480.72314453125 kg\n",
      "Iteration 181 of 200\n",
      "Loss: 1.0229714560130422e+25\n",
      "\n",
      "Iteration 181. Best: 4480.72314453125 kg\n",
      "Iteration 182 of 200\n",
      "Loss: 1.0203057862022407e+25\n",
      "\n",
      "Iteration 182. Best: 4480.72314453125 kg\n",
      "Iteration 183 of 200\n",
      "Loss: 1.0196563455186957e+25\n",
      "\n",
      "Iteration 183. Best: 4480.72314453125 kg\n",
      "Iteration 184 of 200\n",
      "Loss: 1.017351770723137e+25\n",
      "\n",
      "Iteration 184. Best: 4480.72314453125 kg\n",
      "Iteration 185 of 200\n",
      "Loss: 1.015246305471424e+25\n",
      "\n",
      "Iteration 185. Best: 4480.72314453125 kg\n",
      "Iteration 186 of 200\n",
      "Loss: 1.0139956162232265e+25\n",
      "\n",
      "Iteration 186. Best: 4480.72314453125 kg\n",
      "Iteration 187 of 200\n",
      "Loss: 1.0119435312371768e+25\n",
      "\n",
      "Iteration 187. Best: 4480.72314453125 kg\n",
      "Iteration 188 of 200\n",
      "Loss: 1.0114499655410546e+25\n",
      "\n",
      "Iteration 188. Best: 4480.72314453125 kg\n",
      "Iteration 189 of 200\n",
      "Loss: 1.0095989500654083e+25\n",
      "\n",
      "Iteration 189. Best: 4480.72314453125 kg\n",
      "Iteration 190 of 200\n",
      "Loss: 1.0073133984746757e+25\n",
      "\n",
      "Iteration 190. Best: 4480.72314453125 kg\n",
      "Iteration 191 of 200\n",
      "Loss: 1.0156431410533097e+25\n",
      "\n",
      "Iteration 191. Best: 4480.72314453125 kg\n",
      "Iteration 192 of 200\n",
      "Loss: 1.0142966440280794e+25\n",
      "\n",
      "Iteration 192. Best: 4480.72314453125 kg\n",
      "Iteration 193 of 200\n",
      "Loss: 1.0126224867112398e+25\n",
      "\n",
      "Iteration 193. Best: 4480.72314453125 kg\n",
      "Iteration 194 of 200\n",
      "Loss: 1.0107630549086098e+25\n",
      "\n",
      "Iteration 194. Best: 4480.72314453125 kg\n",
      "Iteration 195 of 200\n",
      "Loss: 1.0099764165660166e+25\n",
      "\n",
      "Iteration 195. Best: 4480.72314453125 kg\n",
      "Iteration 196 of 200\n",
      "Loss: 1.007847777592061e+25\n",
      "\n",
      "Iteration 196. Best: 4480.72314453125 kg\n",
      "Iteration 197 of 200\n",
      "Loss: 1.0049557892899052e+25\n",
      "\n",
      "Iteration 197. Best: 4480.72314453125 kg\n",
      "Iteration 198 of 200\n",
      "Loss: 1.0048144411134404e+25\n",
      "\n",
      "Iteration 198. Best: 4480.72314453125 kg\n",
      "Iteration 199 of 200\n",
      "Loss: 1.0031742949809867e+25\n",
      "\n",
      "Iteration 199. Best: 4480.72314453125 kg\n",
      "Iteration 200 of 200\n",
      "Loss: 1.0012719744983854e+25\n",
      "\n",
      "Iteration 200. Best: 4480.72314453125 kg\n"
     ]
    }
   ],
   "source": [
    "while it < N_INFILL:\n",
    "    it += 1\n",
    "    \n",
    "    # Search for the maximum expected improvement\n",
    "    new_point = bsa(expected_improvement, low=LOWER_BOUNDS, up=UPPER_BOUNDS,\n",
    "                    popsize=BSA_POPSIZE, epoch=BSA_EPOCH, data=model)\n",
    "    x_new = torch.from_numpy(new_point.x)\n",
    "    EI = new_point.y\n",
    "\n",
    "    # Objective function at the new point\n",
    "    f_new = obj_fun(x=x_new)\n",
    "    \n",
    "    print(f'Iteration {it} of {N_INFILL}')\n",
    "    if f_new < torch.min(f): print(f'New best: {f_new:.2f} at position {it}')\n",
    "    \n",
    "    # Add new values to the initial sampling\n",
    "    x = torch.cat((x, torch.from_numpy(np.array([np.asarray(x_new)]))), 0)\n",
    "    x = x.to(torch.float32)\n",
    "    f = torch.cat((f, f_new), 0)\n",
    "    \n",
    "    # Update model\n",
    "    model, likelihood = train_model(x, f, TRAINING_ITERATIONS)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    print(f'\\nIteration {it}. Best: {torch.min(f)} kg')\n",
    "    \n",
    "    # if abs(EI) < TOL_MIN_EI:\n",
    "    #     print('Optimization finished. Minimum tolerance achieved.')\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results[\"x\"] = x\n",
    "results[\"f\"] = f\n",
    "results[\"n_initial\"] = N_INITIAL\n",
    "results[\"n_infill\"] = N_INFILL\n",
    "results[\"OFEs\"] = N_INITIAL + it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f*: 4480.72314453125 kg; x*: tensor([0.7097, 0.5206, 0.7261, 0.2552, 0.5334, 0.1589, 0.0682, 0.9707, 0.6417,\n",
      "        0.7202, 0.3730, 0.8859, 0.2567, 0.7863, 0.8893, 0.6273, 0.8870, 0.6643,\n",
      "        0.3576, 0.7533, 0.1121, 0.6268, 0.4831, 0.7478, 0.7098, 0.5823, 0.0339,\n",
      "        0.6778, 0.8060, 0.4855, 0.3113, 0.6491, 0.1007, 0.7651, 0.4384, 0.0853,\n",
      "        0.0815, 0.0304, 0.7789, 0.2740, 0.6065, 0.0547, 0.7040, 0.7034, 0.0755,\n",
      "        0.7931, 0.5590, 0.3406, 0.6881, 0.9228, 0.9669, 0.7053])\n"
     ]
    }
   ],
   "source": [
    "print(f'f*: {torch.min(f)} kg; x*: {x[torch.argmin(f), :]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BSA_POPSIZE = 12  # Ryzen 5 3600x (números múltiplos de 12)\n",
    "BSA_EPOCH = 200  # depende do tempo computacional\n",
    "results_BSA = bsa(obj_fun, low=LOWER_BOUNDS, up=UPPER_BOUNDS,\n",
    "                    popsize=BSA_POPSIZE, epoch=BSA_EPOCH, data=[])\n",
    "print(f'f*: {results_BSA.y} kg; x*: {results_BSA.x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(\n",
    "         results_BSA.convergence.shape[0]),\n",
    "         results_BSA.convergence,\n",
    "         linewidth=2.5)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "# plt.ylim([ymin, ymax])\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 9)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
