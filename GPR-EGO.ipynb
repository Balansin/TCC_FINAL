{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Global Optimization employing Deep Gaussian Process Regression\n",
    "\n",
    "See DGPR implementations in:\n",
    "\n",
    "https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/KISSGP_Deep_Kernel_Regression_CUDA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import erf\n",
    "import scipy.io as sio\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, init\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.optim import Adam\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel, RBFKernel\n",
    "from gpytorch.utils.grid import ScaleToBounds\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.settings import use_toeplitz, fast_pred_var\n",
    "from gpytorch.constraints.constraints import GreaterThan\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from bsa import bsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to launch MAPDL on port 50052: \n",
      "Lock file exists for jobname \"file\" at\n",
      "\"C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\ansys_elyfryqatk\\file.lock\"\n",
      "\n",
      "Set ``override=True`` to or delete the lock file to start MAPDL\n",
      "Failed to launch MAPDL on port 50053: \n",
      "Lock file exists for jobname \"file\" at\n",
      "\"C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\ansys_padlzjhbhs\\file.lock\"\n",
      "\n",
      "Set ``override=True`` to or delete the lock file to start MAPDL\n",
      "Failed to launch MAPDL on port 50054: \n",
      "Lock file exists for jobname \"file\" at\n",
      "\"C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\ansys_ccxpcyryee\\file.lock\"\n",
      "\n",
      "Set ``override=True`` to or delete the lock file to start MAPDL\n",
      "MAPDL launched successfully on port 50055\n"
     ]
    }
   ],
   "source": [
    "from ansys.mapdl.core import launch_mapdl\n",
    "def launch_mapdl_on_available_port(starting_port=50052, max_attempts=5):\n",
    "    for i in range(max_attempts):\n",
    "        port = starting_port + i\n",
    "        try:\n",
    "            mapdl = launch_mapdl(port=port)\n",
    "            print(f\"MAPDL launched successfully on port {port}\")\n",
    "            return mapdl\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to launch MAPDL on port {port}: {e}\")\n",
    "    raise RuntimeError(\"Could not launch MAPDL on any available port\")\n",
    "\n",
    "# Use a função para iniciar uma instância do MAPDL\n",
    "mapdl = launch_mapdl_on_available_port()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 42 # CHANGE SEED IN EACH DIFFERENT INDEPENDENT RUN!\n",
    "\n",
    "# For NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# For PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "\n",
    "# For Python's built-in random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensuring reproducibility in cuDNN using PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *GPR Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "        self.scale_to_bounds = ScaleToBounds(-1., 1.)\n",
    "        \n",
    "        # Store train_y for later use\n",
    "        self.train_outputs = train_y\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.scale_to_bounds(x)\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_g, val_x, val_g, training_iterations):\n",
    "    # Initialize the models and likelihood\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x=train_x, train_y=train_g, likelihood=likelihood)\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.cuda()\n",
    "    #     likelihood = likelihood.cuda()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = Adam([\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.005)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # To track loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Training loop with validation\n",
    "    def train():\n",
    "        best_loss, best_val_loss, best_train_loss = 1e4, 1e4, 1e4\n",
    "        patience = int(training_iterations * 0.1)\n",
    "        wait = 0\n",
    "\n",
    "        for epoch in range(training_iterations):\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass and calculate loss on training set\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_g)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            likelihood.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = model(val_x)\n",
    "                val_loss = -mll(val_output, val_g).item()\n",
    "\n",
    "            # Save the best model based on validation and training loss\n",
    "            training_loss = loss.item()\n",
    "            final_loss = val_loss # val_loss*0.5 + training_loss*0.5\n",
    "\n",
    "            print(\"final_loss\",final_loss)\n",
    "            print(\"best_loss\",best_loss)\n",
    "            \n",
    "            if final_loss < best_loss:\n",
    "                best_loss = final_loss\n",
    "                best_val_loss = val_loss\n",
    "                best_train_loss = training_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'best_model_GPR.pth')\n",
    "                wait = 0  # Reset patience counter when improvement is found\n",
    "            else:\n",
    "                wait += 1  # Increment patience counter if no improvement\n",
    "\n",
    "            # Early stopping\n",
    "            if wait > patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "            # Track losses for plotting\n",
    "            training_losses.append(loss.item())\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            # print(f'Epoch {epoch + 1} - Training Loss: {loss.item()} - Validation Loss: {val_loss}')\n",
    "\n",
    "        print(f'Best Loss: {best_loss} at epoch {best_epoch}. Training loss: {best_train_loss} and val. loss: {best_val_loss}')\n",
    "        return best_val_loss, best_train_loss\n",
    "    best_loss = train()\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load('best_model_GPR.pth'))\n",
    "\n",
    "    # Set model and likelihood to eval mode for further evaluation\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model, likelihood, best_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial sampling plan (Latin Hypercube sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LHS_sample(num_points, DIM):\n",
    "    # Number of variables and sampling points\n",
    "    size = int(num_points)\n",
    "\n",
    "    # Generate LHS samples\n",
    "    sampler = qmc.LatinHypercube(d=DIM,\n",
    "                                 optimization=\"random-cd\")\n",
    "    lhs_sample = sampler.random(n=size)\n",
    "\n",
    "    return lhs_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, model):\n",
    "    \"\"\"\n",
    "    Function to calculate the Expected Improvement using Monte Carlo Integration.\n",
    "    \n",
    "    Parameters:\n",
    "        x (array): Individual under evaluation.\n",
    "        y (array): Valor mínimo obtido até o momento.\n",
    "    \n",
    "    Returns:\n",
    "        array: The value of the Expected Improvement.\n",
    "    \"\"\"\n",
    "    # Get the minimum value obtained so far\n",
    "    ymin = torch.min(model.train_outputs)\n",
    "    # ymin = torch.Tensor(ymin)\n",
    "\n",
    "    # Calculate the prediction value and the variance (Ssqr)\n",
    "    with torch.no_grad(), use_toeplitz(False), fast_pred_var():\n",
    "        preds = model(x)\n",
    "    f = preds.mean\n",
    "    s = preds.variance\n",
    "\n",
    "    # Check for any errors that are less than zero (due to numerical error)\n",
    "    s[s < 0] = 0  # Set negative variances to zero\n",
    "\n",
    "    # Calculate the RMSE (Root Mean Square Error)\n",
    "    s = torch.sqrt(s)\n",
    "\n",
    "    # Calculation of Expected Improvement\n",
    "    term1 = (ymin - f) * (0.5 + 0.5 * erf((ymin - f) / (s * torch.sqrt( torch.from_numpy(np.array([2])) ))))\n",
    "    term2 = (1 / torch.sqrt(2 * torch.from_numpy(np.array([np.pi])))) * s * torch.exp(-((ymin - f) ** 2) / (2 * s ** 2))\n",
    "    \n",
    "    return -(term1 + term2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 10000  # epochs for training the GPR\n",
    "N_INITIAL = 50  # initial number of points\n",
    "N_INFILL = 500  # number of infill points\n",
    "BSA_POPSIZE = 50\n",
    "BSA_EPOCH = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 11 # dimension of your problem (number of design variables)\n",
    "BOUNDS_BSA = (\n",
    "    tuple((0, 1) for _ in range(DIM)) # the number of (0, 1) tuples has to be equal to DIM\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "Your objective function must go inside obj_fun. The input is *xx*, your design variables vector, and the output is the objective function value (your own cost function).\n",
    "\n",
    "You can either import your objective function from a *.py* file using `import obj_fun from my_file` (where *my_file.py* contains your objective function that is called *obj_fun*, already organized to receive a design variable vector (or matrix) as input and output an objective function value) or just replace the *obj_fun* code below by your entire code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Definindo variáveis apenas uma vez\n",
    "raio_estaca = 0.5\n",
    "distancia_minima = raio_estaca * 2.5  # 5 vezes o raio da estaca\n",
    "largura = 6\n",
    "altura = 3\n",
    "n_estacas = 2\n",
    "n_estacas_centrais = round(1) \n",
    "L_estacas=20\n",
    "\n",
    "#FUNÇÕES AUXILIARES\n",
    "def round_to_nearest(value, targets=[0, 0.333, 0.666, 1]): #Arrendonda o valor de angulo vertical para o valor mais próximo a fim de trabalhar dentro de limites construtivos de equipamento e também trabalhar com um menor espaço amostral                    \n",
    "    return min(targets, key=lambda t: abs(value - t))\n",
    "\n",
    "def distancia_entre_pontos(p1, p2): #Mede a distancia entre pontos a fim de assegurar limites geométricos da estrutura\n",
    "    return math.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "\n",
    "def espelhar_pontos(coordinates, n_estacas_centrais): \n",
    "    espelho = [(x, -y, z) for x, y, z in coordinates[:-int(n_estacas_centrais)]]\n",
    "    return espelho\n",
    "\n",
    "def converter_x(x_normalizado): #Converte os valores do de interesse em coordenadas \"X\"\n",
    "    return raio_estaca + x_normalizado * (largura-2*raio_estaca)\n",
    "\n",
    "def converter_y(y_normalizado): #Converte os valores do de interesse em coordenadas \"Y\"\n",
    "    return raio_estaca + y_normalizado * (altura-2*raio_estaca)\n",
    "\n",
    "def converter_angulos_verticais(array_angulos_verticais): #Converte os valores do de interesse em angulos trigonométricos\n",
    "    return [np.round(75 + 15 * ang_v, 2) for ang_v in array_angulos_verticais]\n",
    "\n",
    "def converter_angulos_horizontais(array_angulos_horizontais): #Converte os valores do de interesse em angulos trigonométricos\n",
    "    return [np.round(46 + 268 * ang_h, 2) for ang_h in array_angulos_horizontais]\n",
    "\n",
    "def spherical_to_cartesian(x_inicial, y_inicial, z_inicial, L_estacas, angulos_verticais_theta, angulos_horizontais_theta): #Converte os valores do de interesse em coordendas cartesianas\n",
    "    angulo_vertical_rad = np.deg2rad(angulos_verticais_theta)\n",
    "    angulo_horizontal_rad = np.deg2rad(angulos_horizontais_theta)\n",
    "    \n",
    "    componente_x = L_estacas * np.cos(angulo_vertical_rad) * np.cos(angulo_horizontal_rad)\n",
    "    componente_y = L_estacas * np.cos(angulo_vertical_rad) * np.sin(angulo_horizontal_rad)\n",
    "    componente_z = L_estacas * np.sin(angulo_vertical_rad)\n",
    "    \n",
    "    x_final = x_inicial + componente_x\n",
    "    y_final = y_inicial + componente_y\n",
    "    z_final = z_inicial + componente_z\n",
    "\n",
    "    return x_final, y_final, z_final\n",
    "\n",
    "def gerar_pontos_finais(pontos, angulos_verticais_theta, angulos_horizontais_theta): #Gera pontos finais\n",
    "    coordenadas_finais = []\n",
    "    for i in range(len(pontos)):\n",
    "        x, y, z = pontos[i]\n",
    "        theta_v, theta_h = angulos_verticais_theta[i], angulos_horizontais_theta[i]\n",
    "        x_final, y_final, z_final = spherical_to_cartesian(x, y, z, L_estacas, theta_v, theta_h)\n",
    "        coordenadas_finais.append((round(x_final, 2), round(y_final, 2), -1*round(z_final, 2)))\n",
    "    return coordenadas_finais\n",
    "\n",
    "def espelhar_pontos_finais(pontos, k):\n",
    "    espelho = []\n",
    "    for (x, y, z) in pontos[:-k]:\n",
    "        y_espelhado = -y\n",
    "        espelho.append((x, y_espelhado, z))\n",
    "    return espelho\n",
    "\n",
    "def distancia_entre_pontos_r3(p1, p2): #Garante limites geométricos/normatívos da estrutura\n",
    "    return round(np.linalg.norm(p1 - p2), 2)\n",
    "\n",
    "def encontrar_menor_distancia_entre_vetores(pontos_iniciais, pontos_finais, k, l, comprimento): #Garante limites geométricos/normatívos da estrutura\n",
    "    vetor_k = pontos_finais[k] - pontos_iniciais[k]\n",
    "    vetor_l = pontos_finais[l] - pontos_iniciais[l]\n",
    "\n",
    "    vetor_dividido_k = vetor_k / comprimento\n",
    "    vetor_dividido_l = vetor_l / comprimento\n",
    "\n",
    "    menor_distancia = float('inf')\n",
    "    ponto_menor_distancia_v1 = None\n",
    "    ponto_menor_distancia_v2 = None\n",
    "    N_v1 = None\n",
    "    N_v2 = None\n",
    "\n",
    "    for N1 in np.arange(1, 19, 1):\n",
    "        for N2 in np.arange(1, 19, 1):\n",
    "            ponto_v1 = pontos_iniciais[k] + N1 * vetor_dividido_k\n",
    "            ponto_v2 = pontos_iniciais[l] + N2 * vetor_dividido_l\n",
    "\n",
    "            dist = round(distancia_entre_pontos_r3(ponto_v1, ponto_v2), 2)\n",
    "\n",
    "            if dist < menor_distancia:\n",
    "                menor_distancia = dist\n",
    "                ponto_menor_distancia_v1 = ponto_v1\n",
    "                ponto_menor_distancia_v2 = ponto_v2\n",
    "                N_v1 = N1\n",
    "                N_v2 = N2\n",
    "\n",
    "    return menor_distancia, ponto_menor_distancia_v1, ponto_menor_distancia_v2, N_v1, N_v2\n",
    "\n",
    "def gerar_lista_k(array_angulos_verticais, n_estacas, n_estacas_centrais): #Gera lista auxiliar\n",
    "    indices = list(range(len(array_angulos_verticais)))\n",
    "    indices_menores = sorted(range(len(array_angulos_verticais) - 1), key=lambda i: array_angulos_verticais[i])[:n_estacas]\n",
    "    lista_k = [i for i in indices_menores]\n",
    "    lista_k.append(2)\n",
    "    if len(lista_k) < n_estacas + n_estacas_centrais and lista_k.count(2) < 2:\n",
    "        lista_k.append(2)\n",
    "    return lista_k\n",
    "\n",
    "#Garante limites geométricos/normatívos da estrutura, tentando otimizar brevemente a estrutura, caso não seja possível, penaliza ela\n",
    "def verificar_e_ajustar_pontos(pontos, L_estacas, distancia_minima, pontos_finais, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos, array_angulos_horizontais,p):\n",
    "    max_iteracoes=1*(n_estacas+n_estacas_centrais+n_estacas)\n",
    "    iteracao = 0\n",
    "    pontos_finais = np.array(pontos_finais)\n",
    "    incremento = 0.05\n",
    "    lista_k = gerar_lista_k(array_angulos_verticais_convertidos, n_estacas, n_estacas_centrais)\n",
    "    trial=5\n",
    "\n",
    "    for k in lista_k:\n",
    "        while iteracao <= max_iteracoes:\n",
    "            ajustou_ponto = False\n",
    "\n",
    "            for l in range(len(pontos_finais)):\n",
    "                if k == l:\n",
    "                    continue\n",
    "                ponto1 = pontos_finais[k]\n",
    "                ponto2 = pontos_finais[l]\n",
    "                dist = distancia_entre_pontos_r3(ponto1, ponto2)\n",
    "                menor_distancia, _, _, _, _ = encontrar_menor_distancia_entre_vetores(pontos, pontos_finais, k, l, L_estacas)\n",
    "                if menor_distancia < 3 * raio_estaca or dist < 5 * raio_estaca:\n",
    "                    ajustou_ponto = True\n",
    "                    if k <= n_estacas:\n",
    "                        array_angulos_horizontais_convertidos[k] = np.round(array_angulos_horizontais_convertidos[k]+incremento,2)\n",
    "                        array_angulos_horizontais[k] = np.round(array_angulos_horizontais[k]+incremento,2)\n",
    "                    else:\n",
    "                        # Alterna entre 0/180 e 0/1 usando (-1)**trial\n",
    "                        if (-1) ** trial == 1:\n",
    "                            array_angulos_horizontais_convertidos[k] = 180\n",
    "                            array_angulos_horizontais[k] = 1\n",
    "                        else:\n",
    "                            array_angulos_horizontais_convertidos[k] = 0\n",
    "                            array_angulos_horizontais[k] = 0\n",
    "\n",
    "                    x_novo, y_novo, z_novo = spherical_to_cartesian(\n",
    "                        pontos[k][0], pontos[k][1], pontos[k][2],\n",
    "                        L_estacas, array_angulos_verticais_convertidos[k], array_angulos_horizontais_convertidos[k]\n",
    "                    )\n",
    "                    x = np.round(x_novo, 2)\n",
    "                    y = np.round(y_novo, 2)\n",
    "                    z = -1*np.round(z_novo, 2)\n",
    "\n",
    "                    if k < n_estacas:\n",
    "                        pontos_finais[k, :] = [x, y, z]\n",
    "                        pontos_finais[k + n_estacas + n_estacas_centrais, :] = [x, -y, z]\n",
    "                    else:\n",
    "                        pontos_finais[k, :] = [x, y, z]\n",
    "                    break\n",
    "\n",
    "            if not ajustou_ponto:\n",
    "                break\n",
    "            trial += 1\n",
    "            \n",
    "            if trial >= max_iteracoes:\n",
    "                p+= ((3 * raio_estaca)/(0.1+menor_distancia)+(5 * raio_estaca)/(0.1+dist))*10\n",
    "                k+=1\n",
    "                iteracao+=1\n",
    "\n",
    "                break\n",
    "\n",
    "    return pontos_finais, array_angulos_horizontais_convertidos, array_angulos_horizontais,p\n",
    "\n",
    "def analise(pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais):#Analise estrutural dentro do ansys\n",
    "    mapdl.clear('NOSTART')\n",
    "    mapdl.prep7()\n",
    "\n",
    "    # Título\n",
    "    mapdl.title('Análise de Estacas e Casca')\n",
    "\n",
    "    # Definir o tipo de elemento (BEAM188) e suas propriedades\n",
    "    mapdl.et(1, 'BEAM188')\n",
    "\n",
    "    # Propriedades do material\n",
    "    modulo_elasticidade = 0.85 * 5600 * (40 ** 0.5) * 1e6  # N/m²\n",
    "    mapdl.mp('EX', 1, modulo_elasticidade)\n",
    "    mapdl.mp('PRXY', 1, 0.2)  # Coeficiente de Poisson\n",
    "    mapdl.mp('DENS', 1, 2500)  # Densidade\n",
    "\n",
    "    # Propriedades da seção da viga\n",
    "    mapdl.sectype(1, 'BEAM', 'CSOLID')\n",
    "    mapdl.secoffset('CENT')\n",
    "    mapdl.secdata(0.5)\n",
    "\n",
    "    # Número de nós intermediários\n",
    "    num_intermediate_nodes = 9\n",
    "\n",
    "    # Adicionar nós\n",
    "    node_id = 1\n",
    "    for i in range(2 * n_estacas + n_estacas_centrais):\n",
    "        # Obter coordenadas iniciais e finais\n",
    "        x_inicial, y_inicial, z_inicial = pontos_iniciais[i]\n",
    "        x_final, y_final, z_final = pontos_finais_ajustados[i]\n",
    "\n",
    "        # Criar o nó inicial\n",
    "        mapdl.n(node_id, x_inicial, y_inicial, z_inicial)\n",
    "\n",
    "        # Definir o ID do nó final\n",
    "        node_id_final = node_id + num_intermediate_nodes + 1\n",
    "\n",
    "        # Criar o nó final\n",
    "        mapdl.n(node_id_final, x_final, y_final, z_final)\n",
    "\n",
    "        # Travar o nó final\n",
    "        mapdl.d(node_id_final, 'ALL', 0)\n",
    "\n",
    "        # Preencher nós intermediários\n",
    "        mapdl.fill(node_id, node_id_final, num_intermediate_nodes)\n",
    "\n",
    "        # Atualizar o próximo node_id\n",
    "        node_id = node_id_final + 1\n",
    "\n",
    "    # Geração de elementos\n",
    "    n_elemento = 1\n",
    "    for i in range(2 * n_estacas + n_estacas_centrais):\n",
    "        for j in range(1, num_intermediate_nodes + 2):\n",
    "            N_1 = j + (num_intermediate_nodes + 2) * i\n",
    "            N_2 = N_1 + 1\n",
    "            mapdl.en(n_elemento, N_1, N_2)\n",
    "            n_elemento += 1\n",
    "\n",
    "    # Selecionar elementos tipo BEAM188\n",
    "    mapdl.esel('S', 'TYPE', '', 1)\n",
    "\n",
    "    # Contar elementos selecionados\n",
    "    num_elem = mapdl.get('num_elem', 'ELEM', 0, 'COUNT')\n",
    "    k = num_elem\n",
    "\n",
    "    # Criar elemento de carga\n",
    "    mapdl.n(1000, 3, 0, 0)\n",
    "    mapdl.n(1001, 3, 0, 0.1)\n",
    "    mapdl.en(k + 1, 1000, 1001)\n",
    "\n",
    "    # Definir tipo de elemento SHELL181\n",
    "    mapdl.et(2, 'SHELL181')\n",
    "    mapdl.keyopt(2, 8, 2)  # Elastoplástico\n",
    "    mapdl.keyopt(2, 3, 2)  # Precisão de tensões\n",
    "\n",
    "    # Propriedades do material para SHELL181\n",
    "    modulo_elasticidade_shell = 0.85 * 5600 * (20 ** 0.5) * 1e6\n",
    "    mapdl.mp('EX', 2, modulo_elasticidade_shell)\n",
    "    mapdl.mp('PRXY', 2, 0.2)\n",
    "    mapdl.mp('DENS', 2, 2500)\n",
    "\n",
    "    # Definir seção de casca\n",
    "    mapdl.sectype(2, 'SHELL')\n",
    "    mapdl.secdata(1.5)\n",
    "\n",
    "    # Criar retângulo e malhar\n",
    "    mapdl.rectng(0, 6, -3, 3)\n",
    "    mapdl.esize(0.1)\n",
    "    mapdl.amesh('ALL')\n",
    "\n",
    "    # Selecionar elementos tipo SHELL181\n",
    "    mapdl.esel('S', 'TYPE', '', 2)\n",
    "    mapdl.emodif('ALL', 'SECNUM', 2)\n",
    "\n",
    "    # Merge de nós\n",
    "    mapdl.nsel('S', 'LOC', 'Z', 0, 1e5)\n",
    "    mapdl.nummrg('NODE', 1e-5)\n",
    "    mapdl.nsel('ALL')\n",
    "\n",
    "    # Saída dos resultados\n",
    "    mapdl.allsel('ALL')\n",
    "\n",
    "    # Aplicar gravidade\n",
    "    mapdl.acel(0, 0, -9.81)\n",
    "\n",
    "    # Finalizar\n",
    "    mapdl.finish()\n",
    "\n",
    "    # Entrar no modo de solução\n",
    "    mapdl.slashsolu()\n",
    "    mapdl.antype(0)\n",
    "\n",
    "    # Definir a força aplicada e o número de load steps\n",
    "    f = 1000000  # Força de 1 milhão de N\n",
    "    casos = 12  # Definir loadsteps\n",
    "\n",
    "    # Loop para aplicar as diferentes condições de carga (12 casos de 15 em 15 graus)\n",
    "    for i in range(1, casos + 2):\n",
    "        Rad = (i - 1) * (180 / casos) * math.pi / 180  # Variação de 15 graus\n",
    "\n",
    "        # Calcular as componentes da força horizontal\n",
    "        FX = math.cos(Rad) * f\n",
    "        FY = math.sin(Rad) * f\n",
    "\n",
    "        # Selecionar todos os elementos\n",
    "        mapdl.allsel('ALL')\n",
    "\n",
    "        # Aplicar as forças horizontais no nó 1001\n",
    "        mapdl.f(1001, 'FX', FX)\n",
    "        mapdl.f(1001, 'FY', FY)\n",
    "\n",
    "        # Resolver o modelo\n",
    "        mapdl.solve()\n",
    "\n",
    "        # Salvar os resultados da solução\n",
    "        mapdl.save(f'load_step_{i}')\n",
    "\n",
    "    # Finalizar a análise\n",
    "    mapdl.finish()\n",
    "\n",
    "    # Definir o número de casos\n",
    "    casos = 12\n",
    "\n",
    "    # Entrar no modo de pós-processamento\n",
    "    mapdl.post1()\n",
    "\n",
    "    # Definir os Load Cases para N = 12\n",
    "    for i in range(1, casos + 2):\n",
    "        mapdl.lcdef(i, i, 1)  # Define o Load Case i para o Load Step i\n",
    "\n",
    "    # Carregar o primeiro Load Case\n",
    "    mapdl.lcase(1)\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!CÁLCULO DO MAX!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Comparar o Load Case 1 com os demais e armazenar os resultados (máximos)\n",
    "    for R in range(2, casos + 2):\n",
    "        mapdl.lcoper('MAX', R)  # Compara o Load Case na memória com os próximos\n",
    "        mapdl.lcwrite(50 + R)   # Escreve o resultado em um arquivo\n",
    "\n",
    "    mapdl.lcase(50 + (casos + 1))  # Carregar o último load case comparado\n",
    "    mapdl.etable('Fx_MAX', 'SMISC', 1, 'MAX')  # Força Axial máxima\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!CÁLCULO DO MIN!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Carregar o primeiro Load Case novamente\n",
    "    mapdl.lcase(1)\n",
    "\n",
    "    # Comparar o Load Case 1 com os demais para valores mínimos\n",
    "    for R in range(2, casos + 2):\n",
    "        mapdl.lcoper('MIN', R)  # Compara o Load Case na memória para valores mínimos\n",
    "        mapdl.lcwrite(60 + R)   # Escreve o resultado em um arquivo\n",
    "\n",
    "    mapdl.lcase(60 + (casos + 1))  # Carregar o último load case comparado\n",
    "    mapdl.etable('Fx_MIN', 'SMISC', 1, 'MIN')  # Força Axial mínima\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!CÁLCULO DO ABSMAX!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Carregar o primeiro Load Case\n",
    "    mapdl.lcase(1)\n",
    "\n",
    "    # Comparar o Load Case 1 com os demais e armazenar os resultados (máximos absolutos)\n",
    "    for R in range(2, casos + 2):\n",
    "        mapdl.lcoper('ABMX', R)  # Compara o Load Case na memória com os próximos\n",
    "        mapdl.lcwrite(50 + R)   # Escreve o resultado em um arquivo\n",
    "\n",
    "    mapdl.lcase(50 + (casos + 1))  # Carregar o último load case comparado\n",
    "    mapdl.etable('My_ABMX', 'SMISC', 2, 'ABMX')  # Momento fletor máximo em Y\n",
    "    mapdl.etable('Mz_ABMX', 'SMISC', 3, 'ABMX')  # Momento fletor máximo em Z\n",
    "\n",
    "    # Gerar ETABLE para deslocamentos dos nós selecionados\n",
    "    mapdl.etable('UX', 'U', 'X')  # Deslocamento em X\n",
    "    mapdl.etable('UY', 'U', 'Y')  # Deslocamento em Y\n",
    "    mapdl.etable('UZ', 'U', 'Z')  # Deslocamento em Z\n",
    "\n",
    "    # Definir a sequência de elementos\n",
    "    elementos_sequencia = []\n",
    "    j = 1\n",
    "    for i in range(0, (2 * n_estacas) + n_estacas_centrais, 1):\n",
    "        y = num_intermediate_nodes * i + j\n",
    "        elementos_sequencia.append(y)\n",
    "        j += 1\n",
    "\n",
    "    # Dicionário para armazenar os resultados\n",
    "    data = {\n",
    "        \"Fx_MAX\": [],\n",
    "        \"Fx_MIN\": [],\n",
    "        \"Utot\": [],\n",
    "        \"My\": [],\n",
    "        \"Mz\": [],\n",
    "        \"r_max\": [],\n",
    "        \"load_case_max\": []\n",
    "    }\n",
    "\n",
    "    # Primeiro loop: Calculando Fx_MAX, Fx_MIN e Utot para cada elemento\n",
    "    for elem in elementos_sequencia:\n",
    "        fx_max = mapdl.get_value('ELEM', elem, 'ETABLE', 'Fx_MAX') / 1000  # kN\n",
    "        fx_min = mapdl.get_value('ELEM', elem, 'ETABLE', 'Fx_MIN') / 1000  # kN\n",
    "        ux = mapdl.get_value('ELEM', elem, 'ETABLE', 'UX') * 1000  # mm\n",
    "        uy = mapdl.get_value('ELEM', elem, 'ETABLE', 'UY') * 1000  # mm\n",
    "        uz = mapdl.get_value('ELEM', elem, 'ETABLE', 'UZ') * 1000  # mm\n",
    "        Utot = math.sqrt(ux**2 + uy**2 + uz**2)\n",
    "\n",
    "        # Adicionando os valores calculados ao dicionário\n",
    "        data[\"Fx_MAX\"].append(fx_max)\n",
    "        data[\"Fx_MIN\"].append(fx_min)\n",
    "        data[\"Utot\"].append(Utot)\n",
    "\n",
    "    # Segundo loop: Calculando r_max, My, e Mz para cada elemento em todos os load cases\n",
    "    for elem in elementos_sequencia:\n",
    "        r_max_elem = 0\n",
    "        my_max_elem = 0\n",
    "        mz_max_elem = 0\n",
    "        load_case_max = 0\n",
    "\n",
    "        for load_case in range(1, casos + 2):\n",
    "            mapdl.lcase(load_case)\n",
    "\n",
    "            mapdl.etable('My', 'SMISC', 2)\n",
    "            mapdl.etable('Mz', 'SMISC', 3)\n",
    "\n",
    "            my_val = mapdl.get_value('ELEM', elem, 'ETABLE', 'My') / 1000\n",
    "            mz_val = mapdl.get_value('ELEM', elem, 'ETABLE', 'Mz') / 1000\n",
    "\n",
    "            r_val = math.sqrt(my_val**2 + mz_val**2)\n",
    "\n",
    "            if r_val > r_max_elem:\n",
    "                r_max_elem = r_val\n",
    "                my_max_elem = my_val\n",
    "                mz_max_elem = mz_val\n",
    "                load_case_max = load_case\n",
    "\n",
    "        data[\"My\"].append(my_max_elem)\n",
    "        data[\"Mz\"].append(mz_max_elem)\n",
    "        data[\"r_max\"].append(r_max_elem)\n",
    "        data[\"load_case_max\"].append(load_case_max)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.round(1)\n",
    "\n",
    "    def wp(x):\n",
    "        if x <= 0:\n",
    "            return 0\n",
    "        elif x > 0:\n",
    "            return (x**2)*10**4\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    delta1 = 5500 #Martelo cravação e carga de trabalho - As/Fck\n",
    "    delta2 = 2000 #Facultativo - Armadura máx - Atritolateral -Sem o de ponta (Justificar, mas não tem um valor exato normativo) #NBR6122 Fundações\n",
    "    delta3 = 2500 #NBR6118 2023 armadura max em >pilar< As max\n",
    "    delta4 = 125 #Tabela Nbr6118 - tabela limite de deslocamento/deformação |  Nbr8800 h/850 | ABS para pier\n",
    "\n",
    "    def calcular_Pt(df):\n",
    "        df['Pt_Fx_MAX'] = 0.0\n",
    "        df['Pt_Fx_MIN'] = 0.0\n",
    "        df['Pt_r_max'] = 0.0\n",
    "        df['Pt_Utot'] = 0.0\n",
    "\n",
    "        p_total = 0 #PESO PRÓPRIO DA ESTRUTURA\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Fx_MAX'] > 0:\n",
    "                df.at[index, 'Pt_Fx_MAX'] = wp((abs(row['Fx_MAX']) - delta1) / delta1)\n",
    "            else:\n",
    "                df.at[index, 'Pt_Fx_MAX'] = wp((abs(row['Fx_MAX']) - delta2) / delta2)\n",
    "\n",
    "            if row['Fx_MIN'] > 0:\n",
    "                df.at[index, 'Pt_Fx_MIN'] = wp(((abs(row['Fx_MIN']) - delta1) / delta1))\n",
    "            else:\n",
    "                df.at[index, 'Pt_Fx_MIN'] = wp(((abs(row['Fx_MIN']) - delta2) / delta2))\n",
    "\n",
    "            df.at[index, 'Pt_r_max'] = wp(((abs(row['r_max']) - delta3) / delta3))\n",
    "            df.at[index, 'Pt_Utot'] = wp(((abs(row['Utot']) - delta4) / delta4))\n",
    "\n",
    "            p_total +=max(df['Pt_Fx_MAX'] + df['Pt_Fx_MIN'] + df['Pt_r_max'] + df['Pt_Utot'])\n",
    "\n",
    "        return round(p_total,1)\n",
    "    \n",
    "\n",
    "    p_total = calcular_Pt(df)\n",
    "    return p_total, pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais\n",
    "\n",
    "# Código atualizado da função obj_fun com conversões de float para evitar erro com tensores\n",
    "# Atualizando a função obj_fun para utilizar `.item()` ao acessar elementos individuais de tensores\n",
    "# Ajuste de `obj_fun` para trabalhar com um tensor multi-elemento `xx`\n",
    "def obj_fun(xx):\n",
    "    p = 0\n",
    "\n",
    "    print(\"Tensor XX\",xx)\n",
    "\n",
    "    # Iteração sobre os elementos do tensor `xx` com conversão para float\n",
    "    x_aba_1, y_aba_1 = round(float(xx[0].item()), 1), round(float(xx[1].item()), 1)\n",
    "    x_aba_2, y_aba_2 = round(float(xx[4].item()), 1), round(float(xx[5].item()), 1)\n",
    "    x_central_1 = round(float(xx[8].item()), 1)\n",
    "\n",
    "    # Converter coordenadas normalizadas para X e Y\n",
    "    x_aba_1, y_aba_1 = converter_x(x_aba_1), converter_y(y_aba_1)\n",
    "    x_aba_2, y_aba_2 = converter_x(x_aba_2), converter_y(y_aba_2)\n",
    "    x_central_1 = converter_x(x_central_1)\n",
    "\n",
    "    # Coordenadas iniciais\n",
    "    coordinates = [\n",
    "        [x_aba_1, y_aba_1, 0],\n",
    "        [x_aba_2, y_aba_2, 0],\n",
    "        [x_central_1, 0.0, 0.0]\n",
    "    ]\n",
    "\n",
    "    # Arredondar ângulos verticais e horizontais com conversão de tensor\n",
    "    ang_v1, ang_h1 = round_to_nearest(float(xx[2].item())), round_to_nearest(float(xx[3].item()))\n",
    "    ang_v2, ang_h2 = round_to_nearest(float(xx[6].item())), round_to_nearest(float(xx[7].item()))\n",
    "    ang_v3, ang_h3 = round_to_nearest(float(xx[9].item())), round_to_nearest(float(xx[10].item()))\n",
    "\n",
    "    # Listas de ângulos\n",
    "    array_angulos_verticais = [ang_v1, ang_v2, ang_v3]\n",
    "    array_angulos_horizontais = [ang_h1, ang_h2, ang_h3]\n",
    "\n",
    "    # Converter ângulos para valores em graus\n",
    "    array_angulos_verticais_convertidos = converter_angulos_verticais(array_angulos_verticais)\n",
    "    array_angulos_horizontais_convertidos = converter_angulos_horizontais(array_angulos_horizontais)\n",
    "\n",
    "    # Espelhar coordenadas\n",
    "    espelho = espelhar_pontos(coordinates, n_estacas_centrais)\n",
    "\n",
    "    # Calcular penalidades para pontos próximos demais\n",
    "    for i in range(len(coordinates)):\n",
    "        for j in range(i + 1, len(coordinates)):\n",
    "            if distancia_entre_pontos(coordinates[i], coordinates[j]) < distancia_minima:\n",
    "                p += (distancia_minima / (0.1 + distancia_entre_pontos(coordinates[i], coordinates[j]))) * 10**5\n",
    "\n",
    "    # Gerar pontos finais e espelhados\n",
    "    coordenadas_finais = gerar_pontos_finais(coordinates, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos)\n",
    "    espelhos_finais = espelhar_pontos_finais(coordenadas_finais, int(n_estacas_centrais))\n",
    "\n",
    "    # Preparar pontos iniciais e finais\n",
    "    pontos_iniciais = coordinates + espelho\n",
    "    pontos_finais = np.array(coordenadas_finais + espelhos_finais)\n",
    "\n",
    "    # Ajuste de pontos finais, se necessário, respeitando as restrições\n",
    "    pontos_finais_ajustados, array_angulos_horizontais_convertidos, array_angulos_horizontais, p = verificar_e_ajustar_pontos(\n",
    "        pontos_iniciais, L_estacas, distancia_minima, pontos_finais,\n",
    "        array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos,\n",
    "        array_angulos_horizontais, p\n",
    "    )\n",
    "\n",
    "    # Realizar análise se a penalidade for zero\n",
    "    if p == 0:\n",
    "        p_analise, pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais = analise(\n",
    "            pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais\n",
    "        )\n",
    "        p += p_analise  # Atualizar penalidade com valor da análise\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "    print(p)\n",
    "\n",
    "\n",
    "    return p\n",
    "\n",
    "# Comentário para evitar execução de código no PCI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate initial sampling plan using LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LHS_sample(N_INITIAL, DIM)\n",
    "x = torch.from_numpy(x)\n",
    "x = x.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor XX tensor([0.9447, 0.5373, 0.4820, 0.1461, 0.8378, 0.9466, 0.9481, 0.0268, 0.4867,\n",
      "        0.6057, 0.3843])\n",
      "\n",
      "119844.6479338746\n",
      "Tensor XX tensor([0.7116, 0.9300, 0.1702, 0.7988, 0.9064, 0.4558, 0.4259, 0.7505, 0.6178,\n",
      "        0.8636, 0.8135])\n",
      "\n",
      "80.66587459276306\n",
      "Tensor XX tensor([0.6140, 0.0374, 0.3922, 0.7209, 0.1041, 0.8975, 0.2555, 0.1640, 0.5434,\n",
      "        0.5285, 0.8636])\n",
      "\n",
      "154958.78311123193\n",
      "Tensor XX tensor([0.9042, 0.5125, 0.0217, 0.7783, 0.6172, 0.9697, 0.0637, 0.4089, 0.3196,\n",
      "        0.5549, 0.4670])\n",
      "\n",
      "112.01303637099679\n",
      "Tensor XX tensor([0.6915, 0.8566, 0.4405, 0.0836, 0.4742, 0.1163, 0.8473, 0.9733, 0.2571,\n",
      "        0.2297, 0.6016])\n",
      "\n",
      "94680.53561227291\n",
      "Tensor XX tensor([0.9657, 0.6834, 0.7042, 0.5209, 0.1561, 0.6390, 0.8040, 0.2283, 0.9943,\n",
      "        0.8504, 0.4526])\n",
      "\n",
      "53.51496086072085\n",
      "Tensor XX tensor([0.0395, 0.7709, 0.3195, 0.8598, 0.8489, 0.3231, 0.3516, 0.9230, 0.4782,\n",
      "        0.6809, 0.2447])\n",
      "\n",
      "197.53476342397522\n",
      "Tensor XX tensor([0.8002, 0.4241, 0.4639, 0.3824, 0.7585, 0.1579, 0.1775, 0.2914, 0.9405,\n",
      "        0.6244, 0.7829])\n",
      "\n",
      "360840.27995292476\n",
      "Tensor XX tensor([0.0128, 0.2048, 0.6902, 0.9500, 0.4978, 0.8182, 0.8203, 0.3432, 0.8173,\n",
      "        0.5715, 0.0999])\n",
      "\n",
      "137.6306620209059\n",
      "Tensor XX tensor([0.1786, 0.9135, 0.0682, 0.5695, 0.2917, 0.1849, 0.0542, 0.1344, 0.6670,\n",
      "        0.3336, 0.5316])\n",
      "\n",
      "41.6527196986348\n",
      "Tensor XX tensor([0.3465, 0.0075, 0.2927, 0.7084, 0.9975, 0.6158, 0.8832, 0.2552, 0.0391,\n",
      "        0.4498, 0.5020])\n",
      "\n",
      "47.619047619047606\n",
      "Tensor XX tensor([0.6308, 0.0753, 0.6406, 0.3023, 0.4400, 0.0567, 0.1196, 0.6076, 0.3960,\n",
      "        0.1046, 0.2071])\n",
      "\n",
      "364585.83186363656\n",
      "Tensor XX tensor([0.9329, 0.2708, 0.1326, 0.6611, 0.4120, 0.2696, 0.7165, 0.5165, 0.8583,\n",
      "        0.0063, 0.2975])\n",
      "\n",
      "104342.35483562938\n",
      "Tensor XX tensor([0.4351, 0.0504, 0.2409, 0.4556, 0.7164, 0.0033, 0.4569, 0.5853, 0.9610,\n",
      "        0.3063, 0.0205])\n",
      "\n",
      "39.542993708237816\n",
      "Tensor XX tensor([0.4976, 0.9577, 0.9565, 0.4121, 0.8685, 0.3404, 0.6262, 0.0184, 0.7261,\n",
      "        0.5176, 0.1611])\n",
      "\n",
      "50.31446540880503\n",
      "Tensor XX tensor([0.1100, 0.5606, 0.2260, 0.9348, 0.0967, 0.5553, 0.1574, 0.8394, 0.3533,\n",
      "        0.1994, 0.4248])\n",
      "\n",
      "1250238.5280673238\n",
      "Tensor XX tensor([0.6615, 0.6003, 0.9980, 0.9793, 0.3129, 0.6536, 0.3109, 0.4287, 0.2274,\n",
      "        0.6407, 0.2242])\n",
      "8080.2\n",
      "Tensor XX tensor([0.8544, 0.8352, 0.9221, 0.3211, 0.5653, 0.5149, 0.3628, 0.2092, 0.4117,\n",
      "        0.1647, 0.0707])\n",
      "\n",
      "84.9611816017208\n",
      "Tensor XX tensor([0.4787, 0.9959, 0.8130, 0.6539, 0.3952, 0.8423, 0.1209, 0.7847, 0.1712,\n",
      "        0.7583, 0.7059])\n",
      "\n",
      "168922.57495916143\n",
      "Tensor XX tensor([0.8932, 0.0835, 0.0512, 0.3715, 0.2274, 0.7845, 0.5336, 0.7088, 0.4534,\n",
      "        0.2891, 0.6663])\n",
      "\n",
      "39.321677126738464\n",
      "Tensor XX tensor([0.8278, 0.6758, 0.6165, 0.8608, 0.8031, 0.4711, 0.9929, 0.5780, 0.1050,\n",
      "        0.4003, 0.8391])\n",
      "\n",
      "250135.01276870308\n",
      "Tensor XX tensor([0.1201, 0.7057, 0.8942, 0.2400, 0.1356, 0.9253, 0.5031, 0.6235, 0.9320,\n",
      "        0.4670, 0.6349])\n",
      "\n",
      "250131.5047123457\n",
      "Tensor XX tensor([0.4060, 0.8961, 0.7633, 0.0609, 0.9404, 0.2574, 0.7664, 0.4428, 0.3743,\n",
      "        0.8362, 0.6550])\n",
      "\n",
      "59.65280849181777\n",
      "Tensor XX tensor([0.0456, 0.6285, 0.3328, 0.1910, 0.5377, 0.0258, 0.0110, 0.5211, 0.1944,\n",
      "        0.4839, 0.7722])\n",
      "\n",
      "109.26738949639092\n",
      "Tensor XX tensor([0.2798, 0.4179, 0.2024, 0.8879, 0.6258, 0.3684, 0.4847, 0.1939, 0.1382,\n",
      "        0.0532, 0.9591])\n",
      "\n",
      "56.45858254884624\n",
      "Tensor XX tensor([0.2045, 0.4619, 0.8480, 0.6945, 0.7601, 0.2166, 0.9741, 0.7757, 0.6548,\n",
      "        0.3434, 0.3401])\n",
      "\n",
      "110764.4193157633\n",
      "Tensor XX tensor([0.6444, 0.9765, 0.4238, 0.9197, 0.2782, 0.6878, 0.6801, 0.1023, 0.7501,\n",
      "        0.2078, 0.7522])\n",
      "\n",
      "293.3739686659868\n",
      "Tensor XX tensor([0.7660, 0.8116, 0.5146, 0.6157, 0.6964, 0.7550, 0.2373, 0.8626, 0.8827,\n",
      "        0.2703, 0.0178])\n",
      "\n",
      "208472.50895361757\n",
      "Tensor XX tensor([0.0979, 0.3315, 0.1926, 0.1003, 0.1601, 0.7138, 0.6460, 0.3715, 0.6219,\n",
      "        0.7084, 0.8968])\n",
      "\n",
      "119859.5441823354\n",
      "Tensor XX tensor([0.3109, 0.2898, 0.9746, 0.4261, 0.9273, 0.5631, 0.5931, 0.9106, 0.5112,\n",
      "        0.0320, 0.7243])\n",
      "\n",
      "107.0446823287302\n",
      "Tensor XX tensor([0.4433, 0.8626, 0.2604, 0.4612, 0.5574, 0.9091, 0.5660, 0.8855, 0.0933,\n",
      "        0.9488, 0.3607])\n",
      "\n",
      "113667.67101769078\n",
      "Tensor XX tensor([0.3222, 0.6582, 0.5948, 0.8322, 0.6414, 0.0741, 0.6038, 0.2601, 0.9074,\n",
      "        0.7603, 0.5776])\n",
      "\n",
      "65.82075770367243\n",
      "Tensor XX tensor([0.7276, 0.1182, 0.7939, 0.1670, 0.4287, 0.5928, 0.9287, 0.8557, 0.7879,\n",
      "        0.8896, 0.5922])\n",
      "\n",
      "130367.86667298384\n",
      "Tensor XX tensor([0.2351, 0.3624, 0.7297, 0.7527, 0.1894, 0.9961, 0.4073, 0.5599, 0.5224,\n",
      "        0.0620, 0.1100])\n",
      "\n",
      "96222.13319088319\n",
      "Tensor XX tensor([0.5405, 0.5577, 0.8224, 0.4813, 0.2541, 0.2334, 0.7400, 0.3064, 0.0734,\n",
      "        0.0873, 0.9622])\n",
      "\n",
      "71.61738558294493\n",
      "Tensor XX tensor([0.5041, 0.2385, 0.6724, 0.2798, 0.7263, 0.4281, 0.0349, 0.9594, 0.6902,\n",
      "        0.3722, 0.9814])\n",
      "\n",
      "106261.17889148515\n",
      "Tensor XX tensor([0.3804, 0.1600, 0.7489, 0.0486, 0.2064, 0.5322, 0.2004, 0.1418, 0.0539,\n",
      "        0.3964, 0.3103])\n",
      "\n",
      "98771.03513849279\n",
      "Tensor XX tensor([0.3692, 0.2564, 0.5260, 0.6368, 0.0065, 0.2932, 0.1868, 0.7326, 0.8685,\n",
      "        0.7218, 0.9011])\n",
      "\n",
      "368.951019142835\n",
      "Tensor XX tensor([0.8781, 0.1848, 0.6213, 0.5151, 0.9753, 0.7399, 0.0950, 0.3965, 0.1567,\n",
      "        0.8024, 0.1860])\n",
      "\n",
      "102653.75910657224\n",
      "Tensor XX tensor([0.1912, 0.1594, 0.0898, 0.5558, 0.6677, 0.6782, 0.7373, 0.6701, 0.2723,\n",
      "        0.7885, 0.8409])\n",
      "\n",
      "110715.76420053212\n",
      "Tensor XX tensor([0.1499, 0.3574, 0.3674, 0.2895, 0.7988, 0.8715, 0.2972, 0.0438, 0.3271,\n",
      "        0.2486, 0.5424])\n",
      "\n",
      "22.906403940886698\n",
      "Tensor XX tensor([0.2911, 0.7875, 0.4125, 0.1258, 0.0524, 0.4068, 0.4764, 0.4868, 0.2170,\n",
      "        0.9374, 0.0471])\n",
      "\n",
      "27.265487467516138\n",
      "Tensor XX tensor([0.7469, 0.7314, 0.1078, 0.3440, 0.3462, 0.3828, 0.8626, 0.3387, 0.0151,\n",
      "        0.6660, 0.4125])\n",
      "70869.4\n",
      "Tensor XX tensor([0.9851, 0.3968, 0.8635, 0.9852, 0.5074, 0.1337, 0.3207, 0.6552, 0.5873,\n",
      "        0.9101, 0.4952])\n",
      "\n",
      "130231.74297750373\n",
      "Tensor XX tensor([0.0601, 0.3171, 0.9078, 0.5901, 0.3370, 0.1783, 0.5407, 0.0930, 0.4247,\n",
      "        0.9662, 0.6986])\n",
      "\n",
      "222354.607838348\n",
      "Tensor XX tensor([0.5686, 0.5991, 0.5742, 0.0231, 0.8980, 0.7749, 0.3850, 0.4619, 0.8355,\n",
      "        0.1484, 0.9393])\n",
      "\n",
      "49.5722890743175\n",
      "Tensor XX tensor([0.2482, 0.7458, 0.3546, 0.2367, 0.3702, 0.8312, 0.9022, 0.6933, 0.7007,\n",
      "        0.1218, 0.3225])\n",
      "\n",
      "111687.11167456175\n",
      "Tensor XX tensor([0.7833, 0.1228, 0.5430, 0.8188, 0.0777, 0.0970, 0.6643, 0.8049, 0.2947,\n",
      "        0.4324, 0.2615])\n",
      "\n",
      "94817.94250461426\n",
      "Tensor XX tensor([0.5252, 0.4485, 0.0055, 0.2065, 0.0354, 0.4997, 0.7863, 0.9973, 0.5780,\n",
      "        0.5954, 0.1573])\n",
      "\n",
      "61.17573743248858\n",
      "Tensor XX tensor([5.9598e-01, 4.8818e-01, 1.4101e-01, 1.4480e-04, 5.9146e-01, 3.0155e-01,\n",
      "        2.7001e-01, 6.4041e-02, 7.6225e-01, 9.9576e-01, 1.3352e-01])\n",
      "\n",
      "250075.88739290094\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "name_f, name_x = 'f.mat', 'x.mat'\n",
    "path = './'\n",
    "pathname_f = path + name_f\n",
    "pathname_x = path + name_x\n",
    "check_file = os.path.isfile(pathname_f)\n",
    "\n",
    "if check_file:\n",
    "    mat_contents = sio.loadmat(pathname_f)\n",
    "    f = mat_contents['f']\n",
    "    f = torch.Tensor(f)\n",
    "    f = f.view(N_INITIAL)\n",
    "    \n",
    "    mat_contents = sio.loadmat(pathname_x)\n",
    "    x = mat_contents['x']\n",
    "    x = torch.Tensor(x)\n",
    "else:\n",
    "    # Iterar sobre `x` e aplicar a função `obj_fun` a cada linha individual\n",
    "    results = []\n",
    "    for i in range(x.shape[0]):\n",
    "        linha = x[i]              # Extrai a linha `i` como um tensor 1D\n",
    "        result = obj_fun(linha)   # Passa a linha diretamente para `obj_fun`\n",
    "        results.append(result)    # Armazena o resultado diretamente na lista\n",
    "\n",
    "    # Converte `results` em um tensor e ajusta a forma para `N_INITIAL`\n",
    "    f = torch.tensor(results)\n",
    "    f = f.view(N_INITIAL)\n",
    "\n",
    "    # Salva os resultados nos arquivos .mat\n",
    "    sio.savemat(name_f, {'f': f.numpy()})\n",
    "    sio.savemat(name_x, {'x': x.numpy()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_g, val_g = train_test_split(x, f, test_size=0.20)\n",
    "# Converter os dados de treino e validação para o tipo float32\n",
    "train_x = train_x.float()\n",
    "train_g = train_g.float()\n",
    "val_x = val_x.float()\n",
    "val_g = val_g.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Global Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_loss 1769306880.0\n",
      "best_loss 10000.0\n",
      "final_loss 1758020992.0\n",
      "best_loss 10000.0\n",
      "final_loss 1746735744.0\n",
      "best_loss 10000.0\n",
      "final_loss 1735453440.0\n",
      "best_loss 10000.0\n",
      "final_loss 1724175360.0\n",
      "best_loss 10000.0\n",
      "final_loss 1712904448.0\n",
      "best_loss 10000.0\n",
      "final_loss 1701642112.0\n",
      "best_loss 10000.0\n",
      "final_loss 1690391296.0\n",
      "best_loss 10000.0\n",
      "final_loss 1679154560.0\n",
      "best_loss 10000.0\n",
      "final_loss 1667934080.0\n",
      "best_loss 10000.0\n",
      "final_loss 1656732928.0\n",
      "best_loss 10000.0\n",
      "final_loss 1645554304.0\n",
      "best_loss 10000.0\n",
      "final_loss 1634400896.0\n",
      "best_loss 10000.0\n",
      "final_loss 1623277184.0\n",
      "best_loss 10000.0\n",
      "final_loss 1612186496.0\n",
      "best_loss 10000.0\n",
      "final_loss 1601131776.0\n",
      "best_loss 10000.0\n",
      "final_loss 1590118528.0\n",
      "best_loss 10000.0\n",
      "final_loss 1579149568.0\n",
      "best_loss 10000.0\n",
      "final_loss 1568230656.0\n",
      "best_loss 10000.0\n",
      "final_loss 1557365888.0\n",
      "best_loss 10000.0\n",
      "final_loss 1546560768.0\n",
      "best_loss 10000.0\n",
      "final_loss 1535818880.0\n",
      "best_loss 10000.0\n",
      "final_loss 1525146880.0\n",
      "best_loss 10000.0\n",
      "final_loss 1514550656.0\n",
      "best_loss 10000.0\n",
      "final_loss 1504035200.0\n",
      "best_loss 10000.0\n",
      "final_loss 1493606528.0\n",
      "best_loss 10000.0\n",
      "final_loss 1483270400.0\n",
      "best_loss 10000.0\n",
      "final_loss 1473033472.0\n",
      "best_loss 10000.0\n",
      "final_loss 1462902144.0\n",
      "best_loss 10000.0\n",
      "final_loss 1452882944.0\n",
      "best_loss 10000.0\n",
      "final_loss 1442981504.0\n",
      "best_loss 10000.0\n",
      "final_loss 1433205504.0\n",
      "best_loss 10000.0\n",
      "final_loss 1423561216.0\n",
      "best_loss 10000.0\n",
      "final_loss 1414055168.0\n",
      "best_loss 10000.0\n",
      "final_loss 1404693504.0\n",
      "best_loss 10000.0\n",
      "final_loss 1395483648.0\n",
      "best_loss 10000.0\n",
      "final_loss 1386432384.0\n",
      "best_loss 10000.0\n",
      "final_loss 1377545344.0\n",
      "best_loss 10000.0\n",
      "final_loss 1368829312.0\n",
      "best_loss 10000.0\n",
      "final_loss 1360290944.0\n",
      "best_loss 10000.0\n",
      "final_loss 1351935360.0\n",
      "best_loss 10000.0\n",
      "final_loss 1343768960.0\n",
      "best_loss 10000.0\n",
      "final_loss 1335797120.0\n",
      "best_loss 10000.0\n",
      "final_loss 1328025472.0\n",
      "best_loss 10000.0\n",
      "final_loss 1320458368.0\n",
      "best_loss 10000.0\n",
      "final_loss 1313100800.0\n",
      "best_loss 10000.0\n",
      "final_loss 1305957632.0\n",
      "best_loss 10000.0\n",
      "final_loss 1299032320.0\n",
      "best_loss 10000.0\n",
      "final_loss 1292327808.0\n",
      "best_loss 10000.0\n",
      "final_loss 1285848448.0\n",
      "best_loss 10000.0\n",
      "final_loss 1279595776.0\n",
      "best_loss 10000.0\n",
      "final_loss 1273572608.0\n",
      "best_loss 10000.0\n",
      "final_loss 1267780480.0\n",
      "best_loss 10000.0\n",
      "final_loss 1262220544.0\n",
      "best_loss 10000.0\n",
      "final_loss 1256893568.0\n",
      "best_loss 10000.0\n",
      "final_loss 1251799680.0\n",
      "best_loss 10000.0\n",
      "final_loss 1246938496.0\n",
      "best_loss 10000.0\n",
      "final_loss 1242309504.0\n",
      "best_loss 10000.0\n",
      "final_loss 1237911168.0\n",
      "best_loss 10000.0\n",
      "final_loss 1233741440.0\n",
      "best_loss 10000.0\n",
      "final_loss 1229798272.0\n",
      "best_loss 10000.0\n",
      "final_loss 1226078976.0\n",
      "best_loss 10000.0\n",
      "final_loss 1222579456.0\n",
      "best_loss 10000.0\n",
      "final_loss 1219296512.0\n",
      "best_loss 10000.0\n",
      "final_loss 1216225664.0\n",
      "best_loss 10000.0\n",
      "final_loss 1213362432.0\n",
      "best_loss 10000.0\n",
      "final_loss 1210701696.0\n",
      "best_loss 10000.0\n",
      "final_loss 1208237952.0\n",
      "best_loss 10000.0\n",
      "final_loss 1205965440.0\n",
      "best_loss 10000.0\n",
      "final_loss 1203877632.0\n",
      "best_loss 10000.0\n",
      "final_loss 1201968896.0\n",
      "best_loss 10000.0\n",
      "final_loss 1200232192.0\n",
      "best_loss 10000.0\n",
      "final_loss 1198659840.0\n",
      "best_loss 10000.0\n",
      "final_loss 1197245952.0\n",
      "best_loss 10000.0\n",
      "final_loss 1195982208.0\n",
      "best_loss 10000.0\n",
      "final_loss 1194861568.0\n",
      "best_loss 10000.0\n",
      "final_loss 1193876608.0\n",
      "best_loss 10000.0\n",
      "final_loss 1193018752.0\n",
      "best_loss 10000.0\n",
      "final_loss 1192281344.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191656320.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191135744.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190711552.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190376704.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190123264.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189943936.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189831424.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189777792.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189777280.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189821696.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189905280.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190020864.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190162816.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190324736.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190500864.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190685696.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190873984.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191060992.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191241856.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191411456.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191566464.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191702400.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191815808.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191903104.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191960960.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191986944.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191978240.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191931392.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191845632.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191718400.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191547776.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191332608.0\n",
      "best_loss 10000.0\n",
      "final_loss 1191071360.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190762368.0\n",
      "best_loss 10000.0\n",
      "final_loss 1190405376.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189999232.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189543680.0\n",
      "best_loss 10000.0\n",
      "final_loss 1189038080.0\n",
      "best_loss 10000.0\n",
      "final_loss 1188481664.0\n",
      "best_loss 10000.0\n",
      "final_loss 1187874560.0\n",
      "best_loss 10000.0\n",
      "final_loss 1187216896.0\n",
      "best_loss 10000.0\n",
      "final_loss 1186509184.0\n",
      "best_loss 10000.0\n",
      "final_loss 1185750912.0\n",
      "best_loss 10000.0\n",
      "final_loss 1184942848.0\n",
      "best_loss 10000.0\n",
      "final_loss 1184084992.0\n",
      "best_loss 10000.0\n",
      "final_loss 1183178368.0\n",
      "best_loss 10000.0\n",
      "final_loss 1182222976.0\n",
      "best_loss 10000.0\n",
      "final_loss 1181220096.0\n",
      "best_loss 10000.0\n",
      "final_loss 1180170240.0\n",
      "best_loss 10000.0\n",
      "final_loss 1179074048.0\n",
      "best_loss 10000.0\n",
      "final_loss 1177932672.0\n",
      "best_loss 10000.0\n",
      "final_loss 1176746880.0\n",
      "best_loss 10000.0\n",
      "final_loss 1175518208.0\n",
      "best_loss 10000.0\n",
      "final_loss 1174246912.0\n",
      "best_loss 10000.0\n",
      "final_loss 1172934272.0\n",
      "best_loss 10000.0\n",
      "final_loss 1171581312.0\n",
      "best_loss 10000.0\n",
      "final_loss 1170189184.0\n",
      "best_loss 10000.0\n",
      "final_loss 1168759424.0\n",
      "best_loss 10000.0\n",
      "final_loss 1167293056.0\n",
      "best_loss 10000.0\n",
      "final_loss 1165790464.0\n",
      "best_loss 10000.0\n",
      "final_loss 1164254080.0\n",
      "best_loss 10000.0\n",
      "final_loss 1162683520.0\n",
      "best_loss 10000.0\n",
      "final_loss 1161081472.0\n",
      "best_loss 10000.0\n",
      "final_loss 1159448064.0\n",
      "best_loss 10000.0\n",
      "final_loss 1157785088.0\n",
      "best_loss 10000.0\n",
      "final_loss 1156093824.0\n",
      "best_loss 10000.0\n",
      "final_loss 1154375168.0\n",
      "best_loss 10000.0\n",
      "final_loss 1152629760.0\n",
      "best_loss 10000.0\n",
      "final_loss 1150859136.0\n",
      "best_loss 10000.0\n",
      "final_loss 1149065088.0\n",
      "best_loss 10000.0\n",
      "final_loss 1147247744.0\n",
      "best_loss 10000.0\n",
      "final_loss 1145408512.0\n",
      "best_loss 10000.0\n",
      "final_loss 1143548672.0\n",
      "best_loss 10000.0\n",
      "final_loss 1141669120.0\n",
      "best_loss 10000.0\n",
      "final_loss 1139770624.0\n",
      "best_loss 10000.0\n",
      "final_loss 1137854720.0\n",
      "best_loss 10000.0\n",
      "final_loss 1135922176.0\n",
      "best_loss 10000.0\n",
      "final_loss 1133974016.0\n",
      "best_loss 10000.0\n",
      "final_loss 1132010752.0\n",
      "best_loss 10000.0\n",
      "final_loss 1130033664.0\n",
      "best_loss 10000.0\n",
      "final_loss 1128043904.0\n",
      "best_loss 10000.0\n",
      "final_loss 1126041728.0\n",
      "best_loss 10000.0\n",
      "final_loss 1124028672.0\n",
      "best_loss 10000.0\n",
      "final_loss 1122005248.0\n",
      "best_loss 10000.0\n",
      "final_loss 1119971840.0\n",
      "best_loss 10000.0\n",
      "final_loss 1117929856.0\n",
      "best_loss 10000.0\n",
      "final_loss 1115879552.0\n",
      "best_loss 10000.0\n",
      "final_loss 1113822464.0\n",
      "best_loss 10000.0\n",
      "final_loss 1111758208.0\n",
      "best_loss 10000.0\n",
      "final_loss 1109687552.0\n",
      "best_loss 10000.0\n",
      "final_loss 1107612416.0\n",
      "best_loss 10000.0\n",
      "final_loss 1105531776.0\n",
      "best_loss 10000.0\n",
      "final_loss 1103447296.0\n",
      "best_loss 10000.0\n",
      "final_loss 1101359488.0\n",
      "best_loss 10000.0\n",
      "final_loss 1099268352.0\n",
      "best_loss 10000.0\n",
      "final_loss 1097174912.0\n",
      "best_loss 10000.0\n",
      "final_loss 1095079680.0\n",
      "best_loss 10000.0\n",
      "final_loss 1092982656.0\n",
      "best_loss 10000.0\n",
      "final_loss 1090885376.0\n",
      "best_loss 10000.0\n",
      "final_loss 1088787200.0\n",
      "best_loss 10000.0\n",
      "final_loss 1086688896.0\n",
      "best_loss 10000.0\n",
      "final_loss 1084591104.0\n",
      "best_loss 10000.0\n",
      "final_loss 1082494080.0\n",
      "best_loss 10000.0\n",
      "final_loss 1080397952.0\n",
      "best_loss 10000.0\n",
      "final_loss 1078303744.0\n",
      "best_loss 10000.0\n",
      "final_loss 1076211072.0\n",
      "best_loss 10000.0\n",
      "final_loss 1074120960.0\n",
      "best_loss 10000.0\n",
      "final_loss 1072033408.0\n",
      "best_loss 10000.0\n",
      "final_loss 1069948416.0\n",
      "best_loss 10000.0\n",
      "final_loss 1067866624.0\n",
      "best_loss 10000.0\n",
      "final_loss 1065788608.0\n",
      "best_loss 10000.0\n",
      "final_loss 1063713408.0\n",
      "best_loss 10000.0\n",
      "final_loss 1061643072.0\n",
      "best_loss 10000.0\n",
      "final_loss 1059576320.0\n",
      "best_loss 10000.0\n",
      "final_loss 1057513664.0\n",
      "best_loss 10000.0\n",
      "final_loss 1055456256.0\n",
      "best_loss 10000.0\n",
      "final_loss 1053402816.0\n",
      "best_loss 10000.0\n",
      "final_loss 1051354624.0\n",
      "best_loss 10000.0\n",
      "final_loss 1049312128.0\n",
      "best_loss 10000.0\n",
      "final_loss 1047274304.0\n",
      "best_loss 10000.0\n",
      "final_loss 1045242176.0\n",
      "best_loss 10000.0\n",
      "final_loss 1043215552.0\n",
      "best_loss 10000.0\n",
      "final_loss 1041194816.0\n",
      "best_loss 10000.0\n",
      "final_loss 1039179584.0\n",
      "best_loss 10000.0\n",
      "final_loss 1037170368.0\n",
      "best_loss 10000.0\n",
      "final_loss 1035167616.0\n",
      "best_loss 10000.0\n",
      "final_loss 1033170752.0\n",
      "best_loss 10000.0\n",
      "final_loss 1031180416.0\n",
      "best_loss 10000.0\n",
      "final_loss 1029196288.0\n",
      "best_loss 10000.0\n",
      "final_loss 1027218944.0\n",
      "best_loss 10000.0\n",
      "final_loss 1025247872.0\n",
      "best_loss 10000.0\n",
      "final_loss 1023283520.0\n",
      "best_loss 10000.0\n",
      "final_loss 1021325440.0\n",
      "best_loss 10000.0\n",
      "final_loss 1019374592.0\n",
      "best_loss 10000.0\n",
      "final_loss 1017430656.0\n",
      "best_loss 10000.0\n",
      "final_loss 1015493504.0\n",
      "best_loss 10000.0\n",
      "final_loss 1013562752.0\n",
      "best_loss 10000.0\n",
      "final_loss 1011639424.0\n",
      "best_loss 10000.0\n",
      "final_loss 1009723008.0\n",
      "best_loss 10000.0\n",
      "final_loss 1007813248.0\n",
      "best_loss 10000.0\n",
      "final_loss 1005910720.0\n",
      "best_loss 10000.0\n",
      "final_loss 1004015232.0\n",
      "best_loss 10000.0\n",
      "final_loss 1002126848.0\n",
      "best_loss 10000.0\n",
      "final_loss 1000245120.0\n",
      "best_loss 10000.0\n",
      "final_loss 998371200.0\n",
      "best_loss 10000.0\n",
      "final_loss 996503872.0\n",
      "best_loss 10000.0\n",
      "final_loss 994644288.0\n",
      "best_loss 10000.0\n",
      "final_loss 992791168.0\n",
      "best_loss 10000.0\n",
      "final_loss 990945408.0\n",
      "best_loss 10000.0\n",
      "final_loss 989106880.0\n",
      "best_loss 10000.0\n",
      "final_loss 987274944.0\n",
      "best_loss 10000.0\n",
      "final_loss 985450496.0\n",
      "best_loss 10000.0\n",
      "final_loss 983633408.0\n",
      "best_loss 10000.0\n",
      "final_loss 981823296.0\n",
      "best_loss 10000.0\n",
      "final_loss 980020416.0\n",
      "best_loss 10000.0\n",
      "final_loss 978224128.0\n",
      "best_loss 10000.0\n",
      "final_loss 976435072.0\n",
      "best_loss 10000.0\n",
      "final_loss 974653120.0\n",
      "best_loss 10000.0\n",
      "final_loss 972878144.0\n",
      "best_loss 10000.0\n",
      "final_loss 971110400.0\n",
      "best_loss 10000.0\n",
      "final_loss 969349312.0\n",
      "best_loss 10000.0\n",
      "final_loss 967595520.0\n",
      "best_loss 10000.0\n",
      "final_loss 965848448.0\n",
      "best_loss 10000.0\n",
      "final_loss 964108672.0\n",
      "best_loss 10000.0\n",
      "final_loss 962375040.0\n",
      "best_loss 10000.0\n",
      "final_loss 960649024.0\n",
      "best_loss 10000.0\n",
      "final_loss 958929920.0\n",
      "best_loss 10000.0\n",
      "final_loss 957217280.0\n",
      "best_loss 10000.0\n",
      "final_loss 955510976.0\n",
      "best_loss 10000.0\n",
      "final_loss 953812352.0\n",
      "best_loss 10000.0\n",
      "final_loss 952120000.0\n",
      "best_loss 10000.0\n",
      "final_loss 950434944.0\n",
      "best_loss 10000.0\n",
      "final_loss 948755840.0\n",
      "best_loss 10000.0\n",
      "final_loss 947083456.0\n",
      "best_loss 10000.0\n",
      "final_loss 945417920.0\n",
      "best_loss 10000.0\n",
      "final_loss 943758656.0\n",
      "best_loss 10000.0\n",
      "final_loss 942106432.0\n",
      "best_loss 10000.0\n",
      "final_loss 940460672.0\n",
      "best_loss 10000.0\n",
      "final_loss 938821248.0\n",
      "best_loss 10000.0\n",
      "final_loss 937188160.0\n",
      "best_loss 10000.0\n",
      "final_loss 935561728.0\n",
      "best_loss 10000.0\n",
      "final_loss 933941632.0\n",
      "best_loss 10000.0\n",
      "final_loss 932328064.0\n",
      "best_loss 10000.0\n",
      "final_loss 930720960.0\n",
      "best_loss 10000.0\n",
      "final_loss 929119872.0\n",
      "best_loss 10000.0\n",
      "final_loss 927524992.0\n",
      "best_loss 10000.0\n",
      "final_loss 925936448.0\n",
      "best_loss 10000.0\n",
      "final_loss 924354368.0\n",
      "best_loss 10000.0\n",
      "final_loss 922778304.0\n",
      "best_loss 10000.0\n",
      "final_loss 921208192.0\n",
      "best_loss 10000.0\n",
      "final_loss 919644480.0\n",
      "best_loss 10000.0\n",
      "final_loss 918086656.0\n",
      "best_loss 10000.0\n",
      "final_loss 916535104.0\n",
      "best_loss 10000.0\n",
      "final_loss 914989376.0\n",
      "best_loss 10000.0\n",
      "final_loss 913449664.0\n",
      "best_loss 10000.0\n",
      "final_loss 911916160.0\n",
      "best_loss 10000.0\n",
      "final_loss 910388608.0\n",
      "best_loss 10000.0\n",
      "final_loss 908866560.0\n",
      "best_loss 10000.0\n",
      "final_loss 907350336.0\n",
      "best_loss 10000.0\n",
      "final_loss 905840128.0\n",
      "best_loss 10000.0\n",
      "final_loss 904336000.0\n",
      "best_loss 10000.0\n",
      "final_loss 902837056.0\n",
      "best_loss 10000.0\n",
      "final_loss 901344384.0\n",
      "best_loss 10000.0\n",
      "final_loss 899857216.0\n",
      "best_loss 10000.0\n",
      "final_loss 898375360.0\n",
      "best_loss 10000.0\n",
      "final_loss 896899584.0\n",
      "best_loss 10000.0\n",
      "final_loss 895429120.0\n",
      "best_loss 10000.0\n",
      "final_loss 893964480.0\n",
      "best_loss 10000.0\n",
      "final_loss 892505472.0\n",
      "best_loss 10000.0\n",
      "final_loss 891051840.0\n",
      "best_loss 10000.0\n",
      "final_loss 889603456.0\n",
      "best_loss 10000.0\n",
      "final_loss 888160960.0\n",
      "best_loss 10000.0\n",
      "final_loss 886723584.0\n",
      "best_loss 10000.0\n",
      "final_loss 885291840.0\n",
      "best_loss 10000.0\n",
      "final_loss 883865088.0\n",
      "best_loss 10000.0\n",
      "final_loss 882443968.0\n",
      "best_loss 10000.0\n",
      "final_loss 881027968.0\n",
      "best_loss 10000.0\n",
      "final_loss 879617536.0\n",
      "best_loss 10000.0\n",
      "final_loss 878212096.0\n",
      "best_loss 10000.0\n",
      "final_loss 876812096.0\n",
      "best_loss 10000.0\n",
      "final_loss 875416960.0\n",
      "best_loss 10000.0\n",
      "final_loss 874027392.0\n",
      "best_loss 10000.0\n",
      "final_loss 872642880.0\n",
      "best_loss 10000.0\n",
      "final_loss 871263616.0\n",
      "best_loss 10000.0\n",
      "final_loss 869889024.0\n",
      "best_loss 10000.0\n",
      "final_loss 868520064.0\n",
      "best_loss 10000.0\n",
      "final_loss 867155264.0\n",
      "best_loss 10000.0\n",
      "final_loss 865795904.0\n",
      "best_loss 10000.0\n",
      "final_loss 864441728.0\n",
      "best_loss 10000.0\n",
      "final_loss 863092032.0\n",
      "best_loss 10000.0\n",
      "final_loss 861747584.0\n",
      "best_loss 10000.0\n",
      "final_loss 860408128.0\n",
      "best_loss 10000.0\n",
      "final_loss 859073408.0\n",
      "best_loss 10000.0\n",
      "final_loss 857743360.0\n",
      "best_loss 10000.0\n",
      "final_loss 856418176.0\n",
      "best_loss 10000.0\n",
      "final_loss 855098176.0\n",
      "best_loss 10000.0\n",
      "final_loss 853782592.0\n",
      "best_loss 10000.0\n",
      "final_loss 852471808.0\n",
      "best_loss 10000.0\n",
      "final_loss 851165568.0\n",
      "best_loss 10000.0\n",
      "final_loss 849864064.0\n",
      "best_loss 10000.0\n",
      "final_loss 848567296.0\n",
      "best_loss 10000.0\n",
      "final_loss 847275136.0\n",
      "best_loss 10000.0\n",
      "final_loss 845987904.0\n",
      "best_loss 10000.0\n",
      "final_loss 844704832.0\n",
      "best_loss 10000.0\n",
      "final_loss 843426432.0\n",
      "best_loss 10000.0\n",
      "final_loss 842152704.0\n",
      "best_loss 10000.0\n",
      "final_loss 840883456.0\n",
      "best_loss 10000.0\n",
      "final_loss 839618624.0\n",
      "best_loss 10000.0\n",
      "final_loss 838358080.0\n",
      "best_loss 10000.0\n",
      "final_loss 837102144.0\n",
      "best_loss 10000.0\n",
      "final_loss 835850880.0\n",
      "best_loss 10000.0\n",
      "final_loss 834603712.0\n",
      "best_loss 10000.0\n",
      "final_loss 833361024.0\n",
      "best_loss 10000.0\n",
      "final_loss 832122688.0\n",
      "best_loss 10000.0\n",
      "final_loss 830888704.0\n",
      "best_loss 10000.0\n",
      "final_loss 829658816.0\n",
      "best_loss 10000.0\n",
      "final_loss 828433280.0\n",
      "best_loss 10000.0\n",
      "final_loss 827212416.0\n",
      "best_loss 10000.0\n",
      "final_loss 825995200.0\n",
      "best_loss 10000.0\n",
      "final_loss 824782464.0\n",
      "best_loss 10000.0\n",
      "final_loss 823574144.0\n",
      "best_loss 10000.0\n",
      "final_loss 822369664.0\n",
      "best_loss 10000.0\n",
      "final_loss 821169088.0\n",
      "best_loss 10000.0\n",
      "final_loss 819973568.0\n",
      "best_loss 10000.0\n",
      "final_loss 818781440.0\n",
      "best_loss 10000.0\n",
      "final_loss 817593472.0\n",
      "best_loss 10000.0\n",
      "final_loss 816409728.0\n",
      "best_loss 10000.0\n",
      "final_loss 815230016.0\n",
      "best_loss 10000.0\n",
      "final_loss 814054080.0\n",
      "best_loss 10000.0\n",
      "final_loss 812882560.0\n",
      "best_loss 10000.0\n",
      "final_loss 811714752.0\n",
      "best_loss 10000.0\n",
      "final_loss 810550976.0\n",
      "best_loss 10000.0\n",
      "final_loss 809391488.0\n",
      "best_loss 10000.0\n",
      "final_loss 808235392.0\n",
      "best_loss 10000.0\n",
      "final_loss 807083264.0\n",
      "best_loss 10000.0\n",
      "final_loss 805935104.0\n",
      "best_loss 10000.0\n",
      "final_loss 804790656.0\n",
      "best_loss 10000.0\n",
      "final_loss 803650432.0\n",
      "best_loss 10000.0\n",
      "final_loss 802513984.0\n",
      "best_loss 10000.0\n",
      "final_loss 801381248.0\n",
      "best_loss 10000.0\n",
      "final_loss 800252160.0\n",
      "best_loss 10000.0\n",
      "final_loss 799127040.0\n",
      "best_loss 10000.0\n",
      "final_loss 798005440.0\n",
      "best_loss 10000.0\n",
      "final_loss 796887680.0\n",
      "best_loss 10000.0\n",
      "final_loss 795773568.0\n",
      "best_loss 10000.0\n",
      "final_loss 794663488.0\n",
      "best_loss 10000.0\n",
      "final_loss 793556608.0\n",
      "best_loss 10000.0\n",
      "final_loss 792454144.0\n",
      "best_loss 10000.0\n",
      "final_loss 791354496.0\n",
      "best_loss 10000.0\n",
      "final_loss 790258432.0\n",
      "best_loss 10000.0\n",
      "final_loss 789166784.0\n",
      "best_loss 10000.0\n",
      "final_loss 788077952.0\n",
      "best_loss 10000.0\n",
      "final_loss 786993088.0\n",
      "best_loss 10000.0\n",
      "final_loss 785911872.0\n",
      "best_loss 10000.0\n",
      "final_loss 784833920.0\n",
      "best_loss 10000.0\n",
      "final_loss 783759552.0\n",
      "best_loss 10000.0\n",
      "final_loss 782688512.0\n",
      "best_loss 10000.0\n",
      "final_loss 781621632.0\n",
      "best_loss 10000.0\n",
      "final_loss 780557440.0\n",
      "best_loss 10000.0\n",
      "final_loss 779497152.0\n",
      "best_loss 10000.0\n",
      "final_loss 778440256.0\n",
      "best_loss 10000.0\n",
      "final_loss 777386368.0\n",
      "best_loss 10000.0\n",
      "final_loss 776336064.0\n",
      "best_loss 10000.0\n",
      "final_loss 775289472.0\n",
      "best_loss 10000.0\n",
      "final_loss 774245952.0\n",
      "best_loss 10000.0\n",
      "final_loss 773205632.0\n",
      "best_loss 10000.0\n",
      "final_loss 772168960.0\n",
      "best_loss 10000.0\n",
      "final_loss 771135616.0\n",
      "best_loss 10000.0\n",
      "final_loss 770105408.0\n",
      "best_loss 10000.0\n",
      "final_loss 769078656.0\n",
      "best_loss 10000.0\n",
      "final_loss 768054976.0\n",
      "best_loss 10000.0\n",
      "final_loss 767034688.0\n",
      "best_loss 10000.0\n",
      "final_loss 766017600.0\n",
      "best_loss 10000.0\n",
      "final_loss 765003456.0\n",
      "best_loss 10000.0\n",
      "final_loss 763992960.0\n",
      "best_loss 10000.0\n",
      "final_loss 762985600.0\n",
      "best_loss 10000.0\n",
      "final_loss 761981184.0\n",
      "best_loss 10000.0\n",
      "final_loss 760980032.0\n",
      "best_loss 10000.0\n",
      "final_loss 759981952.0\n",
      "best_loss 10000.0\n",
      "final_loss 758987072.0\n",
      "best_loss 10000.0\n",
      "final_loss 757995264.0\n",
      "best_loss 10000.0\n",
      "final_loss 757006720.0\n",
      "best_loss 10000.0\n",
      "final_loss 756021056.0\n",
      "best_loss 10000.0\n",
      "final_loss 755038592.0\n",
      "best_loss 10000.0\n",
      "final_loss 754059008.0\n",
      "best_loss 10000.0\n",
      "final_loss 753082752.0\n",
      "best_loss 10000.0\n",
      "final_loss 752109248.0\n",
      "best_loss 10000.0\n",
      "final_loss 751139072.0\n",
      "best_loss 10000.0\n",
      "final_loss 750171712.0\n",
      "best_loss 10000.0\n",
      "final_loss 749207488.0\n",
      "best_loss 10000.0\n",
      "final_loss 748246016.0\n",
      "best_loss 10000.0\n",
      "final_loss 747287872.0\n",
      "best_loss 10000.0\n",
      "final_loss 746332288.0\n",
      "best_loss 10000.0\n",
      "final_loss 745379968.0\n",
      "best_loss 10000.0\n",
      "final_loss 744430272.0\n",
      "best_loss 10000.0\n",
      "final_loss 743483520.0\n",
      "best_loss 10000.0\n",
      "final_loss 742539776.0\n",
      "best_loss 10000.0\n",
      "final_loss 741598912.0\n",
      "best_loss 10000.0\n",
      "final_loss 740660928.0\n",
      "best_loss 10000.0\n",
      "final_loss 739725632.0\n",
      "best_loss 10000.0\n",
      "final_loss 738793536.0\n",
      "best_loss 10000.0\n",
      "final_loss 737863872.0\n",
      "best_loss 10000.0\n",
      "final_loss 736936896.0\n",
      "best_loss 10000.0\n",
      "final_loss 736013120.0\n",
      "best_loss 10000.0\n",
      "final_loss 735091840.0\n",
      "best_loss 10000.0\n",
      "final_loss 734173824.0\n",
      "best_loss 10000.0\n",
      "final_loss 733258368.0\n",
      "best_loss 10000.0\n",
      "final_loss 732345536.0\n",
      "best_loss 10000.0\n",
      "final_loss 731435392.0\n",
      "best_loss 10000.0\n",
      "final_loss 730527936.0\n",
      "best_loss 10000.0\n",
      "final_loss 729623232.0\n",
      "best_loss 10000.0\n",
      "final_loss 728721280.0\n",
      "best_loss 10000.0\n",
      "final_loss 727822144.0\n",
      "best_loss 10000.0\n",
      "final_loss 726925440.0\n",
      "best_loss 10000.0\n",
      "final_loss 726031616.0\n",
      "best_loss 10000.0\n",
      "final_loss 725140160.0\n",
      "best_loss 10000.0\n",
      "final_loss 724251776.0\n",
      "best_loss 10000.0\n",
      "final_loss 723365760.0\n",
      "best_loss 10000.0\n",
      "final_loss 722482368.0\n",
      "best_loss 10000.0\n",
      "final_loss 721601600.0\n",
      "best_loss 10000.0\n",
      "final_loss 720723328.0\n",
      "best_loss 10000.0\n",
      "final_loss 719847872.0\n",
      "best_loss 10000.0\n",
      "final_loss 718974720.0\n",
      "best_loss 10000.0\n",
      "final_loss 718104192.0\n",
      "best_loss 10000.0\n",
      "final_loss 717236416.0\n",
      "best_loss 10000.0\n",
      "final_loss 716371008.0\n",
      "best_loss 10000.0\n",
      "final_loss 715508160.0\n",
      "best_loss 10000.0\n",
      "final_loss 714647936.0\n",
      "best_loss 10000.0\n",
      "final_loss 713789952.0\n",
      "best_loss 10000.0\n",
      "final_loss 712934720.0\n",
      "best_loss 10000.0\n",
      "final_loss 712081920.0\n",
      "best_loss 10000.0\n",
      "final_loss 711231488.0\n",
      "best_loss 10000.0\n",
      "final_loss 710383296.0\n",
      "best_loss 10000.0\n",
      "final_loss 709538112.0\n",
      "best_loss 10000.0\n",
      "final_loss 708694912.0\n",
      "best_loss 10000.0\n",
      "final_loss 707854336.0\n",
      "best_loss 10000.0\n",
      "final_loss 707016064.0\n",
      "best_loss 10000.0\n",
      "final_loss 706180224.0\n",
      "best_loss 10000.0\n",
      "final_loss 705346944.0\n",
      "best_loss 10000.0\n",
      "final_loss 704515904.0\n",
      "best_loss 10000.0\n",
      "final_loss 703687232.0\n",
      "best_loss 10000.0\n",
      "final_loss 702860928.0\n",
      "best_loss 10000.0\n",
      "final_loss 702036864.0\n",
      "best_loss 10000.0\n",
      "final_loss 701215616.0\n",
      "best_loss 10000.0\n",
      "final_loss 700396160.0\n",
      "best_loss 10000.0\n",
      "final_loss 699579328.0\n",
      "best_loss 10000.0\n",
      "final_loss 698764672.0\n",
      "best_loss 10000.0\n",
      "final_loss 697952384.0\n",
      "best_loss 10000.0\n",
      "final_loss 697142400.0\n",
      "best_loss 10000.0\n",
      "final_loss 696334464.0\n",
      "best_loss 10000.0\n",
      "final_loss 695529088.0\n",
      "best_loss 10000.0\n",
      "final_loss 694726080.0\n",
      "best_loss 10000.0\n",
      "final_loss 693924992.0\n",
      "best_loss 10000.0\n",
      "final_loss 693126336.0\n",
      "best_loss 10000.0\n",
      "final_loss 692330112.0\n",
      "best_loss 10000.0\n",
      "final_loss 691535680.0\n",
      "best_loss 10000.0\n",
      "final_loss 690743872.0\n",
      "best_loss 10000.0\n",
      "final_loss 689953792.0\n",
      "best_loss 10000.0\n",
      "final_loss 689166336.0\n",
      "best_loss 10000.0\n",
      "final_loss 688380800.0\n",
      "best_loss 10000.0\n",
      "final_loss 687597696.0\n",
      "best_loss 10000.0\n",
      "final_loss 686816640.0\n",
      "best_loss 10000.0\n",
      "final_loss 686037504.0\n",
      "best_loss 10000.0\n",
      "final_loss 685260928.0\n",
      "best_loss 10000.0\n",
      "final_loss 684486400.0\n",
      "best_loss 10000.0\n",
      "final_loss 683713920.0\n",
      "best_loss 10000.0\n",
      "final_loss 682943488.0\n",
      "best_loss 10000.0\n",
      "final_loss 682175360.0\n",
      "best_loss 10000.0\n",
      "final_loss 681409152.0\n",
      "best_loss 10000.0\n",
      "final_loss 680644992.0\n",
      "best_loss 10000.0\n",
      "final_loss 679883200.0\n",
      "best_loss 10000.0\n",
      "final_loss 679123520.0\n",
      "best_loss 10000.0\n",
      "final_loss 678365440.0\n",
      "best_loss 10000.0\n",
      "final_loss 677609984.0\n",
      "best_loss 10000.0\n",
      "final_loss 676856064.0\n",
      "best_loss 10000.0\n",
      "final_loss 676104576.0\n",
      "best_loss 10000.0\n",
      "final_loss 675355008.0\n",
      "best_loss 10000.0\n",
      "final_loss 674607488.0\n",
      "best_loss 10000.0\n",
      "final_loss 673861888.0\n",
      "best_loss 10000.0\n",
      "final_loss 673118272.0\n",
      "best_loss 10000.0\n",
      "final_loss 672376768.0\n",
      "best_loss 10000.0\n",
      "final_loss 671637376.0\n",
      "best_loss 10000.0\n",
      "final_loss 670899776.0\n",
      "best_loss 10000.0\n",
      "final_loss 670163968.0\n",
      "best_loss 10000.0\n",
      "final_loss 669430592.0\n",
      "best_loss 10000.0\n",
      "final_loss 668698816.0\n",
      "best_loss 10000.0\n",
      "final_loss 667968960.0\n",
      "best_loss 10000.0\n",
      "final_loss 667241472.0\n",
      "best_loss 10000.0\n",
      "final_loss 666515712.0\n",
      "best_loss 10000.0\n",
      "final_loss 665791936.0\n",
      "best_loss 10000.0\n",
      "final_loss 665069952.0\n",
      "best_loss 10000.0\n",
      "final_loss 664349824.0\n",
      "best_loss 10000.0\n",
      "final_loss 663631744.0\n",
      "best_loss 10000.0\n",
      "final_loss 662915520.0\n",
      "best_loss 10000.0\n",
      "final_loss 662201152.0\n",
      "best_loss 10000.0\n",
      "final_loss 661488832.0\n",
      "best_loss 10000.0\n",
      "final_loss 660778304.0\n",
      "best_loss 10000.0\n",
      "final_loss 660069632.0\n",
      "best_loss 10000.0\n",
      "final_loss 659362816.0\n",
      "best_loss 10000.0\n",
      "final_loss 658657664.0\n",
      "best_loss 10000.0\n",
      "final_loss 657954624.0\n",
      "best_loss 10000.0\n",
      "final_loss 657253376.0\n",
      "best_loss 10000.0\n",
      "final_loss 656553728.0\n",
      "best_loss 10000.0\n",
      "final_loss 655856320.0\n",
      "best_loss 10000.0\n",
      "final_loss 655160256.0\n",
      "best_loss 10000.0\n",
      "final_loss 654466368.0\n",
      "best_loss 10000.0\n",
      "final_loss 653774080.0\n",
      "best_loss 10000.0\n",
      "final_loss 653083840.0\n",
      "best_loss 10000.0\n",
      "final_loss 652395200.0\n",
      "best_loss 10000.0\n",
      "final_loss 651708544.0\n",
      "best_loss 10000.0\n",
      "final_loss 651023488.0\n",
      "best_loss 10000.0\n",
      "final_loss 650339968.0\n",
      "best_loss 10000.0\n",
      "final_loss 649658496.0\n",
      "best_loss 10000.0\n",
      "final_loss 648978816.0\n",
      "best_loss 10000.0\n",
      "final_loss 648300736.0\n",
      "best_loss 10000.0\n",
      "final_loss 647624512.0\n",
      "best_loss 10000.0\n",
      "final_loss 646950016.0\n",
      "best_loss 10000.0\n",
      "final_loss 646277120.0\n",
      "best_loss 10000.0\n",
      "final_loss 645606016.0\n",
      "best_loss 10000.0\n",
      "final_loss 644936384.0\n",
      "best_loss 10000.0\n",
      "final_loss 644268864.0\n",
      "best_loss 10000.0\n",
      "final_loss 643602816.0\n",
      "best_loss 10000.0\n",
      "final_loss 642938304.0\n",
      "best_loss 10000.0\n",
      "final_loss 642275968.0\n",
      "best_loss 10000.0\n",
      "final_loss 641615104.0\n",
      "best_loss 10000.0\n",
      "final_loss 640955776.0\n",
      "best_loss 10000.0\n",
      "final_loss 640298176.0\n",
      "best_loss 10000.0\n",
      "final_loss 639642240.0\n",
      "best_loss 10000.0\n",
      "final_loss 638987904.0\n",
      "best_loss 10000.0\n",
      "final_loss 638335296.0\n",
      "best_loss 10000.0\n",
      "final_loss 637684352.0\n",
      "best_loss 10000.0\n",
      "final_loss 637035008.0\n",
      "best_loss 10000.0\n",
      "final_loss 636387200.0\n",
      "best_loss 10000.0\n",
      "final_loss 635740864.0\n",
      "best_loss 10000.0\n",
      "final_loss 635096512.0\n",
      "best_loss 10000.0\n",
      "final_loss 634453632.0\n",
      "best_loss 10000.0\n",
      "final_loss 633812352.0\n",
      "best_loss 10000.0\n",
      "final_loss 633172800.0\n",
      "best_loss 10000.0\n",
      "final_loss 632534656.0\n",
      "best_loss 10000.0\n",
      "final_loss 631897984.0\n",
      "best_loss 10000.0\n",
      "final_loss 631263104.0\n",
      "best_loss 10000.0\n",
      "final_loss 630629696.0\n",
      "best_loss 10000.0\n",
      "final_loss 629997952.0\n",
      "best_loss 10000.0\n",
      "final_loss 629367616.0\n",
      "best_loss 10000.0\n",
      "final_loss 628739008.0\n",
      "best_loss 10000.0\n",
      "final_loss 628111744.0\n",
      "best_loss 10000.0\n",
      "final_loss 627486272.0\n",
      "best_loss 10000.0\n",
      "final_loss 626861952.0\n",
      "best_loss 10000.0\n",
      "final_loss 626239488.0\n",
      "best_loss 10000.0\n",
      "final_loss 625618496.0\n",
      "best_loss 10000.0\n",
      "final_loss 624999040.0\n",
      "best_loss 10000.0\n",
      "final_loss 624381056.0\n",
      "best_loss 10000.0\n",
      "final_loss 623764672.0\n",
      "best_loss 10000.0\n",
      "final_loss 623149952.0\n",
      "best_loss 10000.0\n",
      "final_loss 622536256.0\n",
      "best_loss 10000.0\n",
      "final_loss 621924224.0\n",
      "best_loss 10000.0\n",
      "final_loss 621313728.0\n",
      "best_loss 10000.0\n",
      "final_loss 620704704.0\n",
      "best_loss 10000.0\n",
      "final_loss 620097344.0\n",
      "best_loss 10000.0\n",
      "final_loss 619491008.0\n",
      "best_loss 10000.0\n",
      "final_loss 618886592.0\n",
      "best_loss 10000.0\n",
      "final_loss 618283520.0\n",
      "best_loss 10000.0\n",
      "final_loss 617681792.0\n",
      "best_loss 10000.0\n",
      "final_loss 617081536.0\n",
      "best_loss 10000.0\n",
      "final_loss 616482432.0\n",
      "best_loss 10000.0\n",
      "final_loss 615885184.0\n",
      "best_loss 10000.0\n",
      "final_loss 615289216.0\n",
      "best_loss 10000.0\n",
      "final_loss 614694784.0\n",
      "best_loss 10000.0\n",
      "final_loss 614101760.0\n",
      "best_loss 10000.0\n",
      "final_loss 613509952.0\n",
      "best_loss 10000.0\n",
      "final_loss 612919744.0\n",
      "best_loss 10000.0\n",
      "final_loss 612330752.0\n",
      "best_loss 10000.0\n",
      "final_loss 611743296.0\n",
      "best_loss 10000.0\n",
      "final_loss 611157184.0\n",
      "best_loss 10000.0\n",
      "final_loss 610572480.0\n",
      "best_loss 10000.0\n",
      "final_loss 609989248.0\n",
      "best_loss 10000.0\n",
      "final_loss 609407360.0\n",
      "best_loss 10000.0\n",
      "final_loss 608826624.0\n",
      "best_loss 10000.0\n",
      "final_loss 608247424.0\n",
      "best_loss 10000.0\n",
      "final_loss 607669760.0\n",
      "best_loss 10000.0\n",
      "final_loss 607093312.0\n",
      "best_loss 10000.0\n",
      "final_loss 606518080.0\n",
      "best_loss 10000.0\n",
      "final_loss 605944448.0\n",
      "best_loss 10000.0\n",
      "final_loss 605371840.0\n",
      "best_loss 10000.0\n",
      "final_loss 604800896.0\n",
      "best_loss 10000.0\n",
      "final_loss 604231040.0\n",
      "best_loss 10000.0\n",
      "final_loss 603662720.0\n",
      "best_loss 10000.0\n",
      "final_loss 603095616.0\n",
      "best_loss 10000.0\n",
      "final_loss 602529856.0\n",
      "best_loss 10000.0\n",
      "final_loss 601965248.0\n",
      "best_loss 10000.0\n",
      "final_loss 601402176.0\n",
      "best_loss 10000.0\n",
      "final_loss 600840448.0\n",
      "best_loss 10000.0\n",
      "final_loss 600279872.0\n",
      "best_loss 10000.0\n",
      "final_loss 599720640.0\n",
      "best_loss 10000.0\n",
      "final_loss 599162688.0\n",
      "best_loss 10000.0\n",
      "final_loss 598605696.0\n",
      "best_loss 10000.0\n",
      "final_loss 598050304.0\n",
      "best_loss 10000.0\n",
      "final_loss 597496384.0\n",
      "best_loss 10000.0\n",
      "final_loss 596943488.0\n",
      "best_loss 10000.0\n",
      "final_loss 596391808.0\n",
      "best_loss 10000.0\n",
      "final_loss 595841600.0\n",
      "best_loss 10000.0\n",
      "final_loss 595292352.0\n",
      "best_loss 10000.0\n",
      "final_loss 594744640.0\n",
      "best_loss 10000.0\n",
      "final_loss 594198144.0\n",
      "best_loss 10000.0\n",
      "final_loss 593652736.0\n",
      "best_loss 10000.0\n",
      "final_loss 593108800.0\n",
      "best_loss 10000.0\n",
      "final_loss 592565952.0\n",
      "best_loss 10000.0\n",
      "final_loss 592024384.0\n",
      "best_loss 10000.0\n",
      "final_loss 591484032.0\n",
      "best_loss 10000.0\n",
      "final_loss 590944896.0\n",
      "best_loss 10000.0\n",
      "final_loss 590406976.0\n",
      "best_loss 10000.0\n",
      "final_loss 589870208.0\n",
      "best_loss 10000.0\n",
      "final_loss 589334784.0\n",
      "best_loss 10000.0\n",
      "final_loss 588800384.0\n",
      "best_loss 10000.0\n",
      "final_loss 588267520.0\n",
      "best_loss 10000.0\n",
      "final_loss 587735616.0\n",
      "best_loss 10000.0\n",
      "final_loss 587204736.0\n",
      "best_loss 10000.0\n",
      "final_loss 586675264.0\n",
      "best_loss 10000.0\n",
      "final_loss 586146944.0\n",
      "best_loss 10000.0\n",
      "final_loss 585619840.0\n",
      "best_loss 10000.0\n",
      "final_loss 585094016.0\n",
      "best_loss 10000.0\n",
      "final_loss 584569216.0\n",
      "best_loss 10000.0\n",
      "final_loss 584045504.0\n",
      "best_loss 10000.0\n",
      "final_loss 583522944.0\n",
      "best_loss 10000.0\n",
      "final_loss 583001856.0\n",
      "best_loss 10000.0\n",
      "final_loss 582481664.0\n",
      "best_loss 10000.0\n",
      "final_loss 581962944.0\n",
      "best_loss 10000.0\n",
      "final_loss 581444928.0\n",
      "best_loss 10000.0\n",
      "final_loss 580928256.0\n",
      "best_loss 10000.0\n",
      "final_loss 580412672.0\n",
      "best_loss 10000.0\n",
      "final_loss 579898368.0\n",
      "best_loss 10000.0\n",
      "final_loss 579385152.0\n",
      "best_loss 10000.0\n",
      "final_loss 578872896.0\n",
      "best_loss 10000.0\n",
      "final_loss 578361984.0\n",
      "best_loss 10000.0\n",
      "final_loss 577852096.0\n",
      "best_loss 10000.0\n",
      "final_loss 577343104.0\n",
      "best_loss 10000.0\n",
      "final_loss 576835392.0\n",
      "best_loss 10000.0\n",
      "final_loss 576329088.0\n",
      "best_loss 10000.0\n",
      "final_loss 575823488.0\n",
      "best_loss 10000.0\n",
      "final_loss 575319168.0\n",
      "best_loss 10000.0\n",
      "final_loss 574816000.0\n",
      "best_loss 10000.0\n",
      "final_loss 574313728.0\n",
      "best_loss 10000.0\n",
      "final_loss 573812544.0\n",
      "best_loss 10000.0\n",
      "final_loss 573312768.0\n",
      "best_loss 10000.0\n",
      "final_loss 572813888.0\n",
      "best_loss 10000.0\n",
      "final_loss 572316224.0\n",
      "best_loss 10000.0\n",
      "final_loss 571819392.0\n",
      "best_loss 10000.0\n",
      "final_loss 571323648.0\n",
      "best_loss 10000.0\n",
      "final_loss 570829120.0\n",
      "best_loss 10000.0\n",
      "final_loss 570335616.0\n",
      "best_loss 10000.0\n",
      "final_loss 569843264.0\n",
      "best_loss 10000.0\n",
      "final_loss 569351872.0\n",
      "best_loss 10000.0\n",
      "final_loss 568861632.0\n",
      "best_loss 10000.0\n",
      "final_loss 568372160.0\n",
      "best_loss 10000.0\n",
      "final_loss 567884032.0\n",
      "best_loss 10000.0\n",
      "final_loss 567396672.0\n",
      "best_loss 10000.0\n",
      "final_loss 566910656.0\n",
      "best_loss 10000.0\n",
      "final_loss 566425216.0\n",
      "best_loss 10000.0\n",
      "final_loss 565941376.0\n",
      "best_loss 10000.0\n",
      "final_loss 565458432.0\n",
      "best_loss 10000.0\n",
      "final_loss 564976256.0\n",
      "best_loss 10000.0\n",
      "final_loss 564495104.0\n",
      "best_loss 10000.0\n",
      "final_loss 564015360.0\n",
      "best_loss 10000.0\n",
      "final_loss 563536320.0\n",
      "best_loss 10000.0\n",
      "final_loss 563058304.0\n",
      "best_loss 10000.0\n",
      "final_loss 562581376.0\n",
      "best_loss 10000.0\n",
      "final_loss 562105472.0\n",
      "best_loss 10000.0\n",
      "final_loss 561630528.0\n",
      "best_loss 10000.0\n",
      "final_loss 561156608.0\n",
      "best_loss 10000.0\n",
      "final_loss 560683392.0\n",
      "best_loss 10000.0\n",
      "final_loss 560211584.0\n",
      "best_loss 10000.0\n",
      "final_loss 559740608.0\n",
      "best_loss 10000.0\n",
      "final_loss 559270528.0\n",
      "best_loss 10000.0\n",
      "final_loss 558801792.0\n",
      "best_loss 10000.0\n",
      "final_loss 558333632.0\n",
      "best_loss 10000.0\n",
      "final_loss 557866880.0\n",
      "best_loss 10000.0\n",
      "final_loss 557400576.0\n",
      "best_loss 10000.0\n",
      "final_loss 556935552.0\n",
      "best_loss 10000.0\n",
      "final_loss 556471424.0\n",
      "best_loss 10000.0\n",
      "final_loss 556008192.0\n",
      "best_loss 10000.0\n",
      "final_loss 555546240.0\n",
      "best_loss 10000.0\n",
      "final_loss 555084928.0\n",
      "best_loss 10000.0\n",
      "final_loss 554624576.0\n",
      "best_loss 10000.0\n",
      "final_loss 554165440.0\n",
      "best_loss 10000.0\n",
      "final_loss 553706880.0\n",
      "best_loss 10000.0\n",
      "final_loss 553249536.0\n",
      "best_loss 10000.0\n",
      "final_loss 552793088.0\n",
      "best_loss 10000.0\n",
      "final_loss 552337664.0\n",
      "best_loss 10000.0\n",
      "final_loss 551883008.0\n",
      "best_loss 10000.0\n",
      "final_loss 551429312.0\n",
      "best_loss 10000.0\n",
      "final_loss 550976512.0\n",
      "best_loss 10000.0\n",
      "final_loss 550524608.0\n",
      "best_loss 10000.0\n",
      "final_loss 550073728.0\n",
      "best_loss 10000.0\n",
      "final_loss 549623936.0\n",
      "best_loss 10000.0\n",
      "final_loss 549174976.0\n",
      "best_loss 10000.0\n",
      "final_loss 548726720.0\n",
      "best_loss 10000.0\n",
      "final_loss 548279680.0\n",
      "best_loss 10000.0\n",
      "final_loss 547833216.0\n",
      "best_loss 10000.0\n",
      "final_loss 547387776.0\n",
      "best_loss 10000.0\n",
      "final_loss 546943424.0\n",
      "best_loss 10000.0\n",
      "final_loss 546499648.0\n",
      "best_loss 10000.0\n",
      "final_loss 546057024.0\n",
      "best_loss 10000.0\n",
      "final_loss 545615360.0\n",
      "best_loss 10000.0\n",
      "final_loss 545174528.0\n",
      "best_loss 10000.0\n",
      "final_loss 544734336.0\n",
      "best_loss 10000.0\n",
      "final_loss 544295296.0\n",
      "best_loss 10000.0\n",
      "final_loss 543857024.0\n",
      "best_loss 10000.0\n",
      "final_loss 543419712.0\n",
      "best_loss 10000.0\n",
      "final_loss 542983296.0\n",
      "best_loss 10000.0\n",
      "final_loss 542547584.0\n",
      "best_loss 10000.0\n",
      "final_loss 542112832.0\n",
      "best_loss 10000.0\n",
      "final_loss 541678976.0\n",
      "best_loss 10000.0\n",
      "final_loss 541245952.0\n",
      "best_loss 10000.0\n",
      "final_loss 540813760.0\n",
      "best_loss 10000.0\n",
      "final_loss 540382656.0\n",
      "best_loss 10000.0\n",
      "final_loss 539952128.0\n",
      "best_loss 10000.0\n",
      "final_loss 539522688.0\n",
      "best_loss 10000.0\n",
      "final_loss 539093888.0\n",
      "best_loss 10000.0\n",
      "final_loss 538666112.0\n",
      "best_loss 10000.0\n",
      "final_loss 538239168.0\n",
      "best_loss 10000.0\n",
      "final_loss 537812864.0\n",
      "best_loss 10000.0\n",
      "final_loss 537387520.0\n",
      "best_loss 10000.0\n",
      "final_loss 536963136.0\n",
      "best_loss 10000.0\n",
      "final_loss 536539328.0\n",
      "best_loss 10000.0\n",
      "final_loss 536116640.0\n",
      "best_loss 10000.0\n",
      "final_loss 535694688.0\n",
      "best_loss 10000.0\n",
      "final_loss 535273472.0\n",
      "best_loss 10000.0\n",
      "final_loss 534853312.0\n",
      "best_loss 10000.0\n",
      "final_loss 534433696.0\n",
      "best_loss 10000.0\n",
      "final_loss 534015040.0\n",
      "best_loss 10000.0\n",
      "final_loss 533597248.0\n",
      "best_loss 10000.0\n",
      "final_loss 533180256.0\n",
      "best_loss 10000.0\n",
      "final_loss 532764000.0\n",
      "best_loss 10000.0\n",
      "final_loss 532348736.0\n",
      "best_loss 10000.0\n",
      "final_loss 531934048.0\n",
      "best_loss 10000.0\n",
      "final_loss 531520096.0\n",
      "best_loss 10000.0\n",
      "final_loss 531107264.0\n",
      "best_loss 10000.0\n",
      "final_loss 530695264.0\n",
      "best_loss 10000.0\n",
      "final_loss 530283936.0\n",
      "best_loss 10000.0\n",
      "final_loss 529873312.0\n",
      "best_loss 10000.0\n",
      "final_loss 529463616.0\n",
      "best_loss 10000.0\n",
      "final_loss 529054464.0\n",
      "best_loss 10000.0\n",
      "final_loss 528646240.0\n",
      "best_loss 10000.0\n",
      "final_loss 528238912.0\n",
      "best_loss 10000.0\n",
      "final_loss 527832224.0\n",
      "best_loss 10000.0\n",
      "final_loss 527426464.0\n",
      "best_loss 10000.0\n",
      "final_loss 527021408.0\n",
      "best_loss 10000.0\n",
      "final_loss 526616992.0\n",
      "best_loss 10000.0\n",
      "final_loss 526213536.0\n",
      "best_loss 10000.0\n",
      "final_loss 525811008.0\n",
      "best_loss 10000.0\n",
      "final_loss 525408928.0\n",
      "best_loss 10000.0\n",
      "final_loss 525007616.0\n",
      "best_loss 10000.0\n",
      "final_loss 524607328.0\n",
      "best_loss 10000.0\n",
      "final_loss 524207712.0\n",
      "best_loss 10000.0\n",
      "final_loss 523808768.0\n",
      "best_loss 10000.0\n",
      "final_loss 523410624.0\n",
      "best_loss 10000.0\n",
      "final_loss 523013280.0\n",
      "best_loss 10000.0\n",
      "final_loss 522616512.0\n",
      "best_loss 10000.0\n",
      "final_loss 522220864.0\n",
      "best_loss 10000.0\n",
      "final_loss 521825696.0\n",
      "best_loss 10000.0\n",
      "final_loss 521431360.0\n",
      "best_loss 10000.0\n",
      "final_loss 521037728.0\n",
      "best_loss 10000.0\n",
      "final_loss 520644864.0\n",
      "best_loss 10000.0\n",
      "final_loss 520252480.0\n",
      "best_loss 10000.0\n",
      "final_loss 519861152.0\n",
      "best_loss 10000.0\n",
      "final_loss 519470432.0\n",
      "best_loss 10000.0\n",
      "final_loss 519080544.0\n",
      "best_loss 10000.0\n",
      "final_loss 518691264.0\n",
      "best_loss 10000.0\n",
      "final_loss 518302720.0\n",
      "best_loss 10000.0\n",
      "final_loss 517915296.0\n",
      "best_loss 10000.0\n",
      "final_loss 517528160.0\n",
      "best_loss 10000.0\n",
      "final_loss 517141952.0\n",
      "best_loss 10000.0\n",
      "final_loss 516756224.0\n",
      "best_loss 10000.0\n",
      "final_loss 516371456.0\n",
      "best_loss 10000.0\n",
      "final_loss 515987296.0\n",
      "best_loss 10000.0\n",
      "final_loss 515603968.0\n",
      "best_loss 10000.0\n",
      "final_loss 515221152.0\n",
      "best_loss 10000.0\n",
      "final_loss 514839296.0\n",
      "best_loss 10000.0\n",
      "final_loss 514457952.0\n",
      "best_loss 10000.0\n",
      "final_loss 514077504.0\n",
      "best_loss 10000.0\n",
      "final_loss 513697536.0\n",
      "best_loss 10000.0\n",
      "final_loss 513318208.0\n",
      "best_loss 10000.0\n",
      "final_loss 512939776.0\n",
      "best_loss 10000.0\n",
      "final_loss 512562176.0\n",
      "best_loss 10000.0\n",
      "final_loss 512184992.0\n",
      "best_loss 10000.0\n",
      "final_loss 511808448.0\n",
      "best_loss 10000.0\n",
      "final_loss 511432800.0\n",
      "best_loss 10000.0\n",
      "final_loss 511057824.0\n",
      "best_loss 10000.0\n",
      "final_loss 510683712.0\n",
      "best_loss 10000.0\n",
      "final_loss 510309984.0\n",
      "best_loss 10000.0\n",
      "final_loss 509937056.0\n",
      "best_loss 10000.0\n",
      "final_loss 509564928.0\n",
      "best_loss 10000.0\n",
      "final_loss 509193312.0\n",
      "best_loss 10000.0\n",
      "final_loss 508822336.0\n",
      "best_loss 10000.0\n",
      "final_loss 508452256.0\n",
      "best_loss 10000.0\n",
      "final_loss 508082624.0\n",
      "best_loss 10000.0\n",
      "final_loss 507713536.0\n",
      "best_loss 10000.0\n",
      "final_loss 507345472.0\n",
      "best_loss 10000.0\n",
      "final_loss 506977856.0\n",
      "best_loss 10000.0\n",
      "final_loss 506611040.0\n",
      "best_loss 10000.0\n",
      "final_loss 506244928.0\n",
      "best_loss 10000.0\n",
      "final_loss 505879488.0\n",
      "best_loss 10000.0\n",
      "final_loss 505514496.0\n",
      "best_loss 10000.0\n",
      "final_loss 505150208.0\n",
      "best_loss 10000.0\n",
      "final_loss 504786848.0\n",
      "best_loss 10000.0\n",
      "final_loss 504424000.0\n",
      "best_loss 10000.0\n",
      "final_loss 504061600.0\n",
      "best_loss 10000.0\n",
      "final_loss 503699872.0\n",
      "best_loss 10000.0\n",
      "final_loss 503339008.0\n",
      "best_loss 10000.0\n",
      "final_loss 502978752.0\n",
      "best_loss 10000.0\n",
      "final_loss 502619072.0\n",
      "best_loss 10000.0\n",
      "final_loss 502260224.0\n",
      "best_loss 10000.0\n",
      "final_loss 501901728.0\n",
      "best_loss 10000.0\n",
      "final_loss 501544096.0\n",
      "best_loss 10000.0\n",
      "final_loss 501186912.0\n",
      "best_loss 10000.0\n",
      "final_loss 500830400.0\n",
      "best_loss 10000.0\n",
      "final_loss 500474688.0\n",
      "best_loss 10000.0\n",
      "final_loss 500119296.0\n",
      "best_loss 10000.0\n",
      "final_loss 499764896.0\n",
      "best_loss 10000.0\n",
      "final_loss 499411136.0\n",
      "best_loss 10000.0\n",
      "final_loss 499057600.0\n",
      "best_loss 10000.0\n",
      "final_loss 498705216.0\n",
      "best_loss 10000.0\n",
      "final_loss 498353056.0\n",
      "best_loss 10000.0\n",
      "final_loss 498001600.0\n",
      "best_loss 10000.0\n",
      "final_loss 497650688.0\n",
      "best_loss 10000.0\n",
      "final_loss 497300640.0\n",
      "best_loss 10000.0\n",
      "final_loss 496951136.0\n",
      "best_loss 10000.0\n",
      "final_loss 496602176.0\n",
      "best_loss 10000.0\n",
      "final_loss 496253760.0\n",
      "best_loss 10000.0\n",
      "final_loss 495906112.0\n",
      "best_loss 10000.0\n",
      "final_loss 495559008.0\n",
      "best_loss 10000.0\n",
      "final_loss 495212288.0\n",
      "best_loss 10000.0\n",
      "final_loss 494866688.0\n",
      "best_loss 10000.0\n",
      "final_loss 494521344.0\n",
      "best_loss 10000.0\n",
      "final_loss 494176704.0\n",
      "best_loss 10000.0\n",
      "final_loss 493832544.0\n",
      "best_loss 10000.0\n",
      "final_loss 493489056.0\n",
      "best_loss 10000.0\n",
      "final_loss 493146176.0\n",
      "best_loss 10000.0\n",
      "final_loss 492803904.0\n",
      "best_loss 10000.0\n",
      "final_loss 492462272.0\n",
      "best_loss 10000.0\n",
      "final_loss 492121184.0\n",
      "best_loss 10000.0\n",
      "final_loss 491780544.0\n",
      "best_loss 10000.0\n",
      "final_loss 491440704.0\n",
      "best_loss 10000.0\n",
      "final_loss 491101376.0\n",
      "best_loss 10000.0\n",
      "final_loss 490762656.0\n",
      "best_loss 10000.0\n",
      "final_loss 490424576.0\n",
      "best_loss 10000.0\n",
      "final_loss 490086912.0\n",
      "best_loss 10000.0\n",
      "final_loss 489749856.0\n",
      "best_loss 10000.0\n",
      "final_loss 489413536.0\n",
      "best_loss 10000.0\n",
      "final_loss 489077664.0\n",
      "best_loss 10000.0\n",
      "final_loss 488742400.0\n",
      "best_loss 10000.0\n",
      "final_loss 488407808.0\n",
      "best_loss 10000.0\n",
      "final_loss 488073536.0\n",
      "best_loss 10000.0\n",
      "final_loss 487740064.0\n",
      "best_loss 10000.0\n",
      "final_loss 487407168.0\n",
      "best_loss 10000.0\n",
      "final_loss 487074656.0\n",
      "best_loss 10000.0\n",
      "final_loss 486742944.0\n",
      "best_loss 10000.0\n",
      "final_loss 486411712.0\n",
      "best_loss 10000.0\n",
      "final_loss 486081024.0\n",
      "best_loss 10000.0\n",
      "final_loss 485750880.0\n",
      "best_loss 10000.0\n",
      "final_loss 485421216.0\n",
      "best_loss 10000.0\n",
      "final_loss 485092352.0\n",
      "best_loss 10000.0\n",
      "final_loss 484763840.0\n",
      "best_loss 10000.0\n",
      "final_loss 484435872.0\n",
      "best_loss 10000.0\n",
      "final_loss 484108608.0\n",
      "best_loss 10000.0\n",
      "final_loss 483781792.0\n",
      "best_loss 10000.0\n",
      "final_loss 483455552.0\n",
      "best_loss 10000.0\n",
      "final_loss 483129856.0\n",
      "best_loss 10000.0\n",
      "final_loss 482804928.0\n",
      "best_loss 10000.0\n",
      "final_loss 482480288.0\n",
      "best_loss 10000.0\n",
      "final_loss 482156384.0\n",
      "best_loss 10000.0\n",
      "final_loss 481832864.0\n",
      "best_loss 10000.0\n",
      "final_loss 481509888.0\n",
      "best_loss 10000.0\n",
      "final_loss 481187424.0\n",
      "best_loss 10000.0\n",
      "final_loss 480865472.0\n",
      "best_loss 10000.0\n",
      "final_loss 480544256.0\n",
      "best_loss 10000.0\n",
      "final_loss 480223648.0\n",
      "best_loss 10000.0\n",
      "final_loss 479903296.0\n",
      "best_loss 10000.0\n",
      "final_loss 479583488.0\n",
      "best_loss 10000.0\n",
      "final_loss 479264256.0\n",
      "best_loss 10000.0\n",
      "final_loss 478945696.0\n",
      "best_loss 10000.0\n",
      "final_loss 478627520.0\n",
      "best_loss 10000.0\n",
      "final_loss 478309984.0\n",
      "best_loss 10000.0\n",
      "final_loss 477993024.0\n",
      "best_loss 10000.0\n",
      "final_loss 477676352.0\n",
      "best_loss 10000.0\n",
      "final_loss 477360384.0\n",
      "best_loss 10000.0\n",
      "final_loss 477045056.0\n",
      "best_loss 10000.0\n",
      "final_loss 476730048.0\n",
      "best_loss 10000.0\n",
      "final_loss 476415424.0\n",
      "best_loss 10000.0\n",
      "final_loss 476101696.0\n",
      "best_loss 10000.0\n",
      "final_loss 475788128.0\n",
      "best_loss 10000.0\n",
      "final_loss 475475392.0\n",
      "best_loss 10000.0\n",
      "final_loss 475163040.0\n",
      "best_loss 10000.0\n",
      "final_loss 474851136.0\n",
      "best_loss 10000.0\n",
      "final_loss 474539840.0\n",
      "best_loss 10000.0\n",
      "final_loss 474228800.0\n",
      "best_loss 10000.0\n",
      "final_loss 473918656.0\n",
      "best_loss 10000.0\n",
      "final_loss 473608896.0\n",
      "best_loss 10000.0\n",
      "final_loss 473299456.0\n",
      "best_loss 10000.0\n",
      "final_loss 472990656.0\n",
      "best_loss 10000.0\n",
      "final_loss 472682496.0\n",
      "best_loss 10000.0\n",
      "final_loss 472374688.0\n",
      "best_loss 10000.0\n",
      "final_loss 472067392.0\n",
      "best_loss 10000.0\n",
      "final_loss 471760480.0\n",
      "best_loss 10000.0\n",
      "final_loss 471454272.0\n",
      "best_loss 10000.0\n",
      "final_loss 471148352.0\n",
      "best_loss 10000.0\n",
      "final_loss 470843200.0\n",
      "best_loss 10000.0\n",
      "final_loss 470538240.0\n",
      "best_loss 10000.0\n",
      "final_loss 470234016.0\n",
      "best_loss 10000.0\n",
      "final_loss 469930304.0\n",
      "best_loss 10000.0\n",
      "final_loss 469626880.0\n",
      "best_loss 10000.0\n",
      "final_loss 469324128.0\n",
      "best_loss 10000.0\n",
      "final_loss 469021856.0\n",
      "best_loss 10000.0\n",
      "final_loss 468719968.0\n",
      "best_loss 10000.0\n",
      "final_loss 468418560.0\n",
      "best_loss 10000.0\n",
      "final_loss 468117696.0\n",
      "best_loss 10000.0\n",
      "final_loss 467817216.0\n",
      "best_loss 10000.0\n",
      "final_loss 467517344.0\n",
      "best_loss 10000.0\n",
      "final_loss 467217856.0\n",
      "best_loss 10000.0\n",
      "final_loss 466918976.0\n",
      "best_loss 10000.0\n",
      "final_loss 466620608.0\n",
      "best_loss 10000.0\n",
      "final_loss 466322368.0\n",
      "best_loss 10000.0\n",
      "final_loss 466025120.0\n",
      "best_loss 10000.0\n",
      "final_loss 465727936.0\n",
      "best_loss 10000.0\n",
      "final_loss 465431360.0\n",
      "best_loss 10000.0\n",
      "final_loss 465135296.0\n",
      "best_loss 10000.0\n",
      "final_loss 464839680.0\n",
      "best_loss 10000.0\n",
      "final_loss 464544512.0\n",
      "best_loss 10000.0\n",
      "final_loss 464249792.0\n",
      "best_loss 10000.0\n",
      "final_loss 463955616.0\n",
      "best_loss 10000.0\n",
      "final_loss 463661920.0\n",
      "best_loss 10000.0\n",
      "final_loss 463368512.0\n",
      "best_loss 10000.0\n",
      "final_loss 463075648.0\n",
      "best_loss 10000.0\n",
      "final_loss 462783392.0\n",
      "best_loss 10000.0\n",
      "final_loss 462491392.0\n",
      "best_loss 10000.0\n",
      "final_loss 462199968.0\n",
      "best_loss 10000.0\n",
      "final_loss 461909152.0\n",
      "best_loss 10000.0\n",
      "final_loss 461618592.0\n",
      "best_loss 10000.0\n",
      "final_loss 461328544.0\n",
      "best_loss 10000.0\n",
      "final_loss 461038848.0\n",
      "best_loss 10000.0\n",
      "final_loss 460749824.0\n",
      "best_loss 10000.0\n",
      "final_loss 460460992.0\n",
      "best_loss 10000.0\n",
      "final_loss 460172736.0\n",
      "best_loss 10000.0\n",
      "final_loss 459884992.0\n",
      "best_loss 10000.0\n",
      "final_loss 459597632.0\n",
      "best_loss 10000.0\n",
      "final_loss 459310784.0\n",
      "best_loss 10000.0\n",
      "final_loss 459024384.0\n",
      "best_loss 10000.0\n",
      "final_loss 458738240.0\n",
      "best_loss 10000.0\n",
      "final_loss 458452896.0\n",
      "best_loss 10000.0\n",
      "final_loss 458167648.0\n",
      "best_loss 10000.0\n",
      "final_loss 457882976.0\n",
      "best_loss 10000.0\n",
      "final_loss 457598656.0\n",
      "best_loss 10000.0\n",
      "final_loss 457314912.0\n",
      "best_loss 10000.0\n",
      "final_loss 457031744.0\n",
      "best_loss 10000.0\n",
      "final_loss 456748864.0\n",
      "best_loss 10000.0\n",
      "final_loss 456466368.0\n",
      "best_loss 10000.0\n",
      "final_loss 456184416.0\n",
      "best_loss 10000.0\n",
      "final_loss 455902560.0\n",
      "best_loss 10000.0\n",
      "final_loss 455621472.0\n",
      "best_loss 10000.0\n",
      "final_loss 455341056.0\n",
      "best_loss 10000.0\n",
      "final_loss 455060640.0\n",
      "best_loss 10000.0\n",
      "final_loss 454780768.0\n",
      "best_loss 10000.0\n",
      "final_loss 454501216.0\n",
      "best_loss 10000.0\n",
      "final_loss 454222400.0\n",
      "best_loss 10000.0\n",
      "final_loss 453943744.0\n",
      "best_loss 10000.0\n",
      "final_loss 453665600.0\n",
      "best_loss 10000.0\n",
      "final_loss 453388032.0\n",
      "best_loss 10000.0\n",
      "final_loss 453110688.0\n",
      "best_loss 10000.0\n",
      "final_loss 452833728.0\n",
      "best_loss 10000.0\n",
      "final_loss 452557408.0\n",
      "best_loss 10000.0\n",
      "final_loss 452281344.0\n",
      "best_loss 10000.0\n",
      "final_loss 452005792.0\n",
      "best_loss 10000.0\n",
      "final_loss 451730624.0\n",
      "best_loss 10000.0\n",
      "final_loss 451455936.0\n",
      "best_loss 10000.0\n",
      "final_loss 451181504.0\n",
      "best_loss 10000.0\n",
      "final_loss 450907552.0\n",
      "best_loss 10000.0\n",
      "final_loss 450634080.0\n",
      "best_loss 10000.0\n",
      "final_loss 450360832.0\n",
      "best_loss 10000.0\n",
      "final_loss 450088192.0\n",
      "best_loss 10000.0\n",
      "final_loss 449815904.0\n",
      "best_loss 10000.0\n",
      "final_loss 449544256.0\n",
      "best_loss 10000.0\n",
      "final_loss 449272768.0\n",
      "best_loss 10000.0\n",
      "Early stopping at epoch 1001\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_epoch' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[341], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, likelihood, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAINING_ITERATIONS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m likelihood\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[332], line 84\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_x, train_g, val_x, val_g, training_iterations)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_train_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and val. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_val_loss, best_train_loss\n\u001b[1;32m---> 84\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Load the best model state\u001b[39;00m\n\u001b[0;32m     87\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_GPR.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[332], line 82\u001b[0m, in \u001b[0;36mtrain_model.<locals>.train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m     validation_losses\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# print(f'Epoch {epoch + 1} - Training Loss: {loss.item()} - Validation Loss: {val_loss}')\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_train_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and val. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_val_loss, best_train_loss\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'best_epoch' referenced before assignment"
     ]
    }
   ],
   "source": [
    "model, likelihood, best_loss = train_model(train_x, train_g, val_x, val_g, TRAINING_ITERATIONS)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "it = 0\n",
    "print(f'\\nIteration {it}. Best of DGPR: {torch.min(f)}. GPR model: Train loss = {best_loss[0]}; Val. loss = {best_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while it < N_INFILL:\n",
    "    it += 1\n",
    "    \n",
    "    # Search for the maximum expected improvement\n",
    "    new_point = bsa(expected_improvement, bounds=BOUNDS_BSA,\n",
    "                    popsize=BSA_POPSIZE, epoch=BSA_EPOCH, data=model)\n",
    "    x_new = torch.from_numpy(new_point.x)\n",
    "    EI = new_point.y\n",
    "\n",
    "    # Objective function at the new point\n",
    "    f_new = obj_fun(x_new)\n",
    "    \n",
    "    print(f'Iteration {it} of {N_INFILL}')\n",
    "    if f_new < torch.min(f): print(f'New best: {float(f_new):.2f} at position {it}')\n",
    "    \n",
    "    # Add new values to the initial sampling\n",
    "    x = torch.cat((x, torch.from_numpy(np.array([np.asarray(x_new)]))), 0)\n",
    "    x = x.to(torch.float32)\n",
    "    f_new = f_new.view(-1)\n",
    "    f = torch.cat((f, f_new), 0)\n",
    "    \n",
    "    # Update model\n",
    "    train_x, val_x, train_g, val_g = train_test_split(x, f, test_size=0.20)\n",
    "    model, likelihood, best_loss = train_model(train_x, train_g, val_x, val_g, TRAINING_ITERATIONS)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    print(f'\\nIteration {it}. Best of DGPR: {torch.min(f)}. GPR model: Train loss = {best_loss[0]}; Val. loss = {best_loss[-1]}')\n",
    "    \n",
    "    # if abs(EI) < TOL_MIN_EI:\n",
    "    #     print('Optimization finished. Minimum tolerance achieved.')\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'f*: {torch.min(f):.2f}; x*: {x[torch.argmin(f), :]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results[\"x\"] = x\n",
    "results[\"f\"] = f\n",
    "results[\"n_initial\"] = N_INITIAL\n",
    "results[\"n_infill\"] = N_INFILL\n",
    "results[\"OFEs\"] = N_INITIAL + it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_f, name_x = 'f.mat', 'x.mat'\n",
    "path = './'\n",
    "pathname_f = path + name_f\n",
    "pathname_x = path + name_x\n",
    "\n",
    "sio.savemat(name_f, {'f': f.numpy()})\n",
    "sio.savemat(name_x, {'x': x.numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using BSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPSIZE = 11\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bsa = bsa(obj_fun, bounds=BOUNDS_BSA,\n",
    "                    popsize=POPSIZE, epoch=EPOCHS, data=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f* = {results_bsa.y:.2f}; x* = {results_bsa.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot optimization history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the cost history\n",
    "plt.figure(figsize=(8, 6))  # Optional: Set figure size\n",
    "plt.plot(np.arange(EPOCHS), results.convergence, marker='o', linestyle='-', color='b', label='Cost History')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost History per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)  # Optional: Add grid for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sair da port do mapdl e fechar o processo\n",
    "mapdl.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
