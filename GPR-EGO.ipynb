{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Global Optimization employing Deep Gaussian Process Regression\n",
    "\n",
    "See DGPR implementations in:\n",
    "\n",
    "https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/KISSGP_Deep_Kernel_Regression_CUDA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import erf\n",
    "import scipy.io as sio\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, init\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.optim import Adam\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, MaternKernel, RBFKernel\n",
    "from gpytorch.utils.grid import ScaleToBounds\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.settings import use_toeplitz, fast_pred_var\n",
    "from gpytorch.constraints.constraints import GreaterThan\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from bsa import bsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 42 # CHANGE SEED IN EACH DIFFERENT INDEPENDENT RUN!\n",
    "\n",
    "# For NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# For PyTorch\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "\n",
    "# For Python's built-in random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensuring reproducibility in cuDNN using PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *GPR Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "        self.scale_to_bounds = ScaleToBounds(-1., 1.)\n",
    "        \n",
    "        # Store train_y for later use\n",
    "        self.train_outputs = train_y\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected_x = self.scale_to_bounds(x)\n",
    "        mean_x = self.mean_module(projected_x)\n",
    "        covar_x = self.covar_module(projected_x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_g, val_x, val_g, training_iterations):\n",
    "    # Initialize the models and likelihood\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model = GPRegressionModel(train_x=train_x, train_y=train_g, likelihood=likelihood)\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     model = model.cuda()\n",
    "    #     likelihood = likelihood.cuda()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = Adam([\n",
    "        {'params': model.covar_module.parameters()},\n",
    "        {'params': model.mean_module.parameters()},\n",
    "        {'params': model.likelihood.parameters()},\n",
    "    ], lr=0.005)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # To track loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    # Training loop with validation\n",
    "    def train():\n",
    "        best_loss, best_val_loss, best_train_loss = 1e4, 1e4, 1e4\n",
    "        patience = int(training_iterations * 0.1)\n",
    "        wait = 0\n",
    "\n",
    "        for epoch in range(training_iterations):\n",
    "            model.train()\n",
    "            likelihood.train()\n",
    "\n",
    "            # Zero backprop gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass and calculate loss on training set\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_g)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            likelihood.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = model(val_x)\n",
    "                val_loss = -mll(val_output, val_g).item()\n",
    "\n",
    "            # Save the best model based on validation and training loss\n",
    "            training_loss = loss.item()\n",
    "            final_loss = val_loss # val_loss*0.5 + training_loss*0.5\n",
    "            if final_loss < best_loss:\n",
    "                best_loss = final_loss\n",
    "                best_val_loss = val_loss\n",
    "                best_train_loss = training_loss\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'best_model_GPR.pth')\n",
    "                wait = 0  # Reset patience counter when improvement is found\n",
    "            else:\n",
    "                wait += 1  # Increment patience counter if no improvement\n",
    "\n",
    "            # Early stopping\n",
    "            if wait > patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "            # Track losses for plotting\n",
    "            training_losses.append(loss.item())\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            # print(f'Epoch {epoch + 1} - Training Loss: {loss.item()} - Validation Loss: {val_loss}')\n",
    "\n",
    "        print(f'Best Loss: {best_loss} at epoch {best_epoch}. Training loss: {best_train_loss} and val. loss: {best_val_loss}')\n",
    "        return best_val_loss, best_train_loss\n",
    "    best_loss = train()\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load('best_model_GPR.pth'))\n",
    "\n",
    "    # Set model and likelihood to eval mode for further evaluation\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_losses, label='Training Loss')\n",
    "    plt.plot(validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model, likelihood, best_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial sampling plan (Latin Hypercube sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LHS_sample(num_points, DIM):\n",
    "    # Number of variables and sampling points\n",
    "    size = int(num_points)\n",
    "\n",
    "    # Generate LHS samples\n",
    "    sampler = qmc.LatinHypercube(d=DIM,\n",
    "                                 optimization=\"random-cd\")\n",
    "    lhs_sample = sampler.random(n=size)\n",
    "\n",
    "    return lhs_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, model):\n",
    "    \"\"\"\n",
    "    Function to calculate the Expected Improvement using Monte Carlo Integration.\n",
    "    \n",
    "    Parameters:\n",
    "        x (array): Individual under evaluation.\n",
    "        y (array): Valor mínimo obtido até o momento.\n",
    "    \n",
    "    Returns:\n",
    "        array: The value of the Expected Improvement.\n",
    "    \"\"\"\n",
    "    # Get the minimum value obtained so far\n",
    "    ymin = torch.min(model.train_outputs)\n",
    "    # ymin = torch.Tensor(ymin)\n",
    "\n",
    "    # Calculate the prediction value and the variance (Ssqr)\n",
    "    with torch.no_grad(), use_toeplitz(False), fast_pred_var():\n",
    "        preds = model(x)\n",
    "    f = preds.mean\n",
    "    s = preds.variance\n",
    "\n",
    "    # Check for any errors that are less than zero (due to numerical error)\n",
    "    s[s < 0] = 0  # Set negative variances to zero\n",
    "\n",
    "    # Calculate the RMSE (Root Mean Square Error)\n",
    "    s = torch.sqrt(s)\n",
    "\n",
    "    # Calculation of Expected Improvement\n",
    "    term1 = (ymin - f) * (0.5 + 0.5 * erf((ymin - f) / (s * torch.sqrt( torch.from_numpy(np.array([2])) ))))\n",
    "    term2 = (1 / torch.sqrt(2 * torch.from_numpy(np.array([np.pi])))) * s * torch.exp(-((ymin - f) ** 2) / (2 * s ** 2))\n",
    "    \n",
    "    return -(term1 + term2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 10000  # epochs for training the GPR\n",
    "N_INITIAL = 50  # initial number of points\n",
    "N_INFILL = 500  # number of infill points\n",
    "BSA_POPSIZE = 50\n",
    "BSA_EPOCH = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 5 # dimension of your problem (number of design variables)\n",
    "BOUNDS_BSA = (\n",
    "    tuple((0, 1) for _ in range(DIM)) # the number of (0, 1) tuples has to be equal to DIM\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "Your objective function must go inside obj_fun. The input is *xx*, your design variables vector, and the output is the objective function value (your own cost function).\n",
    "\n",
    "You can either import your objective function from a *.py* file using `import obj_fun from my_file` (where *my_file.py* contains your objective function that is called *obj_fun*, already organized to receive a design variable vector (or matrix) as input and output an objective function value) or just replace the *obj_fun* code below by your entire code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Definindo variáveis apenas uma vez\n",
    "raio_estaca = 0.5\n",
    "distancia_minima = raio_estaca * 2.5  # 5 vezes o raio da estaca\n",
    "largura = 6\n",
    "altura = 3\n",
    "n_estacas = 2\n",
    "n_estacas_centrais = round(1) \n",
    "L_estacas=20\n",
    "\n",
    "#FUNÇÕES AUXILIARES\n",
    "def round_to_nearest(value, targets=[0, 0.333, 0.666, 1]): #Arrendonda o valor de angulo vertical para o valor mais próximo a fim de trabalhar dentro de limites construtivos de equipamento e também trabalhar com um menor espaço amostral                    \n",
    "    return min(targets, key=lambda t: abs(value - t))\n",
    "\n",
    "def distancia_entre_pontos(p1, p2): #Mede a distancia entre pontos a fim de assegurar limites geométricos da estrutura\n",
    "    return math.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\n",
    "\n",
    "def espelhar_pontos(coordinates, n_estacas_centrais): \n",
    "    espelho = [(x, -y, z) for x, y, z in coordinates[:-int(n_estacas_centrais)]]\n",
    "    return espelho\n",
    "\n",
    "def converter_x(x_normalizado): #Converte os valores do de interesse em coordenadas \"X\"\n",
    "    return raio_estaca + x_normalizado * (largura-2*raio_estaca)\n",
    "\n",
    "def converter_y(y_normalizado): #Converte os valores do de interesse em coordenadas \"Y\"\n",
    "    return raio_estaca + y_normalizado * (altura-2*raio_estaca)\n",
    "\n",
    "def converter_angulos_verticais(array_angulos_verticais): #Converte os valores do de interesse em angulos trigonométricos\n",
    "    return [np.round(75 + 15 * ang_v, 2) for ang_v in array_angulos_verticais]\n",
    "\n",
    "def converter_angulos_horizontais(array_angulos_horizontais): #Converte os valores do de interesse em angulos trigonométricos\n",
    "    return [np.round(46 + 268 * ang_h, 2) for ang_h in array_angulos_horizontais]\n",
    "\n",
    "def spherical_to_cartesian(x_inicial, y_inicial, z_inicial, L_estacas, angulos_verticais_theta, angulos_horizontais_theta): #Converte os valores do de interesse em coordendas cartesianas\n",
    "    angulo_vertical_rad = np.deg2rad(angulos_verticais_theta)\n",
    "    angulo_horizontal_rad = np.deg2rad(angulos_horizontais_theta)\n",
    "    \n",
    "    componente_x = L_estacas * np.cos(angulo_vertical_rad) * np.cos(angulo_horizontal_rad)\n",
    "    componente_y = L_estacas * np.cos(angulo_vertical_rad) * np.sin(angulo_horizontal_rad)\n",
    "    componente_z = L_estacas * np.sin(angulo_vertical_rad)\n",
    "    \n",
    "    x_final = x_inicial + componente_x\n",
    "    y_final = y_inicial + componente_y\n",
    "    z_final = z_inicial + componente_z\n",
    "\n",
    "    return x_final, y_final, z_final\n",
    "\n",
    "def gerar_pontos_finais(pontos, angulos_verticais_theta, angulos_horizontais_theta): #Gera pontos finais\n",
    "    coordenadas_finais = []\n",
    "    for i in range(len(pontos)):\n",
    "        x, y, z = pontos[i]\n",
    "        theta_v, theta_h = angulos_verticais_theta[i], angulos_horizontais_theta[i]\n",
    "        x_final, y_final, z_final = spherical_to_cartesian(x, y, z, L_estacas, theta_v, theta_h)\n",
    "        coordenadas_finais.append((round(x_final, 2), round(y_final, 2), -1*round(z_final, 2)))\n",
    "    return coordenadas_finais\n",
    "\n",
    "def espelhar_pontos_finais(pontos, k):\n",
    "    espelho = []\n",
    "    for (x, y, z) in pontos[:-k]:\n",
    "        y_espelhado = -y\n",
    "        espelho.append((x, y_espelhado, z))\n",
    "    return espelho\n",
    "\n",
    "def distancia_entre_pontos_r3(p1, p2): #Garante limites geométricos/normatívos da estrutura\n",
    "    return round(np.linalg.norm(p1 - p2), 2)\n",
    "\n",
    "def encontrar_menor_distancia_entre_vetores(pontos_iniciais, pontos_finais, k, l, comprimento): #Garante limites geométricos/normatívos da estrutura\n",
    "    vetor_k = pontos_finais[k] - pontos_iniciais[k]\n",
    "    vetor_l = pontos_finais[l] - pontos_iniciais[l]\n",
    "\n",
    "    vetor_dividido_k = vetor_k / comprimento\n",
    "    vetor_dividido_l = vetor_l / comprimento\n",
    "\n",
    "    menor_distancia = float('inf')\n",
    "    ponto_menor_distancia_v1 = None\n",
    "    ponto_menor_distancia_v2 = None\n",
    "    N_v1 = None\n",
    "    N_v2 = None\n",
    "\n",
    "    for N1 in np.arange(1, 19, 1):\n",
    "        for N2 in np.arange(1, 19, 1):\n",
    "            ponto_v1 = pontos_iniciais[k] + N1 * vetor_dividido_k\n",
    "            ponto_v2 = pontos_iniciais[l] + N2 * vetor_dividido_l\n",
    "\n",
    "            dist = round(distancia_entre_pontos_r3(ponto_v1, ponto_v2), 2)\n",
    "\n",
    "            if dist < menor_distancia:\n",
    "                menor_distancia = dist\n",
    "                ponto_menor_distancia_v1 = ponto_v1\n",
    "                ponto_menor_distancia_v2 = ponto_v2\n",
    "                N_v1 = N1\n",
    "                N_v2 = N2\n",
    "\n",
    "    return menor_distancia, ponto_menor_distancia_v1, ponto_menor_distancia_v2, N_v1, N_v2\n",
    "\n",
    "def gerar_lista_k(array_angulos_verticais, n_estacas, n_estacas_centrais): #Gera lista auxiliar\n",
    "    indices = list(range(len(array_angulos_verticais)))\n",
    "    indices_menores = sorted(range(len(array_angulos_verticais) - 1), key=lambda i: array_angulos_verticais[i])[:n_estacas]\n",
    "    lista_k = [i for i in indices_menores]\n",
    "    lista_k.append(2)\n",
    "    if len(lista_k) < n_estacas + n_estacas_centrais and lista_k.count(2) < 2:\n",
    "        lista_k.append(2)\n",
    "    return lista_k\n",
    "\n",
    "#Garante limites geométricos/normatívos da estrutura, tentando otimizar brevemente a estrutura, caso não seja possível, penaliza ela\n",
    "def verificar_e_ajustar_pontos(pontos, L_estacas, distancia_minima, pontos_finais, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos, array_angulos_horizontais,p):\n",
    "    max_iteracoes=1*(n_estacas+n_estacas_centrais+n_estacas)\n",
    "    iteracao = 0\n",
    "    pontos_finais = np.array(pontos_finais)\n",
    "    incremento = 0.05\n",
    "    print(\"P punido pela geometria (estacas dentro de estacas):\",p)\n",
    "    lista_k = gerar_lista_k(array_angulos_verticais_convertidos, n_estacas, n_estacas_centrais)\n",
    "    trial=5\n",
    "\n",
    "    for k in lista_k:\n",
    "        while iteracao <= max_iteracoes:\n",
    "            ajustou_ponto = False\n",
    "\n",
    "            for l in range(len(pontos_finais)):\n",
    "                if k == l:\n",
    "                    continue\n",
    "                ponto1 = pontos_finais[k]\n",
    "                ponto2 = pontos_finais[l]\n",
    "                dist = distancia_entre_pontos_r3(ponto1, ponto2)\n",
    "                menor_distancia, _, _, _, _ = encontrar_menor_distancia_entre_vetores(pontos, pontos_finais, k, l, L_estacas)\n",
    "                if menor_distancia < 3 * raio_estaca or dist < 5 * raio_estaca:\n",
    "                    ajustou_ponto = True\n",
    "                    if k <= n_estacas:\n",
    "                        array_angulos_horizontais_convertidos[k] = np.round(array_angulos_horizontais_convertidos[k]+incremento,2)\n",
    "                        array_angulos_horizontais[k] = np.round(array_angulos_horizontais[k]+incremento,2)\n",
    "                    else:\n",
    "                        # Alterna entre 0/180 e 0/1 usando (-1)**trial\n",
    "                        if (-1) ** trial == 1:\n",
    "                            array_angulos_horizontais_convertidos[k] = 180\n",
    "                            array_angulos_horizontais[k] = 1\n",
    "                        else:\n",
    "                            array_angulos_horizontais_convertidos[k] = 0\n",
    "                            array_angulos_horizontais[k] = 0\n",
    "\n",
    "                    x_novo, y_novo, z_novo = spherical_to_cartesian(\n",
    "                        pontos[k][0], pontos[k][1], pontos[k][2],\n",
    "                        L_estacas, array_angulos_verticais_convertidos[k], array_angulos_horizontais_convertidos[k]\n",
    "                    )\n",
    "                    x = np.round(x_novo, 2)\n",
    "                    y = np.round(y_novo, 2)\n",
    "                    z = -1*np.round(z_novo, 2)\n",
    "\n",
    "                    if k < n_estacas:\n",
    "                        pontos_finais[k, :] = [x, y, z]\n",
    "                        pontos_finais[k + n_estacas + n_estacas_centrais, :] = [x, -y, z]\n",
    "                    else:\n",
    "                        pontos_finais[k, :] = [x, y, z]\n",
    "                    break\n",
    "\n",
    "            if not ajustou_ponto:\n",
    "                break\n",
    "            trial += 1\n",
    "            \n",
    "            if trial >= max_iteracoes:\n",
    "                p+= ((3 * raio_estaca)/(0.1+menor_distancia)+(5 * raio_estaca)/(0.1+dist))*10**4\n",
    "\n",
    "                print(\"P após verificação de constrains das estacas(Distancia dos bulbos finais e choque entre estacas):\",p)\n",
    "                k+=1\n",
    "                iteracao+=1\n",
    "\n",
    "                break\n",
    "\n",
    "    return pontos_finais, array_angulos_horizontais_convertidos, array_angulos_horizontais,p\n",
    "\n",
    "def analise(pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais):#Analise estrutural dentro do ansys\n",
    "    mapdl.clear('NOSTART')\n",
    "    mapdl.prep7()\n",
    "\n",
    "    # Título\n",
    "    mapdl.title('Análise de Estacas e Casca')\n",
    "\n",
    "    # Definir o tipo de elemento (BEAM188) e suas propriedades\n",
    "    mapdl.et(1, 'BEAM188')\n",
    "\n",
    "    # Propriedades do material\n",
    "    modulo_elasticidade = 0.85 * 5600 * (40 ** 0.5) * 1e6  # N/m²\n",
    "    mapdl.mp('EX', 1, modulo_elasticidade)\n",
    "    mapdl.mp('PRXY', 1, 0.2)  # Coeficiente de Poisson\n",
    "    mapdl.mp('DENS', 1, 2500)  # Densidade\n",
    "\n",
    "    # Propriedades da seção da viga\n",
    "    mapdl.sectype(1, 'BEAM', 'CSOLID')\n",
    "    mapdl.secoffset('CENT')\n",
    "    mapdl.secdata(0.5)\n",
    "\n",
    "    # Número de nós intermediários\n",
    "    num_intermediate_nodes = 9\n",
    "\n",
    "    # Adicionar nós\n",
    "    node_id = 1\n",
    "    for i in range(2 * n_estacas + n_estacas_centrais):\n",
    "        # Obter coordenadas iniciais e finais\n",
    "        x_inicial, y_inicial, z_inicial = pontos_iniciais[i]\n",
    "        x_final, y_final, z_final = pontos_finais_ajustados[i]\n",
    "\n",
    "        # Criar o nó inicial\n",
    "        mapdl.n(node_id, x_inicial, y_inicial, z_inicial)\n",
    "\n",
    "        # Definir o ID do nó final\n",
    "        node_id_final = node_id + num_intermediate_nodes + 1\n",
    "\n",
    "        # Criar o nó final\n",
    "        mapdl.n(node_id_final, x_final, y_final, z_final)\n",
    "\n",
    "        # Travar o nó final\n",
    "        mapdl.d(node_id_final, 'ALL', 0)\n",
    "\n",
    "        # Preencher nós intermediários\n",
    "        mapdl.fill(node_id, node_id_final, num_intermediate_nodes)\n",
    "\n",
    "        # Atualizar o próximo node_id\n",
    "        node_id = node_id_final + 1\n",
    "\n",
    "    # Geração de elementos\n",
    "    n_elemento = 1\n",
    "    for i in range(2 * n_estacas + n_estacas_centrais):\n",
    "        for j in range(1, num_intermediate_nodes + 2):\n",
    "            N_1 = j + (num_intermediate_nodes + 2) * i\n",
    "            N_2 = N_1 + 1\n",
    "            mapdl.en(n_elemento, N_1, N_2)\n",
    "            n_elemento += 1\n",
    "\n",
    "    # Selecionar elementos tipo BEAM188\n",
    "    mapdl.esel('S', 'TYPE', '', 1)\n",
    "\n",
    "    # Contar elementos selecionados\n",
    "    num_elem = mapdl.get('num_elem', 'ELEM', 0, 'COUNT')\n",
    "    k = num_elem\n",
    "\n",
    "    # Criar elemento de carga\n",
    "    mapdl.n(1000, 3, 0, 0)\n",
    "    mapdl.n(1001, 3, 0, 0.1)\n",
    "    mapdl.en(k + 1, 1000, 1001)\n",
    "\n",
    "    # Definir tipo de elemento SHELL181\n",
    "    mapdl.et(2, 'SHELL181')\n",
    "    mapdl.keyopt(2, 8, 2)  # Elastoplástico\n",
    "    mapdl.keyopt(2, 3, 2)  # Precisão de tensões\n",
    "\n",
    "    # Propriedades do material para SHELL181\n",
    "    modulo_elasticidade_shell = 0.85 * 5600 * (20 ** 0.5) * 1e6\n",
    "    mapdl.mp('EX', 2, modulo_elasticidade_shell)\n",
    "    mapdl.mp('PRXY', 2, 0.2)\n",
    "    mapdl.mp('DENS', 2, 2500)\n",
    "\n",
    "    # Definir seção de casca\n",
    "    mapdl.sectype(2, 'SHELL')\n",
    "    mapdl.secdata(1.5)\n",
    "\n",
    "    # Criar retângulo e malhar\n",
    "    mapdl.rectng(0, 6, -3, 3)\n",
    "    mapdl.esize(0.1)\n",
    "    mapdl.amesh('ALL')\n",
    "\n",
    "    # Selecionar elementos tipo SHELL181\n",
    "    mapdl.esel('S', 'TYPE', '', 2)\n",
    "    mapdl.emodif('ALL', 'SECNUM', 2)\n",
    "\n",
    "    # Merge de nós\n",
    "    mapdl.nsel('S', 'LOC', 'Z', 0, 1e5)\n",
    "    mapdl.nummrg('NODE', 1e-5)\n",
    "    mapdl.nsel('ALL')\n",
    "\n",
    "    # Saída dos resultados\n",
    "    mapdl.allsel('ALL')\n",
    "\n",
    "    # Aplicar gravidade\n",
    "    mapdl.acel(0, 0, -9.81)\n",
    "\n",
    "    # Finalizar\n",
    "    mapdl.finish()\n",
    "\n",
    "    # Entrar no modo de solução\n",
    "    mapdl.slashsolu()\n",
    "    mapdl.antype(0)\n",
    "\n",
    "    # Definir a força aplicada e o número de load steps\n",
    "    f = 1000000  # Força de 1 milhão de N\n",
    "    casos = 12  # Definir loadsteps\n",
    "\n",
    "    # Loop para aplicar as diferentes condições de carga (12 casos de 15 em 15 graus)\n",
    "    for i in range(1, casos + 2):\n",
    "        Rad = (i - 1) * (180 / casos) * math.pi / 180  # Variação de 15 graus\n",
    "\n",
    "        # Calcular as componentes da força horizontal\n",
    "        FX = math.cos(Rad) * f\n",
    "        FY = math.sin(Rad) * f\n",
    "\n",
    "        # Selecionar todos os elementos\n",
    "        mapdl.allsel('ALL')\n",
    "\n",
    "        # Aplicar as forças horizontais no nó 1001\n",
    "        mapdl.f(1001, 'FX', FX)\n",
    "        mapdl.f(1001, 'FY', FY)\n",
    "\n",
    "        # Resolver o modelo\n",
    "        mapdl.solve()\n",
    "\n",
    "        # Salvar os resultados da solução\n",
    "        mapdl.save(f'load_step_{i}')\n",
    "\n",
    "    # Finalizar a análise\n",
    "    mapdl.finish()\n",
    "\n",
    "    # Definir o número de casos\n",
    "    casos = 12\n",
    "\n",
    "    # Entrar no modo de pós-processamento\n",
    "    mapdl.post1()\n",
    "\n",
    "    # Definir os Load Cases para N = 12\n",
    "    for i in range(1, casos + 2):\n",
    "        mapdl.lcdef(i, i, 1)  # Define o Load Case i para o Load Step i\n",
    "\n",
    "    # Carregar o primeiro Load Case\n",
    "    mapdl.lcase(1)\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!CÁLCULO DO MAX!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Comparar o Load Case 1 com os demais e armazenar os resultados (máximos)\n",
    "    for R in range(2, casos + 2):\n",
    "        mapdl.lcoper('MAX', R)  # Compara o Load Case na memória com os próximos\n",
    "        mapdl.lcwrite(50 + R)   # Escreve o resultado em um arquivo\n",
    "\n",
    "    mapdl.lcase(50 + (casos + 1))  # Carregar o último load case comparado\n",
    "    mapdl.etable('Fx_MAX', 'SMISC', 1, 'MAX')  # Força Axial máxima\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!CÁLCULO DO MIN!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Carregar o primeiro Load Case novamente\n",
    "    mapdl.lcase(1)\n",
    "\n",
    "    # Comparar o Load Case 1 com os demais para valores mínimos\n",
    "    for R in range(2, casos + 2):\n",
    "        mapdl.lcoper('MIN', R)  # Compara o Load Case na memória para valores mínimos\n",
    "        mapdl.lcwrite(60 + R)   # Escreve o resultado em um arquivo\n",
    "\n",
    "    mapdl.lcase(60 + (casos + 1))  # Carregar o último load case comparado\n",
    "    mapdl.etable('Fx_MIN', 'SMISC', 1, 'MIN')  # Força Axial mínima\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!CÁLCULO DO ABSMAX!!!!!!!!!!!!!!!!!!!!!\n",
    "    # Carregar o primeiro Load Case\n",
    "    mapdl.lcase(1)\n",
    "\n",
    "    # Comparar o Load Case 1 com os demais e armazenar os resultados (máximos absolutos)\n",
    "    for R in range(2, casos + 2):\n",
    "        mapdl.lcoper('ABMX', R)  # Compara o Load Case na memória com os próximos\n",
    "        mapdl.lcwrite(50 + R)   # Escreve o resultado em um arquivo\n",
    "\n",
    "    mapdl.lcase(50 + (casos + 1))  # Carregar o último load case comparado\n",
    "    mapdl.etable('My_ABMX', 'SMISC', 2, 'ABMX')  # Momento fletor máximo em Y\n",
    "    mapdl.etable('Mz_ABMX', 'SMISC', 3, 'ABMX')  # Momento fletor máximo em Z\n",
    "\n",
    "    # Gerar ETABLE para deslocamentos dos nós selecionados\n",
    "    mapdl.etable('UX', 'U', 'X')  # Deslocamento em X\n",
    "    mapdl.etable('UY', 'U', 'Y')  # Deslocamento em Y\n",
    "    mapdl.etable('UZ', 'U', 'Z')  # Deslocamento em Z\n",
    "\n",
    "    mapdl.eplot()\n",
    "\n",
    "\n",
    "\n",
    "    # Definir a sequência de elementos\n",
    "    elementos_sequencia = []\n",
    "    j = 1\n",
    "    for i in range(0, (2 * n_estacas) + n_estacas_centrais, 1):\n",
    "        y = num_intermediate_nodes * i + j\n",
    "        elementos_sequencia.append(y)\n",
    "        j += 1\n",
    "\n",
    "    # Dicionário para armazenar os resultados\n",
    "    data = {\n",
    "        \"Fx_MAX\": [],\n",
    "        \"Fx_MIN\": [],\n",
    "        \"Utot\": [],\n",
    "        \"My\": [],\n",
    "        \"Mz\": [],\n",
    "        \"r_max\": [],\n",
    "        \"load_case_max\": []\n",
    "    }\n",
    "\n",
    "    # Primeiro loop: Calculando Fx_MAX, Fx_MIN e Utot para cada elemento\n",
    "    for elem in elementos_sequencia:\n",
    "        fx_max = mapdl.get_value('ELEM', elem, 'ETABLE', 'Fx_MAX') / 1000  # kN\n",
    "        fx_min = mapdl.get_value('ELEM', elem, 'ETABLE', 'Fx_MIN') / 1000  # kN\n",
    "        ux = mapdl.get_value('ELEM', elem, 'ETABLE', 'UX') * 1000  # mm\n",
    "        uy = mapdl.get_value('ELEM', elem, 'ETABLE', 'UY') * 1000  # mm\n",
    "        uz = mapdl.get_value('ELEM', elem, 'ETABLE', 'UZ') * 1000  # mm\n",
    "        Utot = math.sqrt(ux**2 + uy**2 + uz**2)\n",
    "\n",
    "        # Adicionando os valores calculados ao dicionário\n",
    "        data[\"Fx_MAX\"].append(fx_max)\n",
    "        data[\"Fx_MIN\"].append(fx_min)\n",
    "        data[\"Utot\"].append(Utot)\n",
    "\n",
    "    # Segundo loop: Calculando r_max, My, e Mz para cada elemento em todos os load cases\n",
    "    for elem in elementos_sequencia:\n",
    "        r_max_elem = 0\n",
    "        my_max_elem = 0\n",
    "        mz_max_elem = 0\n",
    "        load_case_max = 0\n",
    "\n",
    "        for load_case in range(1, casos + 2):\n",
    "            mapdl.lcase(load_case)\n",
    "\n",
    "            mapdl.etable('My', 'SMISC', 2)\n",
    "            mapdl.etable('Mz', 'SMISC', 3)\n",
    "\n",
    "            my_val = mapdl.get_value('ELEM', elem, 'ETABLE', 'My') / 1000\n",
    "            mz_val = mapdl.get_value('ELEM', elem, 'ETABLE', 'Mz') / 1000\n",
    "\n",
    "            r_val = math.sqrt(my_val**2 + mz_val**2)\n",
    "\n",
    "            if r_val > r_max_elem:\n",
    "                r_max_elem = r_val\n",
    "                my_max_elem = my_val\n",
    "                mz_max_elem = mz_val\n",
    "                load_case_max = load_case\n",
    "\n",
    "        data[\"My\"].append(my_max_elem)\n",
    "        data[\"Mz\"].append(mz_max_elem)\n",
    "        data[\"r_max\"].append(r_max_elem)\n",
    "        data[\"load_case_max\"].append(load_case_max)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.round(1)\n",
    "\n",
    "    def f(x):\n",
    "        if x <= 0:\n",
    "            return 0\n",
    "        elif x > 0:\n",
    "            return np.exp(x)*10**7\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    delta1 = 5500 #Martelo cravação e carga de trabalho - As/Fck\n",
    "    delta2 = 2000 #Facultativo - Armadura máx - Atritolateral -Sem o de ponta (Justificar, mas não tem um valor exato normativo) #NBR6122 Fundações\n",
    "    delta3 = 2500 #NBR6118 2023 armadura max em >pilar< As max\n",
    "    delta4 = 125 #Tabela Nbr6118 - tabela limite de deslocamento/deformação |  Nbr8800 h/850 | ABS para pier\n",
    "\n",
    "    def calcular_Pt(df):\n",
    "        df['Pt_Fx_MAX'] = 0.0\n",
    "        df['Pt_Fx_MIN'] = 0.0\n",
    "        df['Pt_r_max'] = 0.0\n",
    "        df['Pt_Utot'] = 0.0\n",
    "\n",
    "        p_total = 0 #PESO PRÓPRIO DA ESTRUTURA\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Fx_MAX'] > 0:\n",
    "                df.at[index, 'Pt_Fx_MAX'] = f((abs(row['Fx_MAX']) - delta1) / delta1)\n",
    "            else:\n",
    "                df.at[index, 'Pt_Fx_MAX'] = f((abs(row['Fx_MAX']) - delta2) / delta2)\n",
    "\n",
    "            if row['Fx_MIN'] > 0:\n",
    "                df.at[index, 'Pt_Fx_MIN'] = f(((abs(row['Fx_MIN']) - delta1) / delta1))\n",
    "            else:\n",
    "                df.at[index, 'Pt_Fx_MIN'] = f(((abs(row['Fx_MIN']) - delta2) / delta2))\n",
    "\n",
    "            df.at[index, 'Pt_r_max'] = f(((abs(row['r_max']) - delta3) / delta3))\n",
    "            df.at[index, 'Pt_Utot'] = f(((abs(row['Utot']) - delta4) / delta4))\n",
    "\n",
    "            p_total += max(df.at[index, 'Pt_Fx_MAX'] + df.at[index, 'Pt_Fx_MIN'] +\n",
    "                        df.at[index, 'Pt_r_max'] + df.at[index, 'Pt_Utot'])\n",
    "\n",
    "        return float(p_total.round(1))\n",
    "\n",
    "    p_total = calcular_Pt(df)\n",
    "    return p_total, pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais\n",
    "\n",
    "\n",
    "\n",
    "def objective_function(xx):#Função do projeto, compila todas as demais funções para um objetivo final\n",
    "    p = 0\n",
    "\n",
    "    x_aba_1, y_aba_1 = round(xx[0], 1), round(xx[1], 1)\n",
    "    x_aba_2, y_aba_2 = round(xx[4], 1), round(xx[5], 1)\n",
    "    x_central_1 = round(xx[8], 1)\n",
    "\n",
    "    x_aba_1, y_aba_1 = converter_x(x_aba_1), converter_y(y_aba_1)\n",
    "    x_aba_2, y_aba_2 = converter_x(x_aba_2), converter_y(y_aba_2)\n",
    "    x_central_1 = converter_x(x_central_1)\n",
    "\n",
    "    coordinates = [\n",
    "        [x_aba_1, y_aba_1, 0],\n",
    "        [x_aba_2, y_aba_2, 0],\n",
    "        [x_central_1, 0.0, 0.0]\n",
    "    ]\n",
    "\n",
    "    ang_v1, ang_h1 = round_to_nearest(x[2]), round_to_nearest(x[3])\n",
    "    ang_v2, ang_h2 = round_to_nearest(x[6]), round_to_nearest(x[7])\n",
    "    ang_v3, ang_h3 = round_to_nearest(x[9]), round_to_nearest(x[10])\n",
    "\n",
    "    array_angulos_verticais = [ang_v1, ang_v2, ang_v3]\n",
    "    array_angulos_horizontais = [ang_h1, ang_h2, ang_h3]\n",
    "\n",
    "    array_angulos_verticais_convertidos = converter_angulos_verticais(array_angulos_verticais)\n",
    "    array_angulos_horizontais_convertidos = converter_angulos_horizontais(array_angulos_horizontais)\n",
    "    print(\"array_angulos_horizontais_convertidos ORIGINAIS\",array_angulos_horizontais_convertidos)\n",
    "    espelho = espelhar_pontos(coordinates, n_estacas_centrais)\n",
    "\n",
    "    for i in range(len(coordinates)):\n",
    "        for j in range(i + 1, len(coordinates)):\n",
    "            if distancia_entre_pontos(coordinates[i], coordinates[j]) < distancia_minima:\n",
    "                p+= (distancia_minima / (0.1+distancia_entre_pontos(coordinates[i], coordinates[j]))) * 10**5\n",
    "\n",
    "    coordenadas_finais = gerar_pontos_finais(coordinates, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos)\n",
    "    espelhos_finais = espelhar_pontos_finais(coordenadas_finais, int(n_estacas_centrais))\n",
    "\n",
    "    pontos_iniciais = coordinates + espelho\n",
    "    pontos_finais = np.array(coordenadas_finais + espelhos_finais)\n",
    "    \n",
    "\n",
    "    pontos_finais_ajustados, array_angulos_horizontais_convertidos, array_angulos_horizontais,p = verificar_e_ajustar_pontos(\n",
    "    pontos_iniciais, L_estacas, distancia_minima, pontos_finais, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos, array_angulos_horizontais,p)\n",
    "\n",
    "    # Condição para realizar ou pular a análise\n",
    "    if p==0:\n",
    "        # Realiza a análise se p for = zero\n",
    "        p_analise, pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais = analise(pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais)\n",
    "        # Atualiza p com o valor de p_analise após a análise\n",
    "        p = p  + p_analise\n",
    "    else:\n",
    "        # Pula a análise se p for igual a zero\n",
    "        print(\"Pula a análise, função já punida.\")\n",
    "    \n",
    "    return pontos_iniciais, pontos_finais_ajustados, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos, p\n",
    "\n",
    "\n",
    "\n",
    "def obj_fun(xx):\n",
    "    p = 0\n",
    "\n",
    "    x_aba_1, y_aba_1 = round(xx[0], 1), round(xx[1], 1)\n",
    "    x_aba_2, y_aba_2 = round(xx[4], 1), round(xx[5], 1)\n",
    "    x_central_1 = round(xx[8], 1)\n",
    "\n",
    "    x_aba_1, y_aba_1 = converter_x(x_aba_1), converter_y(y_aba_1)\n",
    "    x_aba_2, y_aba_2 = converter_x(x_aba_2), converter_y(y_aba_2)\n",
    "    x_central_1 = converter_x(x_central_1)\n",
    "\n",
    "    coordinates = [\n",
    "        [x_aba_1, y_aba_1, 0],\n",
    "        [x_aba_2, y_aba_2, 0],\n",
    "        [x_central_1, 0.0, 0.0]\n",
    "    ]\n",
    "\n",
    "    ang_v1, ang_h1 = round_to_nearest(x[2]), round_to_nearest(x[3])\n",
    "    ang_v2, ang_h2 = round_to_nearest(x[6]), round_to_nearest(x[7])\n",
    "    ang_v3, ang_h3 = round_to_nearest(x[9]), round_to_nearest(x[10])\n",
    "\n",
    "    array_angulos_verticais = [ang_v1, ang_v2, ang_v3]\n",
    "    array_angulos_horizontais = [ang_h1, ang_h2, ang_h3]\n",
    "\n",
    "    array_angulos_verticais_convertidos = converter_angulos_verticais(array_angulos_verticais)\n",
    "    array_angulos_horizontais_convertidos = converter_angulos_horizontais(array_angulos_horizontais)\n",
    "    print(\"array_angulos_horizontais_convertidos ORIGINAIS\",array_angulos_horizontais_convertidos)\n",
    "    espelho = espelhar_pontos(coordinates, n_estacas_centrais)\n",
    "\n",
    "    for i in range(len(coordinates)):\n",
    "        for j in range(i + 1, len(coordinates)):\n",
    "            if distancia_entre_pontos(coordinates[i], coordinates[j]) < distancia_minima:\n",
    "                p+= (distancia_minima / (0.1+distancia_entre_pontos(coordinates[i], coordinates[j]))) * 10**5\n",
    "\n",
    "    coordenadas_finais = gerar_pontos_finais(coordinates, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos)\n",
    "    espelhos_finais = espelhar_pontos_finais(coordenadas_finais, int(n_estacas_centrais))\n",
    "\n",
    "    pontos_iniciais = coordinates + espelho\n",
    "    pontos_finais = np.array(coordenadas_finais + espelhos_finais)\n",
    "    \n",
    "\n",
    "    pontos_finais_ajustados, array_angulos_horizontais_convertidos, array_angulos_horizontais,p = verificar_e_ajustar_pontos(\n",
    "    pontos_iniciais, L_estacas, distancia_minima, pontos_finais, array_angulos_verticais_convertidos, array_angulos_horizontais_convertidos, array_angulos_horizontais,p)\n",
    "\n",
    "    # Condição para realizar ou pular a análise\n",
    "    if p==0:\n",
    "        # Realiza a análise se p for = zero\n",
    "        p_analise, pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais = analise(pontos_iniciais, pontos_finais_ajustados, array_angulos_horizontais, array_angulos_verticais)\n",
    "        # Atualiza p com o valor de p_analise após a análise\n",
    "        p = p  + p_analise\n",
    "    else:\n",
    "        # Pula a análise se p for igual a zero\n",
    "        print(\"Pula a análise, função já punida.\")\n",
    "    print(\"p depois do ajuste\",p)\n",
    "\n",
    "    \n",
    "    fobj = torch.sum(xx)\n",
    "    \n",
    "    return fobj,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate initial sampling plan using LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LHS_sample(N_INITIAL, DIM)\n",
    "x = torch.from_numpy(x)\n",
    "x = x.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "name_f, name_x = 'f.mat', 'x.mat'\n",
    "path = './'\n",
    "pathname_f = path + name_f\n",
    "pathname_x = path + name_x\n",
    "check_file = os.path.isfile(pathname_f)\n",
    "if check_file:\n",
    "    mat_contents = sio.loadmat(pathname_f)\n",
    "    f = mat_contents['f']\n",
    "    f = torch.Tensor(f)\n",
    "    f = f.view(N_INITIAL)\n",
    "    \n",
    "    mat_contents = sio.loadmat(pathname_x)\n",
    "    x = mat_contents['x']\n",
    "    x = torch.Tensor(x)\n",
    "else:\n",
    "    f = obj_fun(x)\n",
    "    f = f.view(N_INITIAL)\n",
    "    sio.savemat(name_f, {'f': f.numpy()})\n",
    "    sio.savemat(name_x, {'x': x.numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_g, val_g = train_test_split(x, f, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Global Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, likelihood, best_loss = train_model(train_x, train_g, val_x, val_g, TRAINING_ITERATIONS)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "it = 0\n",
    "print(f'\\nIteration {it}. Best of DGPR: {torch.min(f)}. GPR model: Train loss = {best_loss[0]}; Val. loss = {best_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while it < N_INFILL:\n",
    "    it += 1\n",
    "    \n",
    "    # Search for the maximum expected improvement\n",
    "    new_point = bsa(expected_improvement, bounds=BOUNDS_BSA,\n",
    "                    popsize=BSA_POPSIZE, epoch=BSA_EPOCH, data=model)\n",
    "    x_new = torch.from_numpy(new_point.x)\n",
    "    EI = new_point.y\n",
    "\n",
    "    # Objective function at the new point\n",
    "    f_new = obj_fun(x_new)\n",
    "    \n",
    "    print(f'Iteration {it} of {N_INFILL}')\n",
    "    if f_new < torch.min(f): print(f'New best: {float(f_new):.2f} at position {it}')\n",
    "    \n",
    "    # Add new values to the initial sampling\n",
    "    x = torch.cat((x, torch.from_numpy(np.array([np.asarray(x_new)]))), 0)\n",
    "    x = x.to(torch.float32)\n",
    "    f_new = f_new.view(-1)\n",
    "    f = torch.cat((f, f_new), 0)\n",
    "    \n",
    "    # Update model\n",
    "    train_x, val_x, train_g, val_g = train_test_split(x, f, test_size=0.20)\n",
    "    model, likelihood, best_loss = train_model(train_x, train_g, val_x, val_g, TRAINING_ITERATIONS)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    print(f'\\nIteration {it}. Best of DGPR: {torch.min(f)}. GPR model: Train loss = {best_loss[0]}; Val. loss = {best_loss[-1]}')\n",
    "    \n",
    "    # if abs(EI) < TOL_MIN_EI:\n",
    "    #     print('Optimization finished. Minimum tolerance achieved.')\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'f*: {torch.min(f):.2f}; x*: {x[torch.argmin(f), :]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results[\"x\"] = x\n",
    "results[\"f\"] = f\n",
    "results[\"n_initial\"] = N_INITIAL\n",
    "results[\"n_infill\"] = N_INFILL\n",
    "results[\"OFEs\"] = N_INITIAL + it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_f, name_x = 'f.mat', 'x.mat'\n",
    "path = './'\n",
    "pathname_f = path + name_f\n",
    "pathname_x = path + name_x\n",
    "\n",
    "sio.savemat(name_f, {'f': f.numpy()})\n",
    "sio.savemat(name_x, {'x': x.numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using BSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPSIZE = 11\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bsa = bsa(obj_fun, bounds=BOUNDS_BSA,\n",
    "                    popsize=POPSIZE, epoch=EPOCHS, data=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f* = {results_bsa.y:.2f}; x* = {results_bsa.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot optimization history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the cost history\n",
    "plt.figure(figsize=(8, 6))  # Optional: Set figure size\n",
    "plt.plot(np.arange(EPOCHS), results.convergence, marker='o', linestyle='-', color='b', label='Cost History')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost History per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)  # Optional: Add grid for better visualization\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
